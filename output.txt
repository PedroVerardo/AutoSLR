<--page_start:1-->
Whence to Learn? Transferring Knowledge in<--size=23.9-->

Conﬁgurable Systems Using BEETLE<--size=23.9-->

Rahul Krishna<--size=11.0-->
, Vivek Nair<--size=11.0-->
, Pooyan Jamshidi, and Tim Menzies<--size=11.0-->
, Fellow, IEEE<--size=11.0-->

Abstract—As software systems grow in complexity and the space of possible conﬁgurations increases exponentially, ﬁnding the near-<--size=8.0-->
optimal conﬁguration of a software system becomes challenging. Recent approaches address this challenge by learning performance<--size=8.0-->
models based on a sample set of conﬁgurations. However, collecting enough sample conﬁgurations can be very expensive since each<--size=8.0-->
such sample requires conﬁguring, compiling, and executing the entire system using a complex test suite. When learning on new data is<--size=8.0-->
too expensive, it is possible to use Transfer Learning to “transfer” old lessons to the new context. Traditional transfer learning has a<--size=8.0-->
number of challenges, speciﬁcally, (a) learning from excessive data takes excessive time, and (b) the performance of the models built<--size=8.0-->
via transfer can deteriorate as a result of learning from a poor source. To resolve these problems, we propose a novel transfer learning<--size=8.0-->
framework called BEETLE, which is a “bellwether”-based transfer learner that focuses on identifying and learning from the most<--size=8.0-->
relevant source from amongst the old data. This paper evaluates BEETLE with 57 different software conﬁguration problems based<--size=8.0-->
on ﬁve software systems (a video encoder, an SAT solver, a SQL database, a high-performance C-compiler, and a streaming<--size=8.0-->
data analytics tool). In each of these cases, BEETLE found conﬁgurations that are as good as or better than those found by other<--size=8.0-->
state-of-the-art transfer learners while requiring only a fraction (1<--size=8.0-->

7th) of the measurements needed by those other methods. Based on<--size=5.6-->
these results, we say that BEETLE is a new high-water mark in optimally conﬁguring software.<--size=8.0-->

Index Terms—Performance optimization, SBSE, transfer learning, bellwether<--size=8.0-->

1<--size=11.5-->
INTRODUCTION<--size=11.5-->
A<--size=24.4-->

problem of increasing difﬁculty in modern software is<--size=9.5-->
ﬁnding the right set of conﬁgurations that can achieve<--size=9.5-->
the best performance. As more functionality is added to the<--size=9.5-->
code, it becomes increasingly difﬁcult for users to under-<--size=9.5-->
stand all the options a software offers [1], [2], [3], [4], [5], [6],<--size=9.5-->
[7], [8], [9], [10], [11], [12]. It is hard to overstate the impor-<--size=9.5-->
tance of good conﬁguration choices and the impact poor<--size=9.5-->
choices can have. For example, it has been reported that for<--size=9.5-->
Apache Storm, the throughput achieved using the worst<--size=9.5-->
conﬁguration was 480 times slower than that achieved by the<--size=9.5-->
best conﬁguration [3].<--size=9.5-->

Recent research has attempted to address this problem<--size=9.5-->
usually by creating accurate performance models that<--size=9.5-->
predict performance characteristics. While this approach is<--size=9.5-->
certainly cheaper and more effective than manual conﬁgu-<--size=9.5-->
ration it still incurs the expense of extensive data collection.<--size=9.5-->
This is undesirable, since the data collection must be<--size=9.5-->
repeated if the software is updated or the workload of the<--size=9.5-->
system changes.<--size=9.5-->

Rather than learning new conﬁgurations afresh, in this<--size=9.5-->
paper, we ask if we can learn from existing conﬁgurations.<--size=9.5-->
Formally, this is called “transfer learning”; i.e., the transfer of<--size=9.5-->
information from selected “source” software conﬁgurations<--size=9.5-->
running on one environment to learn a model for predicting<--size=9.5-->
the performance of some “target” conﬁgurations in a different<--size=9.5-->
environment. Transfer learning has been extensively explored<--size=9.5-->
in other areas of software analytics [13], [14], [15], [16], [17],<--size=9.5-->
[18]. This is a practical possibility since often when a software<--size=9.5-->
is being deployed in a new environment, there are examples<--size=9.5-->
of the system already executing under a different environ-<--size=9.5-->
ment. To the best of our knowledge, this paper is among the<--size=9.5-->
earliest studies to apply transfer learning for performance<--size=9.5-->
optimization. Our proposed method is signiﬁcantly faster<--size=9.5-->
than any current state-of-the-art methods in identifying near-<--size=9.5-->
optimum conﬁgurations for a software system.<--size=9.5-->

Transfer learning can only be useful in cases where the<--size=9.5-->
source environment is similar to the target environment. If<--size=9.5-->
the source and the target are not similar, knowledge should<--size=9.5-->
not be transferred. In such situations, transfer learning can<--size=9.5-->
be unsuccessful and can lead to a negative transfer. Prior<--size=9.5-->
work on transfer learning focused on “What to transfer” and<--size=9.5-->
“How to transfer”, by implicitly assuming that the source<--size=9.5-->
and target are related to each other. However, those work<--size=9.5-->
failed to address “From where (whence) to transfer” [19].<--size=9.5-->
Jamshidi et al. [20] alluded to this and explained when trans-<--size=9.5-->
fer learning works but, did not provide a method which can<--size=9.5-->
help in selecting a suitable source.<--size=9.5-->

The issue of identifying a suitable source is a common<--size=9.5-->
problem in transfer learning. To address this, some research-<--size=9.5-->
ers [18], [21], [22], [23] have recently proposed the use of the<--size=9.5-->
bellwether effect, which states that:<--size=9.5-->

<--size=8.0-->
Rahul Krishna is with the Department of Computer Science, Columbia Univer-<--size=8.0-->
sity, New York, NY 10027 USA. E-mail: i.m.ralk@gmail.com.<--size=8.0-->
<--size=8.0-->
Vivek Nair and Tim Menzies is with the Department of Computer Science,<--size=8.0-->
North Carolina State University, Raleigh, NC 27695 USA.<--size=8.0-->
E-mail: {vivekaxl, tim.menzies}@gmail.com.<--size=8.0-->
<--size=8.0-->
Pooyan Jamshidi is with the Department of Computer Science and Engi-<--size=8.0-->
neering, University of South Carolina, Columbia, SC 29208 USA.<--size=8.0-->
E-mail: pooyan.jamshidi@gmail.com.<--size=8.0-->

Manuscript received 2 June 2019; revised 7 Feb. 2020; accepted 10 Mar. 2020.<--size=8.0-->
Date of publication 30 Mar. 2020; date of current version 10 Dec. 2021.<--size=8.0-->
(Corresponding author: Rahul Krishna.)<--size=8.0-->
Recommended for acceptance by M. Kim.<--size=8.0-->
Digital Object Identiﬁer no. 10.1109/TSE.2020.2983927<--size=8.0-->

2956<--size=7.0-->
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 47, NO. 12, DECEMBER 2021<--size=7.0-->

0098-5589 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.<--size=6.0-->

See ht_tps://www.ieee.org/publications/rights/index.html for more information.<--size=6.0-->
horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:1-->

<--page_start:2-->
“When analyzing a community of software data, there is at<--size=9.5-->
least one exemplary source data, called bellwether(s), which<--size=9.5-->
best deﬁnes predictors for all the remaining datasets ...”<--size=9.5-->

Inspired by the success of bellwethers in other areas, this<--size=9.5-->
paper deﬁnes and evaluates a new transfer learner for soft-<--size=9.5-->
ware conﬁguration called Bellwether Transfer Learner<--size=9.5-->
(henceforth referred to as BEETLE). BEETLE can perform<--size=9.5-->
knowledge transfer using just a few samples from a care-<--size=9.5-->
fully identiﬁed source environment(s).<--size=9.5-->

For evaluation, we explore ﬁve real-world software<--size=9.5-->
systems from different domains– a video encoder, a SAT<--size=9.5-->
solver, a SQL database, a high-performance C-compiler,<--size=9.5-->
and a streaming data analytics tool (measured under 57<--size=9.5-->
enviroments overall). In each case, we discovered that<--size=9.5-->
BEETLE found conﬁgurations as good as or better than<--size=9.5-->
those found by other state-of-the-art transfer learners<--size=9.5-->
while requiring only1<--size=9.5-->

7th of the measurements needed by<--size=6.6-->
those other methods. Reducing the number of measure-<--size=9.5-->
ments is an important consideration since collecting data<--size=9.5-->
in this domain can be computationally and monetarily<--size=9.5-->
expensive.<--size=9.5-->

Overall, this work makes the following contributions:<--size=9.5-->

1)<--size=9.5-->
Source selection: We show that the bellwether effect<--size=9.5-->
exists in performance optimization and that we can<--size=9.5-->
use this to discover suitable sources (called bell-<--size=9.5-->
wether environments) to perform transfer learning<--size=9.5-->
(see Section 7).<--size=9.5-->
2)<--size=9.5-->
Cheap source selection: BEETLE, using bellwethers,<--size=9.5-->
evaluates at most  10% of the conﬁguration space<--size=9.5-->
(see Section 4).<--size=9.5-->
3)<--size=9.5-->
Simple Transfer learning using Bellwethers: We develop<--size=9.5-->
a novel transfer learning algorithm using bellwether<--size=9.5-->
called BEETLE that exploits the bellwether environ-<--size=9.5-->
ment to construct a simple transfer learner (see<--size=9.5-->
Section 4).<--size=9.5-->
4)<--size=9.5-->
More effective than non-transfer learning: We show that<--size=9.5-->
using the BEETLE is better than non-transfer learning<--size=9.5-->
approaches. It is also lot more economical (see<--size=9.5-->
Section 7).<--size=9.5-->
5)<--size=9.5-->
More effective than state-of-the-art methods: Conﬁgura-<--size=9.5-->
tions discovered using the bellwether environment<--size=9.5-->
are better than the state-of-the-art methods [24], [25]<--size=9.5-->
(see Section 7).<--size=9.5-->
6)<--size=9.5-->
Reproduction Package: To assist other researchers, a<--size=9.5-->
reproduction package with all our scripts and data<--size=9.5-->
are available online (see https://git.io/fjsky).<--size=9.5-->
The rest of this article is structured as follows: The<--size=9.5-->
remainder of this section presents the research questions<--size=9.5-->
asked here (Section 1.1) answered in this paper. Section 2<--size=9.5-->
presents some motivation for this work. Section 3 describes<--size=9.5-->
the problem formulation and explains the concept of Bell-<--size=9.5-->
wethers. Section 4 describes BEETLE followed by a quick<--size=9.5-->
overview of the prior work in transfer learning in perfor-<--size=9.5-->
mance conﬁguration optimization in Section 5. In Section 6,<--size=9.5-->
we present experimental setup and followed by answers to<--size=9.5-->
research questions in Section 7. In Section 8, we discuss our<--size=9.5-->
ﬁndings further and answer some additional questions per-<--size=9.5-->
taining to our results. Section 9 discusses some threats to<--size=9.5-->
validity, related work and conclusion are presented in Sec-<--size=9.5-->
tions 10 and 11 respectively.<--size=9.5-->

1.1<--size=10.0-->
Research Questions<--size=10.0-->

RQ1:<--size=9.5-->
Does there exist a Bellwether Environment?First, we ask<--size=9.5-->
if there exist bellwether environments to train trans-<--size=9.5-->
fer<--size=9.5-->
learners<--size=9.5-->
for<--size=9.5-->
performance<--size=9.5-->
optimization.<--size=9.5-->
We<--size=9.5-->
hypothesize that, if these bellwether environments<--size=9.5-->
exist, we can improve the efﬁcacy of transfer learning.<--size=9.5-->
Result: We ﬁnd that bellwether environments are<--size=9.5-->
prevalent in performance optimization. That is, in<--size=9.5-->
each of the software systems, there exists at least one<--size=9.5-->
environment that can be used to construct superior<--size=9.5-->
transfer learners.<--size=9.5-->

RQ2:<--size=9.5-->
How many performance measurements are required to dis-<--size=9.5-->
cover bellwether environments?Having established that<--size=9.5-->
bellwether environments are prevalent, the purpose<--size=9.5-->
of this research question is to establish how many<--size=9.5-->
performance measurements are needed in each of<--size=9.5-->
the environments to discover these bellwether envi-<--size=9.5-->
ronments.<--size=9.5-->
Result: We can discover a potential bellwether envi-<--size=9.5-->
ronment by measuring as little as 10 percent of the<--size=9.5-->
total conﬁgurations across all the software system.<--size=9.5-->

RQ3:<--size=9.5-->
How does BEETLE compare with other non-transfer-<--size=9.5-->
learning based methods?The alternative to transfer<--size=9.5-->
learning is just to use the target data to ﬁnd the near-<--size=9.5-->
optimal conﬁgurations. In the literature are many<--size=9.5-->
examples of this “non-transfer” approach [7], [10],<--size=9.5-->
[12], [26] and for our comparisons, we used the cur-<--size=9.5-->
rent state-of-the-art performance optimization model<--size=9.5-->
proposed by Nair et al. [10].<--size=9.5-->
Result: Our experiments demonstrate that transfer<--size=9.5-->
learning using bellwethers (BEETLE) outperforms<--size=9.5-->
other methods that do not use transfer learning both<--size=9.5-->
in terms of cost and the quality of the model.<--size=9.5-->

RQ4:<--size=9.5-->
How does BEETLE compare to state-of-the-art transfer<--size=9.5-->
learners?The ﬁnal research question compares BEE-<--size=9.5-->
TLE with two other state-of-the-art transfer learners<--size=9.5-->
used commonly in performance optimization (for<--size=9.5-->
details see Section 5). The purpose of this research<--size=9.5-->
question is to determine if a simple transfer learner<--size=9.5-->
like BEETLE with carefully selected source environ-<--size=9.5-->
ments can perform as well as other complex transfer<--size=9.5-->
learners that do not perform any source selection.<--size=9.5-->
Result: We show that a simple transfer learning using<--size=9.5-->
bellwether environment (BEETLE) just as good as (or<--size=9.5-->
better than) current state-of-the-art transfer learners.<--size=9.5-->

2<--size=11.5-->
MOTIVATION<--size=11.5-->

With the appearance of continuous software engineering<--size=9.5-->
and devops, conﬁgurability has become a primary concern of<--size=9.5-->
software engineers. System administrators today develop<--size=9.5-->
and use different versions software programs under running<--size=9.5-->
several different workloads and in numerous environments.<--size=9.5-->
In doing so, they try to apply software engineering methods<--size=9.5-->
to best conﬁgure these software systems. Despite their best<--size=9.5-->
efforts, the available evidence is that they need to be better<--size=9.5-->
assited in making all the conﬁguration decisions. Xu et al. [1]<--size=9.5-->

KRISHNA ET AL.: WHENCE TO LEARN? TRANSFERRING KNOWLEDGE IN CONFIGURABLE SYSTEMS USING BEETLE<--size=7.0-->
2957<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:2-->

<--page_start:3-->
reports that, when left to their own judgementments,<--size=9.5-->
developers ignore up to 80 percent of conﬁguration<--size=9.5-->
options, which exposes them to many potential prob-<--size=9.5-->
lems. For this reason, the research community is devot-<--size=9.5-->
ing a lot of effort to conﬁguration studies, as witnessed<--size=9.5-->
by many recent software engineering research publica-<--size=9.5-->
tions [4], [5], [6], [7], [8], [9], [10], [12], [27]. For details,<--size=9.5-->
see Section 10 for the additional related work.<--size=9.5-->

Without automatic support (e.g., with systems like BEE-<--size=9.5-->
TLE), humans ﬁnd it difﬁcult to settle on their initial choice<--size=9.5-->
for software conﬁgurations. The available evidence [2], [3],<--size=9.5-->
[28] shows that system administrators frequently make<--size=9.5-->
poor conﬁguration choices. Typically, off-the-shelf defaults<--size=9.5-->
are used, which often behave poorly. There are various<--size=9.5-->
examples presented in the literature which have established<--size=9.5-->
that choosing default conﬁguration can lead to sub-optimal<--size=9.5-->
performance. For instance, Van Aken et al. report that the<--size=9.5-->
default MySQL conﬁgurations in 2016 assume that it will be<--size=9.5-->
installed on a machine that has 160 MB of RAM (which, at<--size=9.5-->
that time, was incorrect by, at least, an order of magni-<--size=9.5-->
tude) [2]. Also, Herodotou et al. [28] report that default set-<--size=9.5-->
tings for Hadoop results in the worst possible performance.<--size=9.5-->

Traditional approaches to ﬁnding good conﬁguration are<--size=9.5-->
very resource intensive. A typical approach uses sensitivity<--size=9.5-->
analysis [29], where performance models are learned by<--size=9.5-->
measuring the performance of the system under a limited<--size=9.5-->
number of sampled conﬁgurations. While this approach is<--size=9.5-->
cheaper and more effective than manual exploration, it still<--size=9.5-->
incurs the expense of extensive data collection about the<--size=9.5-->
software [3], [4], [7], [8], [10], [12], [26], [27], [30]. This is<--size=9.5-->
undesirable since this data collection has to be repeated if<--size=9.5-->
ever the software is updated or the environment of the sys-<--size=9.5-->
tem changes abruptly. While we cannot tame the pace of<--size=9.5-->
change in modern software systems, we can reduce the data<--size=9.5-->
collection effort required to react to that change. The experi-<--size=9.5-->
ments of this paper make the case that BEETLE scales to<--size=9.5-->
some large conﬁguration problems, better than the prior<--size=9.5-->
state of the art. Further, it does so using fewer measure-<--size=9.5-->
ments than existing state-of-the-art methods.<--size=9.5-->

Further, we note that BEETLE is particularly recom-<--size=9.5-->
mended in highly dynamic projects where the environ-<--size=9.5-->
ments keep changing. When context changes, so to must the<--size=9.5-->
solutions applied by software engineers. When frequently<--size=9.5-->
re-computing best conﬁgurations, it becomes vitally impor-<--size=9.5-->
tant that computation cost is kept to a minimum. Amongst<--size=9.5-->

the space of known conﬁguration tools, we most endorse<--size=9.5-->
BEETLE for very dynamic environments. We say this since,<--size=9.5-->
of all the systems surveyed here, BEETLE has the lowest<--size=9.5-->
CPU cost (and we conjecture that this is so since BEETLE<--size=9.5-->
makes the best use of old conﬁgurations).<--size=9.5-->

As a more concerete example, consider an organization<--size=9.5-->
that runs, say, N heavy Apache Spark workloads on the<--size=9.5-->
cloud. To optimize the performance of Apache Spark on the<--size=9.5-->
given workloads, the DevOps Team need to ﬁnd the opti-<--size=9.5-->
mal solutions for each of these workloads, i.e., conduct per-<--size=9.5-->
formance optimization N times. This setup has two major<--size=9.5-->
shortcomings: hardware change and workload change.<--size=9.5-->

Hardware Change. Even though the DevOps engineer of a<--size=9.5-->
software system performs a performance optimization for a<--size=9.5-->
speciﬁc workload in its staging environment, as soon as<--size=9.5-->
the software is moved to the production environment<--size=9.5-->
the optimal conﬁguration found previously may be inac-<--size=9.5-->
curate. This problem is further accentuated if the pro-<--size=9.5-->
duction environment changes due to the ever-expanding<--size=9.5-->
cloud portfolios.<--size=9.5-->

Workload Change. The developers of a database system<--size=9.5-->
can optimize the system for a read-heavy workload, how-<--size=9.5-->
ever, the optimal conﬁguration may change once the work-<--size=9.5-->
load changes to, say, a write-heavy counterpart. The reason<--size=9.5-->
is that if the workload changes, different functionalities of<--size=9.5-->
the software might get activated more often and so the non-<--size=9.5-->
functional behavior changes too. This means that as soon as<--size=9.5-->
a new workload is introduced (new feature in the organ-<--size=9.5-->
ization’s product) or if the workload changes, the process of<--size=9.5-->
performance optimization needs to be repeated.<--size=9.5-->

Given the fragility of traditional performance optimiza-<--size=9.5-->
tion, it is imperative that we develop a method to learn<--size=9.5-->
from our previous experiences and hence reduce the burden<--size=9.5-->
of having to ﬁnd optimum conﬁgurations ad nauseam.<--size=9.5-->

3<--size=11.5-->
DEFINITIONS AND PROBLEM STATEMENT<--size=11.5-->

Conﬁguration. A software system, S, may offer a number of<--size=9.5-->
conﬁguration options that can be changed. We denote the<--size=9.5-->
total number of conﬁguration options of a software system<--size=9.5-->
S as N. A conﬁguration option of the software system can<--size=9.5-->
either be a (1) continuous numeric value or a (2) categorical<--size=9.5-->
value. This distinction is very important since it impacts the<--size=9.5-->
choice of machine learning algorithms. The conﬁguration<--size=9.5-->
options in all software systems studied in this paper are a<--size=9.5-->
combination of both categorical and continuous in nature.<--size=9.5-->
The learning algorithm used in this paper namely, Regres-<--size=9.5-->
sion Trees, are particularly well suited to handle such a com-<--size=9.5-->
bination of continuous and categorical data.<--size=9.5-->

A conﬁguration is represented by ci, where i represents<--size=9.5-->
the ith conﬁguration of a system. A set of all conﬁgurations<--size=9.5-->
is called the conﬁguration space, denoted as C. Formally, C is<--size=9.5-->

<--image width=2075.0 height=1066.0-->
Fig. 1. Traditional transfer learning compared with using bellwethers to<--size=8.0-->
discover near optimal conﬁgurations.<--size=8.0-->

Fig. 2. Some conﬁguration options for SQLite.<--size=8.0-->

2958<--size=7.0-->
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 47, NO. 12, DECEMBER 2021<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:3-->

<--page_start:4-->
a Cartesian product of all possible options C = Dom(c1) <--size=9.5-->
Dom(c2)      Dom(cN), where DomðciÞ is either R (Real<--size=9.5-->
Numbers) or B (Catergorical/Boolean value) and N is the<--size=9.5-->
number of conﬁguration options.<--size=9.5-->

As a simple example, consider a subset of conﬁguration<--size=9.5-->
options from SQLite, i.e., S  SQLite. This is shown in<--size=9.5-->
Fig. 2. The subset of SQLite offers three conﬁguration<--size=9.5-->
options namely, ATOMIC (atomic delete), USE_LFS (use<--size=9.5-->
large ﬁle storage), and SECURE (secure delete), ie., N ¼ 3.<--size=9.5-->
The last column contains the latency in ms when various<--size=9.5-->
combinations of these options are chosen.<--size=9.5-->

Environment. As deﬁned by Jamshidi et al.[20], the differ-<--size=9.5-->
ent ways a software system is deployed and used is called<--size=9.5-->
its environment (e). The environment is usually deﬁned in<--size=9.5-->
terms of: (1) workload (w): the input which the system oper-<--size=9.5-->
ates upon; (2) hardware (h): the hardware on which the<--size=9.5-->
system is running; and (3) version (v): the state of the<--size=9.5-->
software.<--size=9.5-->

Note that, other environmental changes might be possi-<--size=9.5-->
ble (e.g., JVM version used, etc.). For example, consider<--size=9.5-->
software system Apache Storm, here we must ensure that<--size=9.5-->
an appropriate JVM is installed in an environment before<--size=9.5-->
it can be deployed in that environment. Indeed, the selec-<--size=9.5-->
tion of one version of a JVM over another can have a<--size=9.5-->
profound performance impact. However, the perceived<--size=9.5-->
improvement in the performance is due to the optimiza-<--size=9.5-->
tions in JVM, not the original software system being stud-<--size=9.5-->
ied. Therefore, in this paper, we do not alter these other<--size=9.5-->
factors which do not have a direct impact on the perfor-<--size=9.5-->
mance of the software system.<--size=9.5-->

The following criteria is used to deﬁne an environment:<--size=9.5-->

1)<--size=9.5-->
Environmental factors of the software systems that<--size=9.5-->
we can vary in the deployment stack of the system.<--size=9.5-->
This prevents us from varying factors such as the<--size=9.5-->
JVM version, CPU frequency, system software, etc.,<--size=9.5-->
which deﬁne the deployment stack and not the soft-<--size=9.5-->
ware system.<--size=9.5-->
2)<--size=9.5-->
Common changes developers choose to alter in the<--size=9.5-->
software system. In practice, it is these factors that<--size=9.5-->
affect the performance of systems the most [20], [24],<--size=9.5-->
[25], [31].<--size=9.5-->
3)<--size=9.5-->
Factors that are most amenable for transfer learning.<--size=9.5-->
Preliminary studies have shown that factors such as<--size=9.5-->
workload, hardware, and software version lend<--size=9.5-->
themselves very well to transfer learning [20], [25].<--size=9.5-->
For a more detailed description of the factors that were<--size=9.5-->
changed and those that were left unchanged, see Table 1.<--size=9.5-->

Formally, we say an environment is ee ¼ fw; h; vg where<--size=9.5-->
w  W, h  H, and v  V . Here, W; H; V are the space of all<--size=9.5-->
possible hardware changes H; all possible software versions<--size=9.5-->
V , and all possible workload changes W. With this, the envi-<--size=9.5-->
ronment space is deﬁned as E  fW  H  V g, i.e., a subset<--size=9.5-->
of environmental conditions ee for various workloads, hard-<--size=9.5-->
ware, and environments.<--size=9.5-->

Performance. For each enviroment ee, the instances in our<--size=9.5-->
data are of the form fðc1; y1Þ; . . . ; ðcN; yNÞg, where ci is a<--size=9.5-->
vector of conﬁgurations of the ith example and it has a cor-<--size=9.5-->
responding performance measure yi 2 YS;c;e associated with<--size=9.5-->
it. We denote the performance measure associated with a<--size=9.5-->

TABLE 1<--size=9.0-->
Overview of the Real-World Subject Systems<--size=9.0-->

jCj:Number of Conﬁgurations sampled per environment, N=Number of conﬁguration options, jEj: Number of Environments, jHj: Hardware, jWj: Workloads,<--size=8.0-->
and jV j: Versions.<--size=8.0-->

KRISHNA ET AL.: WHENCE TO LEARN? TRANSFERRING KNOWLEDGE IN CONFIGURABLE SYSTEMS USING BEETLE<--size=7.0-->
2959<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:4-->

<--page_start:5-->
given conﬁguration (ci) by y ¼ fðciÞ. We consider the prob-<--size=9.5-->
lem of ﬁnding the near-optimal conﬁgurations (c) such that<--size=9.5-->
fðcÞ is better than other conﬁgurations in CA;e, i.e.,<--size=9.5-->

fðcÞ 
 fðcÞ 8c 2 CA;h;w;v n c<--size=9.5-->
fðcÞ  fðcÞ 8c 2 CA;h;w;v n c<--size=9.5-->

Bellwethers. In the context of performance optimization, the<--size=9.5-->
bellwether effect states that: For a conﬁgurable system, when<--size=9.5-->
performance measurements are made under different environ-<--size=9.5-->
ments, then among those environments there exists one exemplary<--size=9.5-->
environment, called the bellwether, which can be used determine<--size=9.5-->
near optimum conﬁguration for other environments for that sys-<--size=9.5-->
tem. We show that, when performing transfer learning,<--size=9.5-->
there are exemplar source environments called the bell-<--size=9.5-->
wether environment(s) (B ¼ es1; es2; . . . ; esn  E), which are<--size=9.5-->
the best source environment(s) to ﬁnd near-optimal conﬁgu-<--size=9.5-->
ration for the rest of the environments (8e 2 E n B).<--size=9.5-->

Problem Statement: The problem statement of this paper:<--size=9.5-->

Find a near-optimal conﬁguration for a target environ-<--size=9.5-->
ment (Set), by learning from the measurements (hc; yi)<--size=9.5-->
for the same system operating in different source envi-<--size=9.5-->
ronments (Ses).<--size=9.5-->

In other words, we aim to reuse the measurements from<--size=9.5-->
a system operating in an environment to optimize the same<--size=9.5-->
system operating in the different environment thereby<--size=9.5-->
reducing the number of measurements required to ﬁnd the<--size=9.5-->
near-optimal conﬁguration.<--size=9.5-->

4<--size=11.5-->
BEETLE: BELLWETHER TRANSFER LEARNER<--size=9.2-->

This section describes BEETLE, a bellwether based approach<--size=9.5-->
that ﬁnds the near-optimal conﬁguration using the knowl-<--size=9.5-->
edge in the “bellwether” environment. BEETLE can be sepa-<--size=9.5-->
rated into two main steps: (i) Discovery: ﬁnding the bellwether<--size=9.5-->
environment, and (ii) Transfer: using the bellwether environ-<--size=9.5-->
ment to ﬁnd the near-optimal conﬁguration for target envi-<--size=9.5-->
ronments. These steps will are explained in greater detail in<--size=9.5-->
Sections 4.1 and 4.2. We outline it below,<--size=9.5-->

1)<--size=9.5-->
Discovery: Leverages the existence of the bellwether<--size=9.5-->
effect to discover which of the available environments<--size=9.5-->
are best suited to be a source enviroment (known as<--size=9.5-->
the bellwether environment). To do this, BEETLE uses<--size=9.5-->
a racing algorithm to sequentially evaluate candidate<--size=9.5-->
environments [32]. In short,<--size=9.5-->
a)<--size=9.5-->
A fraction (about 10 percent) of all available data<--size=9.5-->
is sampled. A prediction model is built with<--size=9.5-->
these sampled datasets.<--size=9.5-->
b)<--size=9.5-->
Each enviroment is used as a source to build a<--size=9.5-->
prediction model and all the others are used as<--size=9.5-->
targets in a round-robin fashion.<--size=9.5-->
c)<--size=9.5-->
Performance of all the enviroments are measured<--size=9.5-->
and are statistically ranked from the best source<--size=9.5-->
environemnt to the worst. Environments with a<--size=9.5-->
poor performance (i.e., those ranked last) are<--size=9.5-->
eliminated.<--size=9.5-->
d)<--size=9.5-->
For the remaining enviroments, another 10 percent<--size=9.5-->
of the samples are added and the steps (a)–(c) are<--size=9.5-->
repeated.<--size=9.5-->

e)<--size=9.5-->
When the ranking order doesn’t change for a<--size=9.5-->
ﬁxed number of repeats, we terminate the process<--size=9.5-->
and nominate the best ranked enviroment(s) as<--size=9.5-->
the bellwether.<--size=9.5-->
2)<--size=9.5-->
Transfer: Next, to perform transfer learning, we just<--size=9.5-->
use these bellwether environments to train a perfor-<--size=9.5-->
mance prediction model with regression trees [33].<--size=9.5-->
We conjecture that once a bellwether source environment is<--size=9.5-->
identiﬁed, it is possible to build a simple transfer model<--size=9.5-->
without any complex methods and still be able to discover<--size=9.5-->
near-optimal conﬁgurations in a target environment.<--size=9.5-->

4.1<--size=10.0-->
Discovery: Finding Bellwether Environments<--size=10.0-->
In the previous work on bellwethers [18], the discovery pro-<--size=9.5-->
cess involved a round-robin experimentation comprised of<--size=9.5-->
the following steps:<--size=9.5-->

1)<--size=9.5-->
Pick an enviroment eei from the space of all available<--size=9.5-->
enviroments, i.e., eei 2 E.<--size=9.5-->
2)<--size=9.5-->
Use eej as a source to build a prediction model.<--size=9.5-->
3)<--size=9.5-->
Using all the other enviroments eej 2 E and eej 6¼ eei as<--size=9.5-->
the target, determine the prediction performance of eei.<--size=9.5-->
4)<--size=9.5-->
Next, repeat the steps by choosing a different eei 2 E<--size=9.5-->
5)<--size=9.5-->
Finally, rank the performances of all the enviroments<--size=9.5-->
and pick the best ranked enviroment(s) as bell-<--size=9.5-->
wether(s).<--size=9.5-->
The above methodology is a form of an exhaustive<--size=9.5-->
search. While it worked for the relatively small datasets in<--size=9.5-->
[18], [21], the amount of data in this paper is sufﬁciently<--size=9.5-->
large (see Table 1) that scoring all candidates using every<--size=9.5-->
sample is too costly. More formally, let us say that we have<--size=9.5-->
M candidate enviroments with N measurements each. The<--size=9.5-->
classical approach, described above, will construct M mod-<--size=9.5-->
els. If we assume that the model construction time is a func-<--size=9.5-->
tion of number of samples fðNÞ, then for one round in the<--size=9.5-->
round-robin, the computation time will OðM  fðNÞÞ. Since<--size=9.5-->
this is repeated M times for each enviroment, the total<--size=9.5-->
computational complexity is OðM2 fðnÞÞ. When M and/or<--size=9.5-->
N is/are extremely large, it becomes necessary to seek alter-<--size=9.5-->
native methods. Therefore, in this paper, we use a racing<--size=9.5-->
algorithm to achieve computational speedups.<--size=9.5-->

Instead of evaluating every available instance to deter-<--size=9.5-->
mine the best source enviroment, Racing algorithms take the<--size=9.5-->
following steps:<--size=9.5-->

<--size=9.5-->
Sample a small fraction of instances from the original<--size=9.5-->
enviroments to minimize computational costs.<--size=9.5-->
<--size=9.5-->
Evaluate the performance of enviroments statistically.<--size=9.5-->
<--size=9.5-->
Discarded<--size=9.5-->
the<--size=9.5-->
enviroments<--size=9.5-->
with<--size=9.5-->
the<--size=9.5-->
poorest<--size=9.5-->
performance.<--size=9.5-->
<--size=9.5-->
Repeated the process with the remaining datasets<--size=9.5-->
with slightly larger sample size.<--size=9.5-->
Fig. 3a shows how BEETLE eliminates inferior environ-<--size=9.5-->
ments at every iteration (thus reducing the overall num-<--size=9.5-->
ber of environments evaluated). Since each iteration only<--size=9.5-->
uses a small sample of the available data, the model<--size=9.5-->
building time also reduces signiﬁcantly. It has been<--size=9.5-->
shown that racing algorithms are extremely effective in<--size=9.5-->
model selection when the size of the data is arbitrarily<--size=9.5-->
large [32], [34].<--size=9.5-->

2960<--size=7.0-->
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 47, NO. 12, DECEMBER 2021<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:5-->

<--page_start:6-->
In Fig. 3b, we illustrate the discovery of the bellwether<--size=9.5-->
environments with an example. Here, there are two groups<--size=9.5-->
of environments:<--size=9.5-->

(i)<--size=9.5-->
Group 1: Environments e1;e2; ... ;e7, for which perfor-<--size=9.5-->
mance measurements have been gathered. One or more<--size=9.5-->
these environment(s) are potentially bellwether(s).<--size=9.5-->
(ii)<--size=9.5-->
Group 2: Environments e8; e9; . . . ; e12, these represent<--size=9.5-->
the target environments, for which need to deter-<--size=9.5-->
mine an optimal conﬁguration.<--size=9.5-->
In the discovery process, BEETLE’s objective is to ﬁnd<--size=9.5-->
bellwethers from among the environments in Group 1. And,<--size=9.5-->
later in the Transfer phase, we use the bellwether enviro-<--size=9.5-->
ments to ﬁnd the near-optimal conﬁguration for the target<--size=9.5-->
environments from Group 2. Note that, for the enviroments<--size=9.5-->
in Group 2, we do have to make any measurements regarding<--size=9.5-->
it’s performance. Having found bellwether enviroment(s)<--size=9.5-->
from Group 1, it is sufﬁcient to just use the bellwether<--size=9.5-->

enviroment(s) to predict the optimal conﬁgurations for the<--size=9.5-->
enviroments in Group 2.<--size=9.5-->

Fig. 3d outlines a pseudocode for the algorithm used to<--size=9.5-->
ﬁnd bellwethers. The key steps are listed below:<--size=9.5-->

<--size=9.5-->
Lines 3–5: Randomly sample a small subset of conﬁg-<--size=9.5-->
urations from the source environments. The size of<--size=9.5-->
the subset (of conﬁgurations) is controlled by a pre-<--size=9.5-->
deﬁned parameter frac, which deﬁnes the percent of<--size=9.5-->
conﬁgurations to be sampled in each iteration.<--size=9.5-->
<--size=9.5-->
Line 6–7: Calculate sampling cost for the conﬁgurations.<--size=9.5-->
<--size=9.5-->
Line 8–9: Use the sampled conﬁgurations from each<--size=9.5-->
environment as a source build a prediction model with<--size=9.5-->
regression trees. For all the remaining enviroments,<--size=9.5-->
this regression tree model is used to predict for opti-<--size=9.5-->
mum conﬁguration. After using every enviroment as a<--size=9.5-->
source, the environments are ranked from best to worst<--size=9.5-->
using the evaluation criteria discussed in Section 6.2.<--size=9.5-->

<--image width=3850.0 height=1881.0-->
Fig. 3. BEETLE framework and Pseudocode.<--size=8.0-->

KRISHNA ET AL.: WHENCE TO LEARN? TRANSFERRING KNOWLEDGE IN CONFIGURABLE SYSTEMS USING BEETLE<--size=7.0-->
2961<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:6-->

<--page_start:7-->
<--size=9.5-->
Line 10–14: We check to see if the rankings of the<--size=9.5-->
enviroments have changed since the last iteration. If<--size=9.5-->
not, then a “life” is lost. We go back to Line 3 and<--size=9.5-->
repeat the process. When all lives are expired, or we<--size=9.5-->
run out of the budget, the search process terminates.<--size=9.5-->
This acts as an early stopping criteria, we need not<--size=9.5-->
sample more data if those samples do not help in<--size=9.5-->
improving the outcome.<--size=9.5-->
<--size=9.5-->
Line 15–17: If there is some change in the rankings,<--size=9.5-->
then new conﬁguration samples are informative and<--size=9.5-->
the environments that are ranked last are eliminated.<--size=9.5-->
These environments are not able to ﬁnd near-optimal<--size=9.5-->
conﬁgurations for the other environments and there-<--size=9.5-->
fore cannot be bellwethers.<--size=9.5-->
<--size=9.5-->
Line 18: Once we have exhausted all the lives or the<--size=9.5-->
sampling budget, we simply return the source proj-<--size=9.5-->
ect with the best rank. These would be the bellwether<--size=9.5-->
enviroments.<--size=9.5-->
On line 6–7 we measure the sampling cost. In our case,<--size=9.5-->
we use the number of samples as a proxy for cost. This is<--size=9.5-->
because<--size=9.5-->
each<--size=9.5-->
measurement<--size=9.5-->
consumes<--size=9.5-->
computational<--size=9.5-->
resources, which in turn has a monetary cost. Therefore, it<--size=9.5-->
is a commonplace to set a budget and sample such that the<--size=9.5-->
budget is honored. Our choice of using the number of<--size=9.5-->
measurements as a cost measure was an engineering judg-<--size=9.5-->
ment; this can be replaced by any user-deﬁned cost func-<--size=9.5-->
tion such as (1) the actual cost, or (2) the wallclock time.<--size=9.5-->
The accuracy of either of the above is dependent on the<--size=9.5-->
business context. If one is either constrained by the runtime<--size=9.5-->
or there is a large variance in the measurements of time per<--size=9.5-->
conﬁguration, then the wallclock time might be a more rea-<--size=9.5-->
sonable measure. On the other hand, if the cost of measure-<--size=9.5-->
ments is the limiting factor, it makes sense to use the actual<--size=9.5-->
measurement cost. Using the number of samples encom-<--size=9.5-->
passes these two factors since it both costs more money and<--size=9.5-->
takes time to obtain more samples. In the ideal case, we<--size=9.5-->
would like to have performance measurements for all pos-<--size=9.5-->
sible conﬁgurations of a software system. But this is not<--size=9.5-->
practical because certain systems have over 250unique con-<--size=9.5-->
ﬁgurations (see Table 1).<--size=9.5-->

It is entirely possible for the FindBellwether method to<--size=9.5-->
identify multiple bellwethers (e.g., in the case of Fig. 3b the<--size=9.5-->
bellwethers were e1 and e2). When mutliple bellwethers are<--size=9.5-->
found, we may use (a) any one of the bellwether enviro-<--size=9.5-->
ments at random, (b) use all the enviroments, or (c) use heu-<--size=9.5-->
ristics based on human intuition. In this paper, we pick one<--size=9.5-->
enviroment from among the bellwethers at random. As<--size=9.5-->
long as the chosen project is among the bellwether enviro-<--size=9.5-->
ments, the results remain unchanged.<--size=9.5-->

The BEETLE approach assumes that a ﬁxed set of enviro-<--size=9.5-->
ments exist from which we pick one or more bellwethers.<--size=9.5-->
But, approach would work just as well where new measure-<--size=9.5-->
ments from new enviroments are added. Speciﬁcally, when<--size=9.5-->
more environments are added into a project, it is possible<--size=9.5-->
that the newly added environment could be the bellwether.<--size=9.5-->
Therefore, we recommend repeating FindBellwether method<--size=9.5-->
prior to using the new enviroment. Note that, repeating<--size=9.5-->
FindBellwether for new enviroments would add minimal<--size=9.5-->
computational overhead since the measurements have<--size=9.5-->
already been made for the new enviroments. Also note that,<--size=9.5-->
this approach of revisiting FindBellwether on availability of<--size=9.5-->

new data, has been previously been proposed in other<--size=9.5-->
domains in software engineering [18], [21].<--size=9.5-->

4.2<--size=10.0-->
Transfer: Using the Bellwether Environments<--size=10.0-->
Once the bellwether environment is identiﬁed, it can be used<--size=9.5-->
to ﬁnd the near-optimal conﬁgurations of target environ-<--size=9.5-->
ments. As shown in Fig. 3c, FindBellwether eliminates enviro-<--size=9.5-->
ments that are not potentially bellwethers and returns only<--size=9.5-->
the candidate bellwether enviroments. For the remaining tar-<--size=9.5-->
get enviroments, we use the model built with the bellwether<--size=9.5-->
enviroments to identify the near optimal conﬁgurations.<--size=9.5-->

Fig. 3e outlines the pseudocode used to perform the<--size=9.5-->
transfer. The key steps are listed below:<--size=9.5-->

<--size=9.5-->
Line 9-10: We use the FindBellwether from Fig. 3d to<--size=9.5-->
identify the bellwether enviroments.<--size=9.5-->
<--size=9.5-->
Line 13-14: If there exists more than one bellwether,<--size=9.5-->
we randomly chose one among them be used as the<--size=9.5-->
bellwether enviroment.<--size=9.5-->
<--size=9.5-->
Line 15-16: The conﬁgurations from the bellwether and<--size=9.5-->
their corresponding performance measures are used<--size=9.5-->
to build a prediction model using regression trees.<--size=9.5-->
<--size=9.5-->
Line 17-18: Predict the performances of various con-<--size=9.5-->
ﬁgurations from the target enviroment.<--size=9.5-->
<--size=9.5-->
Line 19-20: Return the best conﬁguration for the<--size=9.5-->
target.<--size=9.5-->
Note that, on Line 10, we use regression trees to make pre-<--size=9.5-->
dictions. It has been the most preferred prediction algorithm<--size=9.5-->
in this domain [7], [10], [26]. This is primarily because much<--size=9.5-->
of the data used in this domain are a mixture numerical and<--size=9.5-->
categorical attributes. Given conﬁguration measurement in<--size=9.5-->
the form fðci; yiÞg, ci is a vector of categorical/numeric val-<--size=9.5-->
ues and yi is a continuous numeric value. For such data,<--size=9.5-->
regression trees are the best suited prediction algorithms [12],<--size=9.5-->
[24], [27], [30].<--size=9.5-->

In terms of computational complexity in comparison<--size=9.5-->
with previous methods [18], [21], BEETLE offers noticible<--size=9.5-->
speedups. Given that we have M enviroments and n meas-<--size=9.5-->
urements in each enviroment, we may categorize the speed-<--size=9.5-->
ups into the following cases:<--size=9.5-->

<--size=9.5-->
Best Case: Here, we expect the racing algorithm of BEE-<--size=9.5-->
TLE to eliminate atleast half of the non-bellwether<--size=9.5-->
enviroments at every iteration. This gives us a recur-<--size=9.5-->
rence relation of TðMÞ ¼ TðM=2Þ þ fðnÞ, which gives<--size=9.5-->
us a best case complexity of Oðlog2ðMÞ  fðnÞÞ.<--size=9.5-->
<--size=9.5-->
Worst Case: Here, we expect the racing algorithm of<--size=9.5-->
BEETLE to eliminate just one non-bellwether enviro-<--size=9.5-->
ment at every iteration. This gives us a recurrence<--size=9.5-->
relation of TðMÞ ¼ TðM 1Þ þ fðnÞ, which gives us<--size=9.5-->
a worst case complexity of OðM2 fðnÞÞ. Note that<--size=9.5-->
this worst case is same as the complexity of [18], [21].<--size=9.5-->
In practice, we note that the average case speedup is<--size=9.5-->
somewhere in between that of the best case (i.e., Oðlog2ðMÞ <--size=9.5-->
fðnÞÞ) and the worst case (i.e., OðM2 fðnÞÞ).<--size=9.5-->

5<--size=11.5-->
OTHER TRANSFER LEARNING METHODS<--size=11.5-->

This section describes the methods we use to compare BEE-<--size=9.5-->
TLE against. These alternatives are (a) two state-of-the-<--size=9.5-->
art transfer learners for performance optimization: Valov<--size=9.5-->

2962<--size=7.0-->
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 47, NO. 12, DECEMBER 2021<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:7-->

<--page_start:8-->
et al. [24] and Jamshidi et al. [25]; and (b) a non-transfer learner:<--size=9.5-->
Nair et al. [12].<--size=9.5-->

5.1<--size=10.0-->
Transfer Learning With Linear Regression<--size=10.0-->
Valov et al. [24] proposed an approach for transferring perfor-<--size=9.5-->
mance models of software systems across platforms with dif-<--size=9.5-->
ferent hardware settings. The method (as shown in Fig. 4(a))<--size=9.5-->
consists of the following two components:<--size=9.5-->

<--size=9.5-->
Performance prediction model: The conﬁgurations on a<--size=9.5-->
source hardware are sampled using Sobol sampling.<--size=9.5-->
The number of conﬁgurations is given by T  Nf,<--size=9.5-->
where T ¼ 3; 4; 5 is the training coefﬁcient and Nf is<--size=9.5-->
the number of conﬁguration options. These conﬁgu-<--size=9.5-->
rations are used to construct a Regression Tree model.<--size=9.5-->
<--size=9.5-->
Transfer Model: To transfer the predictions from the<--size=9.5-->
source to the target, a linear regression model is<--size=9.5-->
used since it was found to provide good approxima-<--size=9.5-->
tions of the transfer function. To construct this<--size=9.5-->
model, a small number of random conﬁgurations are<--size=9.5-->
obtained from both the source and the target. Note that<--size=9.5-->
this is a shortcoming since, without making some<--size=9.5-->
preliminary measurements on the target, one cannot<--size=9.5-->
begin to perform transfer learning.<--size=9.5-->

5.2<--size=10.0-->
Transfer Learning With Gaussian Process<--size=10.0-->
Jamshidi et al. [25] took a slightly different approach to<--size=9.5-->
transfer learning. They used Multi-Task Gaussian Processes<--size=9.5-->
(GP) to ﬁnd the relatedness between the performance meas-<--size=9.5-->
ures in source and the target. The relationships between<--size=9.5-->
input conﬁgurations were captured in the GP model using a<--size=9.5-->
covariance matrix that deﬁned the kernel function to con-<--size=9.5-->
struct the Gaussian processes model. To encode the relation-<--size=9.5-->
ships between the measured performance of the source and<--size=9.5-->
the target, a scaling factor is used with the above kernel.<--size=9.5-->
The new kernel function is deﬁned as follows:<--size=9.5-->

kðs; t; fðsÞ; fðtÞÞ ¼ ktðs; tÞ  kxxðfðsÞ; fðtÞÞ;<--size=9.5-->
(1)<--size=9.5-->

where ktðs; tÞ represents the multiplicative scaling factor.<--size=9.5-->
ktðs; tÞ is given by the correlation between source f(s)<--size=9.5-->
and target f(t) function, while kxx is the covariance func-<--size=9.5-->
tion for input environments (s & t). The essence of this<--size=9.5-->
(summarized in Fig. 4(a)) method is that the kernel<--size=9.5-->

captures the interdependence between the source and<--size=9.5-->
target environments.<--size=9.5-->

5.3<--size=10.0-->
Non-Transfer Learning Performance<--size=10.0-->
Optimization<--size=10.0-->
A performance optimization model with no transfer was<--size=9.5-->
proposed by Nair et al. [12] in FSE ’17. It works as follows:<--size=9.5-->

1)<--size=9.5-->
Sample a small set of measurements of conﬁgura-<--size=9.5-->
tions from the target environment.<--size=9.5-->
2)<--size=9.5-->
Construct performance model with regression trees.<--size=9.5-->
3)<--size=9.5-->
Predict for near-optimal conﬁgurations.<--size=9.5-->
The key distinction here is that unlike transfer learners,<--size=9.5-->
that use a different source environment to build to predict<--size=9.5-->
for near-optimal conﬁgurations in a target environment, a<--size=9.5-->
non-transfer method such as this uses conﬁgurations from<--size=9.5-->
within the target environment to predict for near-optimal<--size=9.5-->
conﬁgurations.<--size=9.5-->

6<--size=11.5-->
EXPERIMENTAL SETUP<--size=11.5-->

6.1<--size=10.0-->
Subject Systems<--size=10.0-->
In this study, we selected ﬁve conﬁgurable software systems<--size=9.5-->
from different domains, with different functionalities, and<--size=9.5-->
written in different programming languages. We selected<--size=9.5-->
these real-world software systems since their characteristics<--size=9.5-->
cover a broad spectrum of scenarios. Brieﬂy,<--size=9.5-->

<--size=9.5-->
SPEAR is an industrial strength bit-vector arithmetic<--size=9.5-->
decision procedure and Boolean satisﬁability (SAT)<--size=9.5-->
solver. It is designed for proving software veriﬁca-<--size=9.5-->
tion conditions, and it is used for bug hunting. It con-<--size=9.5-->
sists of a binary conﬁguration space with 14 options<--size=9.5-->
with 214or 16,384 conﬁgurations. We measured how<--size=9.5-->
long it takes to solve an SAT problem in all 214con-<--size=9.5-->
ﬁgurations in 10 environments.<--size=9.5-->
<--size=9.5-->
X264 is a video encoder that compresses video ﬁles<--size=6.6-->
and has 16 conﬁgurations options to adjust output<--size=9.5-->
quality, encoder types, and encoding heuristics. Due<--size=9.5-->
to the cost of sampling the entire conﬁguration<--size=9.5-->
space, we randomly sample 4,000 conﬁgurations in<--size=9.5-->
21 environments.<--size=9.5-->
<--size=9.5-->
SQLITE is a lightweight relational database manage-<--size=9.5-->
ment system, which has 14 conﬁguration options to<--size=9.5-->

Fig. 4. Pseudocodes of other transfer learning methods.<--size=8.0-->

KRISHNA ET AL.: WHENCE TO LEARN? TRANSFERRING KNOWLEDGE IN CONFIGURABLE SYSTEMS USING BEETLE<--size=7.0-->
2963<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:8-->

<--page_start:9-->
change indexing and features for size compression.<--size=9.5-->
Due to the cost of sampling and a limited budget, we<--size=9.5-->
use 1,000 randomly selected conﬁgurations in 15 dif-<--size=9.5-->
ferent environments.<--size=9.5-->
<--size=9.5-->
SAC is a compiler for high-performance computing.<--size=9.5-->
The SaC compiler implements a large number of<--size=9.5-->
high-level and low-level optimizations to tune pro-<--size=9.5-->
grams for efﬁcient parallel executions. It has 50 con-<--size=9.5-->
ﬁguration options to control optimization options.<--size=9.5-->
We measure the execution time of the program for<--size=9.5-->
846 conﬁgurations in 5 enviroments.<--size=9.5-->
<--size=9.5-->
STORM is a distributed stream processing framework<--size=9.5-->
which is used for data analytics. We measure the<--size=9.5-->
latency of the benchmark in 2,048 randomly selected<--size=9.5-->
conﬁgurations in 4 environments.<--size=9.5-->
Table 1 lists the details of the software systems used in this<--size=9.5-->
paper. Here, jNj is the number of conﬁguration options avail-<--size=9.5-->
able in the software system. If the options for each conﬁgura-<--size=9.5-->
tion is binary, then there can be as much as 2jNjpossible<--size=9.5-->
conﬁgurations for a given system,1since it is not possible for<--size=9.5-->
us measure the performance of all possible conﬁgurations, we<--size=9.5-->
measure the performance of a subset of the 2jNjsamples, this<--size=9.5-->
subset is denoted by jCj. The performance of each of the jCj<--size=9.5-->
conﬁgurations are measured under different hardware (H),<--size=9.5-->
workloads (W), and software versions (V ). A unique combi-<--size=9.5-->
nation of H;W;V constitutes an enviroment which is denoted<--size=9.5-->
by E. Note that, measuring the performance of jCj conﬁgura-<--size=9.5-->
tions in each of the jEj enviroments can be very costly and<--size=9.5-->
time consuming. Therefore, instead of all combinations of<--size=9.5-->
H  W  V , we measure the performance in only a subset of<--size=9.5-->
the enviroments (the total number is denoted by jEj).<--size=9.5-->

6.2<--size=10.0-->
Evaluation Criterion<--size=10.0-->
Typically, performance models are evaluated based on<--size=9.5-->
accuracy or error using measures such as Mean Magnitude of<--size=9.5-->
Relative error (abbrv. MMRE) which is given by:<--size=9.5-->

MMRE ¼ jpredicted actualj<--size=9.5-->

actual<--size=9.5-->
 100:<--size=9.5-->

It has recently been shown that exact measures like MMRE<--size=9.5-->
can be somewhat misleading to assess conﬁgurations [12],<--size=9.5-->
[27], [35]. An alternative is to use rank-based metrics that com-<--size=9.5-->
pute the difference between the relative rankings of the per-<--size=9.5-->
formance scores [12], [27]. The key intuition behind relative<--size=9.5-->

rankings is that the raw accuracy (as measured by MMRE) is<--size=9.5-->
less important than the rank-ordering of conﬁgurations<--size=9.5-->
from best to worst. As long as a model can preserve the<--size=9.5-->
order of the rankings of the conﬁgurations, it is still possible<--size=9.5-->
to determine which conﬁguration is the most optimum. We<--size=9.5-->
can quantify this by measuring the differences in ranking<--size=9.5-->
between the actual rank and the predicted rank. More for-<--size=9.5-->
mally, rank-difference Rdis measured as<--size=9.5-->

Rd¼ jRankðPredictedÞ  RankðActualÞj:<--size=9.5-->

We note that rank difference is still not particularly informa-<--size=9.5-->
tive. This is because it ignores the distribution of perfor-<--size=9.5-->
mance scores and a small difference in performance measure can<--size=9.5-->
lead to a large rank difference and vice-versa [36].<--size=9.5-->

To illustrate the challenges with Rdand MMRE, consider<--size=9.5-->
the example in Fig. 5 where we are trying to ﬁnd a conﬁgura-<--size=9.5-->
tion with the minimum value. Here, although the difference<--size=9.5-->
between the predicted value and the actual value is only 0.02,<--size=9.5-->
the rank difference Rdis 90. But this does not tell us if Rd¼ 90<--size=9.5-->
is good or bad. While, in the same Fig. 5, when we calculate<--size=9.5-->
MMRE we get an error of only 22 percent, this may convey a<--size=9.5-->
false sense of accuracy. In the same example, let us say that the<--size=9.5-->
maximum value permissible is 0.11, then according to Fig. 5,<--size=9.5-->
our predicted value for the best performance (which recall is<--size=9.5-->
supposed to the lowest) is the highest permissible value of 0.11.<--size=9.5-->

Therefore, to obtain a realistic estimate of optimality of a<--size=9.5-->
conﬁguration, in this paper, we propose a measure called Nor-<--size=9.5-->
malized Absolute Residual (NAR) inspired by Generational Dis-<--size=9.5-->
tance or Inverted Generation Distance used commonly is search<--size=9.5-->
based software engineering [37], [38], [39]. It represents the<--size=9.5-->
ratio of (a) difference between the actual performance value of<--size=9.5-->
the optimal conﬁguration and the predicted performace value<--size=9.5-->
of the optimal conﬁguration, and (b) The absolute difference<--size=9.5-->
between the maximum and minimum possible performace val-<--size=9.5-->
ues. Formally<--size=9.5-->

NAR ¼<--size=9.5-->
jminðfðcÞÞ  fðcÞj<--size=9.5-->
maxðfðcÞÞ  minðfðcÞÞ  100:<--size=9.5-->
(2)<--size=9.5-->

Where minðfðcÞÞ is the value of the true minima of conﬁgura-<--size=9.5-->
tion c, fðcÞ is the predicted value of the minima, and<--size=9.5-->
maxðfðcÞÞ is the largest performance value of a conﬁguration.<--size=9.5-->
This measure is equivalent to Absolute Residual between pre-<--size=9.5-->
dicted and actual, normalized to lie between 0 to 100 percent<--size=9.5-->
(hence the name Normalized Absolute Residual or NAR).<--size=9.5-->
According to this formulation, the lower the NAR, the better.<--size=9.5-->
Reﬂecting back on Fig. 5, we see that the NAR is 100 percent<--size=9.5-->
which is exact what is expected when a predicted “minima”<--size=9.5-->
(0.11) is equal to the actual “maxima” (also 0.11).<--size=9.5-->

6.3<--size=10.0-->
Statistical Validation<--size=10.0-->
Our experiments are all subjected to inherent randomness<--size=9.5-->
introduced by sampling conﬁgurations or by a different<--size=9.5-->
source and target environments. To overcome this, we use<--size=9.5-->
30 repeated runs, each time with a different random number<--size=9.5-->
seed. The repeated runs provide us with sufﬁciently large<--size=9.5-->
sample size for statistical comparisons. Each repeated run<--size=9.5-->
collects the values of NAR.<--size=9.5-->

To rank these 30 numbers collected as above, we use the<--size=9.5-->
Scott-Knott test recommended by Mittas and Angelis [40]:<--size=9.5-->

Fig. 5. A contrived example to illustrate the challenges with MMRE and<--size=8.0-->
rank based measures.<--size=8.0-->

1. On the other hand, if there are joj possible options, then there may<--size=8.0-->
be jojNpossible conﬁgurations.<--size=8.0-->

2964<--size=7.0-->
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 47, NO. 12, DECEMBER 2021<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:9-->

<--page_start:10-->
<--size=9.5-->
A list of treatments, sorted by their mean value, are<--size=9.5-->
split at the point that maximizes the expected value<--size=9.5-->
of the difference in their mean before after the split.<--size=9.5-->
<--size=9.5-->
That split is accepted if, between the two splits, (a)<--size=9.5-->
there is a statistically signiﬁcant difference using a<--size=9.5-->
hypothesis test H, and (b) the difference between the<--size=9.5-->
two splits is not due to a small effect.<--size=9.5-->
<--size=9.5-->
Recurse on both splits if the split is acceptable.<--size=9.5-->
<--size=9.5-->
Once no more splits are found, they are “ranked”<--size=9.5-->
smallest to largest (based on their median value).<--size=9.5-->
In our work, in order to judge the statistical signiﬁcance<--size=9.5-->
we use a non-parametric bootstrap test with 95 percent conﬁ-<--size=9.5-->
dence [41]. Also, to make sure that the statistical signiﬁcance<--size=9.5-->
is not due to the presence of small effects, we use an A12<--size=9.5-->
test [42]. Brieﬂy, the A12 test measures the probability that<--size=9.5-->
one split has a lower NAR values than another. If the two<--size=9.5-->
splits are equivalent, then A12 ¼ 0:5. Likewise if A12 0:6,<--size=9.5-->
then 60 percent of the times, values of one split are signiﬁ-<--size=9.5-->
cantly smaller that the other. In such a case, it can be claimed<--size=9.5-->
that there is a signiﬁcant effect to justify the hypothesis test.<--size=9.5-->
We use these two tests (bootstrap and A12) since these are<--size=9.5-->
non-parametric and have been previously demonstrated to<--size=9.5-->
be informative [43], [44], [45], [46], [47], [48].<--size=9.5-->

7<--size=11.5-->
RESULTS<--size=11.5-->

RQ1 : Does there exist a Bellwether Environment?<--size=9.5-->

Purpose. The ﬁrst research question seeks to establish the<--size=9.5-->
presence of bellwether environments within different envi-<--size=9.5-->
ronments of a software system. If there exists a bellwether<--size=9.5-->
environment, then identifying that environment can greatly<--size=9.5-->
reduce the cost of ﬁnding a near-optimal conﬁguration for<--size=9.5-->
different environments.<--size=9.5-->

Approach. For each subject software system, we use the<--size=9.5-->
environments to perform a pair-wise comparison as follows:<--size=9.5-->

1)<--size=9.5-->
We pick one environment as a source and evaluate all<--size=9.5-->
conﬁgurations to construct a regression tree model.<--size=9.5-->
2)<--size=9.5-->
The remaining environments are used as targets. For<--size=9.5-->
every target environment, we use the regression tree<--size=9.5-->
model constructed above to predict for the best<--size=9.5-->
conﬁguration.<--size=9.5-->
3)<--size=9.5-->
Then, we measure the NAR of the predictions (see<--size=9.5-->
Section 6.2).<--size=9.5-->
4)<--size=9.5-->
Afterwards, we repeat steps 1, 2, and 3 for all the<--size=9.5-->
other source environments and gather the outcomes.<--size=9.5-->
We repeat the whole process above 30 times and use the<--size=9.5-->
Scott-Knott test to rank each environment best to worst.<--size=9.5-->

Result. Our results are shown in Fig. 6. Overall, we ﬁnd<--size=9.5-->
that there is always at least one environment (the bellwether<--size=9.5-->
environment) in all the subject systems, that is much supe-<--size=9.5-->
rior to others. Note that, STORM is an interesting case, where<--size=9.5-->
all the environments are ranked 1, which means that all the<--size=9.5-->
environments are equally useful as a bellwether environ-<--size=9.5-->
ment —in such cases, any randomly selected environment<--size=9.5-->
could serve as a bellwether. Further, we note that the vari-<--size=9.5-->
ance in the bellwether environments are much lower com-<--size=9.5-->
pared to other environments. Low variance indicates the<--size=9.5-->
low median NAR is not an effect of randomness in our<--size=9.5-->
experiments and hence increases our conﬁdence in the exis-<--size=9.5-->
tence of bellwethers.<--size=9.5-->

Please note, in this speciﬁc experiment, we use all mea-<--size=9.5-->
sured conﬁgurations (i.e., 100 percent of jCj in Table 1) to<--size=9.5-->
determine if bellwethers exist. This ensures that the exis-<--size=9.5-->
tence of bellwethers is not biased by how we sampled the<--size=9.5-->
conﬁguration space. Later, in RQ2, we will restrict our study<--size=9.5-->

Fig. 6. Median NAR of 30 repeats. Median NAR is the normalized absolute residual values as described in Equation (2), and IQR the difference<--size=8.0-->
between 75th percentile and 25th percentile found during multiple repeats. Lines with a dot in the middle (<--size=8.0-->
), show the median as a round dot within<--size=8.0-->
the IQR. All the results are sorted by the median NAR: a lower median value is better. The left-hand column (Rank) ranks the various techniques where<--size=8.0-->
lower ranks are better. Overall, we ﬁnd that there is always at least one environment, denoted in light gray, that is much superior (lower NAR) to others.<--size=8.0-->

KRISHNA ET AL.: WHENCE TO LEARN? TRANSFERRING KNOWLEDGE IN CONFIGURABLE SYSTEMS USING BEETLE<--size=7.0-->
2965<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:10-->

<--page_start:11-->
to determine what fraction of the samples would be ade-<--size=9.5-->
quate to ﬁnd the bellwethers.<--size=9.5-->

One may be tempted to argue that the answer to this ques-<--size=9.5-->
tion trivially could be answered as “yes” since it is unlikely<--size=9.5-->
that all environments exhibit identical performance and there<--size=9.5-->
will always be some environment that can make better predic-<--size=9.5-->
tions. However, observe that the environments ranked ﬁrst<--size=9.5-->
performs much better than the rest (with certain exceptions),<--size=9.5-->
and hence, the difference between the bellwether environment<--size=9.5-->
and others is not coincidental. Further, by exhaustively com-<--size=9.5-->
paring the performance of all available environments, we dem-<--size=9.5-->
onstrate that it is ill advised to randomly pick any available<--size=9.5-->
source lest we risk choosing a sub-optimal conﬁguration<--size=9.5-->
setting.<--size=9.5-->

RQ2 : How many measurements are required to discover<--size=9.5-->
bellwether environments?<--size=9.5-->

Purpose. The bellwether environments found in RQ1<--size=9.5-->
required us to use 100 percent of the measured performance<--size=9.5-->
values from all the environments.2Sampling all conﬁgura-<--size=9.5-->
tions may not be practical, since that may take an extremely<--size=9.5-->
long time [20]. Thus, we ask if we can ﬁnd the bellwether<--size=9.5-->
environments sooner using fewer samples. Further, we ask<--size=9.5-->
how many such samples are required.<--size=9.5-->

Approach. We used the racing algorithm discussed in<--size=9.5-->
Section 4.1 to incrementally sample the conﬁgurations until<--size=9.5-->
a bellwether environment has been discovered. It works as<--size=9.5-->
follows:<--size=9.5-->

1)<--size=9.5-->
We start from 1 percent of conﬁgurations from each<--size=9.5-->
environment and assume that every environment is<--size=9.5-->
a potential bellwether environment.<--size=9.5-->
2)<--size=9.5-->
Then, we increment the number of conﬁgurations in<--size=9.5-->
steps of 1 percent and measure the NAR values.<--size=9.5-->
3)<--size=9.5-->
We rank the environments and eliminate those that<--size=9.5-->
do not show much promise.<--size=9.5-->
4)<--size=9.5-->
We repeat the above steps until we cannot eliminate<--size=9.5-->
any more environments.<--size=9.5-->
When the above discover process terminates, we note that<--size=9.5-->
only a fraction of the available samples are used to discover<--size=9.5-->
the bellwether. We measure the number of samples required<--size=9.5-->
for estimating the bellwether. Further, to understand if the<--size=9.5-->
smaller sample size is sufﬁcient to identify a near-optimal<--size=9.5-->
conﬁguration, we compare the performance of the discov-<--size=9.5-->
ered bellwether environment with 100 percent with the pre-<--size=9.5-->
dicted bellwether environment using a smaller sample size.<--size=9.5-->

Result: Table 2 summarizes our ﬁndings. We ﬁnd:<--size=9.5-->

<--size=9.5-->
In all 5 cases, the racing algorithm for ﬁnding bell-<--size=9.5-->
wether terminated after using the following percent-<--size=9.5-->
age of samples:<--size=9.5-->
1)<--size=9.5-->
x264: 10.21 percent of 4,000 samples<--size=9.5-->
2)<--size=9.5-->
SQLite: 11.42 percent of 1,000 samples<--size=9.5-->
3)<--size=9.5-->
Spear: 13.79 percent of 16,384 samples<--size=9.5-->
4)<--size=9.5-->
SaC: 15.4 percent of 846 samples<--size=9.5-->
5)<--size=9.5-->
Storm: 17.40 percent of 2,048 samples<--size=9.5-->
<--size=9.5-->
Further, from Table 2, when compared with the<--size=9.5-->
NAR values obtained with using all 100 percent of<--size=9.5-->
the available samples (Columns 2 and 3) to the<--size=9.5-->
NAR values when using only the fraction required<--size=9.5-->
to ﬁnd the bellwether using the racing algorithm<--size=9.5-->
(Columns 4 and 5), we see that the difference which<--size=9.5-->
is formally is given by D% ¼ NAR100% NAR10%<--size=9.5-->
j<--size=9.5-->
j<--size=9.5-->
is very minimal. We note that these differences<--size=9.5-->
(Delta%) are:<--size=9.5-->
1)<--size=9.5-->
1 percent in SQLite;<--size=9.5-->
2)<--size=9.5-->
0 percent in Spear and Storm;<--size=9.5-->
3)<--size=9.5-->
0.55 percent in x264; and<--size=9.5-->
4)<--size=9.5-->
0.36 percent in SaC<--size=9.5-->
These results are most encouraging in that we need only<--size=9.5-->
about 10 percent of the samples to determine the bellwether:<--size=9.5-->

RQ3 : How does BEETLE compare with other non transfer<--size=8.9-->
learning based methods?<--size=9.5-->

Purpose. We explore how BEETLE compares to a non-<--size=9.5-->
transfer learning approach. For our experiment, we use the<--size=9.5-->
non-transfer performance optimizer proposed by Nair<--size=9.5-->
et al. [12]. Both BEETLE and Nair et al.’s methods seek to<--size=9.5-->
achieve the same goal—ﬁnd the optimal conﬁguration in a<--size=9.5-->
target environment. BEETLE uses conﬁgurations from a dif-<--size=9.5-->
ferent source to achieve this, whereas the non-transfer learner<--size=9.5-->
uses conﬁgurations from within the target. Please note BEE-<--size=9.5-->
TLE can use anywhere between 0–100 percent of the conﬁg-<--size=9.5-->
urations from the bellwether environment. In the previous<--size=9.5-->
RQs, we showed that 10 percent was adequate when using<--size=9.5-->
the bellwether environment.<--size=9.5-->

Approach. Our setup involves evaluating the Win/Loss<--size=9.5-->
ratio of BEETLE to the non-transfer learning algorithm while<--size=9.5-->
predicting for the optimal conﬁguration. Comparing against<--size=9.5-->
true optima, we deﬁne “win” as cases where BEETLE has a<--size=9.5-->
better (or same) NAR as the non-transfer learner. If the non-<--size=9.5-->
transfer learner has a better NAR, that counts as a “loss”.<--size=9.5-->

Result. Our results are shown in Figs. 7 and 8. In Fig. 7, the<--size=9.5-->
x-axis represents the number of conﬁgurations (expressed in<--size=9.5-->
%) to train the non-transfer learner and BEETLE, and the<--size=9.5-->
y-axis represents the number of wins/losses. We observe:<--size=9.5-->

<--size=9.5-->
Better performance: In4<--size=9.5-->

5systems, BEETLE “wins” sig-<--size=6.6-->
niﬁcantly more than it “losses”. This means that<--size=9.5-->
BEETLE is better than (or similar to) non-transfer<--size=9.5-->
learning methods.<--size=9.5-->
<--size=9.5-->
Lower cost: Regarding cost, we note that BEETLE out-<--size=9.5-->
performs<--size=9.5-->
the<--size=9.5-->
non-transfer<--size=9.5-->
learner<--size=9.5-->
signiﬁcantly,<--size=9.5-->
“winning” at conﬁgurations of 10 to 100 percent of<--size=9.5-->
the original sample size. Further, when we look at the<--size=9.5-->
trade-off between performance and number of meas-<--size=9.5-->
urements in Fig. 8, we note that BEETLE achieves a<--size=9.5-->
NAR close to zero with around 100 samples. Also, the<--size=9.5-->

TABLE 2<--size=9.0-->
Effectiveness of Source Selection Method<--size=9.0-->

Subject System<--size=7.5-->
100% Samples<--size=7.5-->
FindBellwether<--size=7.5-->
Difference (D%)<--size=7.5-->

Median<--size=7.5-->
IQR<--size=7.5-->
Median<--size=7.5-->
IQR<--size=7.5-->
Median<--size=7.5-->
IQR<--size=7.5-->

SQLite<--size=7.5-->
0.8<--size=7.5-->
1.13<--size=7.5-->
1.8<--size=7.5-->
2.48<--size=7.5-->
1.0<--size=7.5-->
1.35<--size=7.5-->
Spear<--size=7.5-->
0.1<--size=7.5-->
0.1<--size=7.5-->
0.1<--size=7.5-->
0<--size=7.5-->
0.0<--size=7.5-->
0.0<--size=7.5-->
x264<--size=7.5-->
0.35<--size=7.5-->
1.62<--size=7.5-->
0.9<--size=7.5-->
1.06<--size=7.5-->
0.55<--size=7.5-->
0.16<--size=7.5-->
Storm<--size=7.5-->
0.0<--size=7.5-->
0.0<--size=7.5-->
0.0<--size=7.5-->
0.0<--size=7.5-->
0.0<--size=7.5-->
0.0<--size=7.5-->
SaC<--size=7.5-->
0.27<--size=7.5-->
0.14<--size=7.5-->
0.63<--size=7.5-->
7.4<--size=7.5-->
0.36<--size=7.5-->
6.9<--size=7.5-->

2. Note, except for SPEAR, we only have measured a subset of all<--size=7.5-->
possible conﬁguration space since we were limited by the time and the<--size=8.0-->
cost required to make exhaustive measurements<--size=8.0-->

2966<--size=7.0-->
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 47, NO. 12, DECEMBER 2021<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:11-->

<--page_start:12-->
non-transfer learning method of Nair et al. [12] has<--size=9.5-->
signiﬁcantly larger NAR while also requiring more<--size=9.5-->
samples.<--size=9.5-->

RQ4 : How does BEETLE compare to state of the art<--size=9.4-->
methods?<--size=9.5-->

Purpose. The main motivation of this work is to show that<--size=9.5-->
the source environment can have a signiﬁcant impact on<--size=9.5-->
transfer learning. In this research question, we seek to com-<--size=9.5-->
pare BEETLE with other state-of-the-art transfer learners by<--size=9.5-->
Jamshidi et al. [25] and Valov et al. [24].<--size=9.5-->

Approach. We perform transfer learning the methods<--size=9.5-->
proposed by Valov et al. [24] and Jamshidi et al. [25]<--size=9.5-->
(see Section 5). Then we measure the NAR values and com-<--size=9.5-->
pare them statistically using Skott-Knott tests. Finally, we<--size=9.5-->
rank the methods from best to worst based on their Skott-<--size=9.5-->
Knott ranks.<--size=9.5-->

Result. Our results are shown in Fig. 9. In this ﬁgure, the<--size=9.5-->
best transfer learner is ranked 1. We note that in 4 out of 5<--size=9.5-->
cases, BEETLE performs just as well as (or better than) the<--size=9.5-->
state-of-the-art. This result is encouraging in that it points to<--size=9.5-->
a signiﬁcant impact on choosing a good source environment<--size=9.5-->
can have on the performance of transfer learners. Further,<--size=9.5-->
in Fig. 10 we compare the number of performance measure-<--size=9.5-->
ments required to construct the transfer learners (note the<--size=9.5-->
logarithmic scale on the vertical axis). Here, we note that<--size=9.5-->
BEETLE uses an order of magnitude fewer samples (13%<--size=9.5-->
on average) that the other methods. The total number of<--size=9.5-->
available samples for each software system is shown in the<--size=9.5-->
second column of Table 1 (see values corresponding to jCj).<--size=9.5-->
Based on these results, we note that BEETLE requires far<--size=9.5-->

fewer measurements compared to the other transfer-learn-<--size=9.5-->
ing methods.<--size=9.5-->

8<--size=11.5-->
DISCUSSION<--size=11.5-->

This section addresses some additional questions that may<--size=9.5-->
arise with regards to BEETLE’s real-world applicability.<--size=9.5-->

What is the effect of BEETLE on the day to day business of a<--size=9.5-->
software engineer?From an industrial perspective, BEETLE<--size=9.5-->
can be used in at least the following ways:<--size=9.5-->

<--size=9.5-->
Consider an organization which has to optimize<--size=9.5-->
their software system for different clients (who have<--size=9.5-->
different workload and hardware—different AWS<--size=9.5-->
subscriptions). While on-boarding new clients, the<--size=9.5-->
company might not be able to afford to invest exten-<--size=9.5-->
sive resources in ﬁnding the near-optimal conﬁgura-<--size=9.5-->
tion to appease the client. State-of-the-art transfer<--size=9.5-->
learning techniques would expect the organization<--size=9.5-->
to provide a source workload (or environment) for<--size=9.5-->
this task. But without a subject matter expert (SME)<--size=9.5-->
with the relevant knowledge, it is hard for humans<--size=9.5-->
to select a suitable source. BEETLE removes the need<--size=9.5-->
for such SMEs since it automates source selection, along<--size=9.5-->
with transferring knowledge between the source and the<--size=9.5-->
target environment.<--size=9.5-->

<--image width=2010.0 height=1010.0-->
Fig. 8. Trade-off between the quality of the conﬁgurations and the cost to<--size=8.0-->
build the model for X264. The cost to ﬁnd a good conﬁguration using bell-<--size=8.0-->
wethers is much lower than that of non-transfer-learning methods.<--size=8.0-->

Fig. 7. Win/Loss analysis of learning from the bellwether environment and target environment using Scott Knott. The x-axis represents the % of avail-<--size=8.0-->
able samples used to build a model. The y-axis is the count.<--size=8.0-->

Fig. 9. Comparison between state-of-the-art transfer learners and BEE-<--size=8.0-->
TLE. The best transfer learner is shaded gray. The “ranks” shown in the<--size=8.0-->
left-hand-side column come from the statistical analysis described in<--size=8.0-->
Section 6.3.<--size=8.0-->

KRISHNA ET AL.: WHENCE TO LEARN? TRANSFERRING KNOWLEDGE IN CONFIGURABLE SYSTEMS USING BEETLE<--size=7.0-->
2967<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:12-->

<--page_start:13-->
<--size=9.5-->
Consider an organization, which needs to migrate all<--size=9.5-->
their workload from a legacy platform to a different<--size=9.5-->
cloud platform (e.g., AWS to AZURE or vice versa).<--size=9.5-->
Such an organization now has many workloads that<--size=9.5-->
they need to optimize; however, they lack experience<--size=9.5-->
and performance measurements, on the new plat-<--size=9.5-->
form to accomplish this goal. In such cases, BEETLE<--size=9.5-->
provides a way to discover an ideal source to transfer<--size=9.5-->
knowledge to enable efﬁcient migration of workloads.<--size=9.5-->
How complex is BEETLE compared to other methods? BEE-<--size=9.5-->
TLE is among the easiest transfer learning methods cur-<--size=9.5-->
rently available. In comparison with the state-of-the-art<--size=9.5-->
methods studied here, we require only few measurements<--size=9.5-->
of software systems running under different environ-<--size=9.5-->
ments, we can build a findbellwether method that com-<--size=9.5-->
prises of an all-pairs round-robin comparison followed by<--size=9.5-->
elimination of poorly performing enviroments. Then,<--size=9.5-->
transfer learning uses one of many off-the-shelf machine<--size=9.5-->
learners to build a prediction model (here we use Regres-<--size=9.5-->
sion Trees). In this paper, we demonstrate that this method<--size=9.5-->
is just as powerful as other methods while being an order<--size=9.5-->
of magnitude cheaper in terms of the number of measure-<--size=9.5-->
ments required.<--size=9.5-->

What are the impact of different hyperparameter choices?<--size=9.5-->
With all the transfer learners and predictors discussed<--size=9.5-->
here, there are a number of internal parameters that may<--size=9.5-->
(or may not) have a signiﬁcant impact on the outcomes of<--size=9.5-->
this study. We identify two key hyperparameters that<--size=9.5-->
affect BEETLE namely, Budget and Lives. As shown in<--size=9.5-->
Fig. 3d, both these hyperparameters determine when to<--size=9.5-->
stop sampling the source and declare the bellwethers.<--size=9.5-->
These bellwethers subsequently affect transfer learning. To<--size=9.5-->
study the effect of these hyperparameters, we plot the<--size=9.5-->
trade-off between the budget and lives versus NAR. This<--size=9.5-->
is shown in Fig. 11. Here,<--size=9.5-->

<--size=9.5-->
Budget: There is discernible impact of larger budget<--size=9.5-->
on the performance of bellwethers. We note that the<--size=9.5-->
performance is directly related to the budget, i.e., as<--size=9.5-->
the budget increases the NAR value decreases (lower<--size=9.5-->
NAR values are better). This is to be expected, an<--size=9.5-->
increased<--size=9.5-->
budget<--size=9.5-->
permits<--size=9.5-->
an<--size=9.5-->
larger<--size=9.5-->
sample<--size=9.5-->
to<--size=9.5-->

construct transfer learners, thereby improving the<--size=9.5-->
likelihood of ﬁnding a near optimal solution.<--size=9.5-->
<--size=9.5-->
Lives: Although lower lives seems to correspond to<--size=9.5-->
larger NAR (worse). The relationship between the<--size=9.5-->
number of lives and NAR is less pronounced than<--size=9.5-->
that between Budget and NAR. That said, we noted<--size=9.5-->
that having 5 or lives generally corresponds to better<--size=9.5-->
NAR values. Thus, in all the experiments in this<--size=9.5-->
paper, we use 5 lives as default.<--size=9.5-->
Is BEETLE applicable in other domains? In principle, yes.<--size=9.5-->
BEETLE could be applied to any transfer learning applica-<--size=9.5-->
tion, where the choice of the source data impacts the perfor-<--size=9.5-->
mance of transfer learning. This can be applied to problems<--size=9.5-->
such as conﬁguring big data systems [3], ﬁnding suitable<--size=9.5-->
cloud conﬁguration for a workload [49], [50], conﬁguring<--size=9.5-->
hyperparameters of machine learning algorithms [51], [52],<--size=9.5-->
runtime adaptation of robotic systems [25]. In these applica-<--size=9.5-->
tions, the correct choice of source datasets using bellwethers<--size=9.5-->
can help to reduce the amount of time it takes to discover a<--size=9.5-->
near-optimal conﬁguration setting.<--size=9.5-->

Can BEETLE identify bellwethers in completely dissimilar<--size=9.5-->
environments?In theory, yes. Given a software system, BEE-<--size=9.5-->
TLE currently looks for an environment which can be used<--size=9.5-->
to ﬁnd a near-optimal conﬁguration for a majority of other<--size=9.5-->
environments for that software system. Therefore, given<--size=9.5-->
performance measurements in various environments, BEE-<--size=9.5-->
TLE can assist in discovering a suitable source environment<--size=9.5-->
to transfer knowledge across environments comprised of<--size=9.5-->
different hardware, software versions, and workloads.<--size=9.5-->

When are bellwethers ineffective?The existence of bell-<--size=9.5-->
wethers depends on the following:<--size=9.5-->

<--size=9.5-->
Metrics used: Finding bellwether using metrics that<--size=9.5-->
are not justiﬁable, may be unsuccessful, for example,<--size=9.5-->
discovering bellwethers in performance optimiza-<--size=9.5-->
tion, by measuring MMRE instead of NAR may<--size=9.5-->
fail [12].<--size=9.5-->
<--size=9.5-->
Different Software System: Bellwethers of a certain<--size=9.5-->
software system ‘A’ may not work for software sys-<--size=9.5-->
tem ‘B.’ In other words, it cannot be used for cases<--size=9.5-->
where the conﬁguration spaces across environment<--size=9.5-->
are not consistent.<--size=9.5-->
<--size=9.5-->
Different Performance Measures: Bellwether discovered<--size=9.5-->
for one performance measure (time) may not work<--size=9.5-->
for other performance measures (throughput).<--size=9.5-->

Fig. 10. Number of samples required by BEETLE (in light gary) v/s the<--size=8.0-->
other two state-of-the-art transfer learners (in gray). Note: The other two<--size=8.0-->
transfer learners require that all available data be used for transfer learn-<--size=8.0-->
ing therefore the chart shows one bar for both transfer learners.<--size=8.0-->

<--image width=2010.0 height=904.0-->
Fig. 11. The trade-off between the budget of the search, the number of<--size=8.0-->
lives, and the NAR (quality) of the solutions for x264. Performance<--size=8.0-->
depends on the budget and number of lives, i.e., as the budget increases<--size=8.0-->
the NAR value decreases; likewise, as the number lives increases, the<--size=8.0-->
NAR improves.<--size=8.0-->

2968<--size=7.0-->
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 47, NO. 12, DECEMBER 2021<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:13-->

<--page_start:14-->
9<--size=11.5-->
THREATS TO VALIDITY<--size=11.5-->

As with any empirical study, biases can affect the ﬁnal<--size=9.5-->
results. Therefore, any conclusions of this work must be<--size=9.5-->
considered with the following issues in mind:<--size=9.5-->

<--size=9.5-->
Evaluation Bias: In RQ2, RQ3 and RQ4, we have shown<--size=9.5-->
the performance of BEETLE by comparing them using<--size=9.5-->
statistical tests on their NAR to draw conclusions<--size=9.5-->
regarding their performance when compared to other<--size=9.5-->
transfer learning and non-transfer-learning learning<--size=9.5-->
methods. While those results are true, the conclusions<--size=9.5-->
are scoped by the evaluation metrics we used to write<--size=9.5-->
this paper (i.e., NAR). It is possible that with other<--size=9.5-->
measurements, there may be slightly different conclu-<--size=9.5-->
sions. This is to be explored in future research.<--size=9.5-->
<--size=9.5-->
Construct Validity: At various places in this report,<--size=9.5-->
we made engineering decisions about (e.g.,) choice<--size=9.5-->
of machine learning models (in our case decision<--size=9.5-->
tree regression), step-size for incremental sampling,<--size=9.5-->
etc. While these decisions were made using advice<--size=9.5-->
from the literature, we acknowledge that other con-<--size=9.5-->
structs might lead to other conclusions.<--size=9.5-->
<--size=9.5-->
External Validity: For this study, we have selected a<--size=9.5-->
diverse set of subject systems, and a large number<--size=9.5-->
of environment changes from the data collected by<--size=9.5-->
Jamshidi et al. [20] for their studies. The perfor-<--size=9.5-->
mance measures were gathered on known software<--size=9.5-->
environments such as AWS, Azure, and NUC.<--size=9.5-->
There is a possibility that measurement of other per-<--size=9.5-->
formance measures or availability of additional<--size=9.5-->
performance measures may result in a different out-<--size=9.5-->
come. Therefore, one has to be careful when gener-<--size=9.5-->
alizing our ﬁndings to other subject systems and<--size=9.5-->
environment changes. Even though we tried to run<--size=9.5-->
our experiment on a variety of software systems<--size=9.5-->
from different domains, we do not claim that our<--size=9.5-->
results generalize beyond the speciﬁc case studies<--size=9.5-->
explored here. That said, to enable reproducibility,<--size=9.5-->
we have shared our scripts and the gather perfor-<--size=9.5-->
mance data.<--size=9.5-->
<--size=9.5-->
Statistical Validity: To increase the validity of our<--size=9.5-->
results, we applied Scott-Knott tests (which in turn<--size=9.5-->
comprises of two statistical tests, bootstrap, and the<--size=9.5-->
a12). Hence, anytime in this paper, we reported that<--size=9.5-->
“X was different from Y”, then that report was based<--size=9.5-->
on Scott-Knott tests.<--size=9.5-->
<--size=9.5-->
Sampling Bias: Our conclusions are based on the per-<--size=9.5-->
formance measure of the ﬁve software systems col-<--size=9.5-->
lected by Jamshidi et al. [20] for their studies.<--size=9.5-->
Different initial samples may have lead to different<--size=9.5-->
conclusions. That said, we note that our samples are<--size=9.5-->
sufﬁciently large, so we have some conﬁdence that<--size=9.5-->
these samples represent an interesting range of con-<--size=9.5-->
ﬁgurations and their performances. As evidenced by<--size=9.5-->
our results that are remarkably stable over 30<--size=9.5-->
repeated runs.<--size=9.5-->
<--size=9.5-->
Learner Bias: There are various models used in per-<--size=9.5-->
formance optimization such as Gaussian Process [25],<--size=9.5-->
Regression<--size=9.5-->
Trees<--size=9.5-->
[7],<--size=9.5-->
[26],<--size=9.5-->
[27],<--size=9.5-->
and<--size=9.5-->
Bagging,<--size=9.5-->
Random<--size=9.5-->
Forest,<--size=9.5-->
and<--size=9.5-->
Support<--size=9.5-->
Vector<--size=9.5-->
Machines<--size=9.5-->
(SVMs) [5]. It is possible that changing the learner<--size=9.5-->

used may change our ﬁndings. However, we strive<--size=9.5-->
to minimize the uncertainty by choosing Decision<--size=9.5-->
Tree Regressor, which is the machine learning algo-<--size=9.5-->
rithm that has most consistently been used in the<--size=9.5-->
domain of performance modeling and optimiza-<--size=9.5-->
tion [7], [26], [27]. Further, we have made available<--size=9.5-->
our replication package that enables one to replace<--size=9.5-->
Decision Tree with any other machine learning<--size=9.5-->
model quickly.<--size=9.5-->

10<--size=11.5-->
RELATED WORK<--size=11.5-->

Performance Optimization. Modern software systems come<--size=9.5-->
with a large number of conﬁguration options. For example,<--size=9.5-->
in APACHE (a popular web server) there are around 600 dif-<--size=9.5-->
ferent conﬁguration options and in HADOOP, as of version<--size=9.5-->
2.0.0, there are around 150 different conﬁguration options,<--size=9.5-->
and the number of options is constantly growing [1]. These<--size=9.5-->
conﬁguration options control the internal properties of the<--size=9.5-->
system such as memory and response times. Given the large<--size=9.5-->
number of conﬁgurations, it becomes increasingly difﬁcult<--size=9.5-->
to assess the impact of the conﬁguration options on the sys-<--size=9.5-->
tem’s performance. To address this issue, a common prac-<--size=9.5-->
tice is to employ performance prediction models to estimate<--size=9.5-->
the performance of the system under these conﬁgura-<--size=9.5-->
tions [26], [31], [53], [54], [55], [56]. To leverage the full bene-<--size=9.5-->
ﬁt of a software system and its features, researchers<--size=9.5-->
augment performance prediction models to enable perfor-<--size=9.5-->
mance optimization [8], [12].<--size=9.5-->

Performance optimization is an essential challenge in soft-<--size=9.5-->
ware engineering. As shown in the next few paragraphs, this<--size=9.5-->
problem<--size=9.5-->
has<--size=9.5-->
attracted<--size=9.5-->
much<--size=9.5-->
recent<--size=9.5-->
research<--size=9.5-->
interest.<--size=9.5-->
Approaches that use meta-heuristic search algorithms to<--size=9.5-->
explore the conﬁguration space of Hadoop for high-perform-<--size=9.5-->
ing conﬁgurations have been proposed [9]. It has been<--size=9.5-->
reported that such meta-heuristic search can ﬁnd conﬁgura-<--size=9.5-->
tions options that perform signiﬁcantly better than baseline<--size=9.5-->
default conﬁgurations. In other work, a control-theoretic<--size=9.5-->
framework called SmartConf to automatically set and dynami-<--size=9.5-->
cally adjust performance-sensitive conﬁgurations to optimize<--size=9.5-->
conﬁguration options [57]. For the speciﬁc case of deep learn-<--size=9.5-->
ing clusters, a job scheduler called Optimus has been devel-<--size=9.5-->
oped to determine conﬁguration options that optimize<--size=9.5-->
training speed and resource allocations [59]. Performance<--size=9.5-->
optimization has also been extensively explored in other<--size=9.5-->
domains such as Systems Research [60], [61] and Cloud Com-<--size=9.5-->
puting [11], [49], [50], [61], [62].<--size=9.5-->

Much of the performance optimization tasks introduced<--size=9.5-->
above require access to measurements of the software sys-<--size=9.5-->
tem<--size=9.5-->
under<--size=9.5-->
various<--size=9.5-->
conﬁguration<--size=9.5-->
settings.<--size=9.5-->
However,<--size=9.5-->
obtaining these performance measurements can cost a sig-<--size=9.5-->
niﬁcant amount of time and money. For example, in one of<--size=9.5-->
the software systems studied here (X264), it takes over<--size=9.5-->
1,536 hours to obtain performance measurements for 11<--size=9.5-->
out the 16 possible conﬁguration options [24]. This is in<--size=9.5-->
addition<--size=9.5-->
to<--size=9.5-->
other<--size=9.5-->
time-consuming<--size=9.5-->
tasks<--size=9.5-->
involved<--size=9.5-->
in<--size=9.5-->
commissioning these systems such as setup, tear down, etc.<--size=9.5-->
Further, making performance measurements can cost an<--size=9.5-->
exorbitant amount of money, e.g., it cost several thousand<--size=9.5-->
dollars to obtain of 2,048 conﬁgurations on X264 deployed<--size=9.5-->
in AWS c4.large.<--size=9.5-->

KRISHNA ET AL.: WHENCE TO LEARN? TRANSFERRING KNOWLEDGE IN CONFIGURABLE SYSTEMS USING BEETLE<--size=7.0-->
2969<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:14-->

<--page_start:15-->
Transfer Learning. When a software system is deployed in<--size=9.5-->
a new environment, not every user can afford to repeat the<--size=9.5-->
costly process of building a new performance model to ﬁnd<--size=9.5-->
an optimum conﬁguration for that new environment.<--size=9.5-->
Instead, researchers propose the use of transfer learning to<--size=9.5-->
reuse<--size=9.5-->
the<--size=9.5-->
measurements<--size=9.5-->
made<--size=9.5-->
for<--size=9.5-->
previous<--size=9.5-->
environ-<--size=9.5-->
ments [24], [25], [63], [64]. Jamshidi et al. [25], conducted a<--size=9.5-->
preliminary exploratory study of transfer learning in perfor-<--size=9.5-->
mance optimization to identify transferable knowledge<--size=9.5-->
between a source and a target environment, ranging from<--size=9.5-->
easily exploitable relationships to more subtle ones. They<--size=9.5-->
demonstrated that information about inﬂuential conﬁgura-<--size=9.5-->
tion options could be exploited in transfer learning and that<--size=9.5-->
knowledge about performance behavior can be transferred.<--size=9.5-->

Following this, a number of transfer learning methods<--size=9.5-->
were developed to predict for the optimum conﬁgurations<--size=9.5-->
in a new target environment, using the performance meas-<--size=9.5-->
ures of another source environment as a proxy. Several<--size=9.5-->
researchers have shown that transfer learning can decrease<--size=9.5-->
the cost of learning signiﬁcantly [20], [24], [25], [64].<--size=9.5-->

All transfer learning methods place implicit faith in the<--size=9.5-->
quality of the source. A poor source can signiﬁcantly deteri-<--size=9.5-->
orate the performance of transfer learners.<--size=9.5-->

Source Selection With Bellwethers.It is advised that the<--size=9.5-->
source used for transfer learning must be chosen with care to<--size=9.5-->
ensure optimum performance [52], [65], [66]. An incorrect<--size=9.5-->
choice of the source may result in the all too common negative<--size=9.5-->
transferphenomenon [52], [67], [68], [69]. A negative transfer<--size=9.5-->
can be particularly damaging in that it often leads to perfor-<--size=9.5-->
mance degradation [20], [52]. A preferred way to avoid nega-<--size=9.5-->
tive transfer is with source selection. Many methods have been<--size=9.5-->
proposed for identifying a suitable source for transfer learn-<--size=9.5-->
ing [18], [21], [52]. Of these, source selection using the bell-<--size=9.5-->
wether effect is one of the simplest. It has been effective in<--size=9.5-->
several domains of software engineering [18], [22], [23].<--size=9.5-->

Besides negative transfer, previous approaches suffer<--size=9.5-->
from a lack of scalability. For example, Google Visor [65]<--size=9.5-->
Jamshidi et al. [25] rely on a Gaussian process which known<--size=9.5-->
to not scaling to large amounts of data in high dimensional<--size=9.5-->
spaces [70] Accordingly, in this work, we introduce the<--size=9.5-->
notion of source selection with bellwether effect for transfer<--size=9.5-->
learning in performance optimization. With this, we develop<--size=9.5-->
a Bellwether Transfer Learner called BEETLE. We show that,<--size=9.5-->
for performance optimization, BEETLE can outperform both<--size=9.5-->
non-transfer and the transfer learning methods.<--size=9.5-->

11<--size=11.5-->
CONCLUSION<--size=11.5-->

Our approach, BEETLE, exploits the bellwether effect—there<--size=9.5-->
are one or more bellwether environments which can be used<--size=9.5-->
to ﬁnd good conﬁgurations for the rest of the environments.<--size=9.5-->
We also propose a new transfer learning method, called BEE-<--size=9.5-->
TLE, which exploits this phenomenon. As shown in this<--size=9.5-->
paper, BEETLE can quickly identify the bellwether environ-<--size=9.5-->
ments with only a few measurements ( 10%) and use it to<--size=9.5-->
ﬁnd the near-optimal solutions in the target environments.<--size=9.5-->
Further, after extensive experiments with ﬁve highly-conﬁg-<--size=9.5-->
urable systems demonstrating, we show that BEETLE:<--size=9.5-->

<--size=9.5-->
Identiﬁes<--size=9.5-->
suitable<--size=9.5-->
sources<--size=9.5-->
to<--size=9.5-->
construct<--size=9.5-->
transfer<--size=9.5-->
learners;<--size=9.5-->

<--size=9.5-->
Finds near-optimal conﬁgurations with only a small<--size=9.5-->
number of measurements (an average of 13:5% 1<--size=9.5-->

7th<--size=6.6-->
of the available number of samples);<--size=9.5-->
<--size=9.5-->
Performs as well as non-transfer learning approaches;<--size=9.5-->
and<--size=9.5-->
<--size=9.5-->
Performs as well as state-of-the-art transfer learners.<--size=9.5-->
Based on our experiments, we demonstrate our initial<--size=9.5-->
problem–“whence to learn?” is an important question, and,<--size=9.5-->

A good source with a simple transfer learner is better<--size=9.5-->
than source agnostic complex transfer learners.<--size=9.5-->

REFERENCES<--size=11.5-->

[1]<--size=8.0-->
T. Xu, L. Jin, X. Fan, Y. Zhou, S. Pasupathy, and R. Talwadker,<--size=8.0-->
“Hey, you have given me too many knobs!: Understanding<--size=8.0-->
and<--size=8.0-->
dealing<--size=8.0-->
with<--size=8.0-->
over-designed<--size=8.0-->
conﬁguration<--size=8.0-->
in<--size=8.0-->
system<--size=8.0-->
software,” in Proc. 10th Joint Meeting Found. Softw. Eng., 2015,<--size=8.0-->
pp. 307–319.<--size=8.0-->
[2]<--size=8.0-->
D. Van Aken, A. Pavlo, G. J. Gordon, and B. Zhang, “Automatic<--size=8.0-->
database management system tuning through large-scale machine<--size=8.0-->
learning,” in Proc. Int. Conf. Manage. Data, 2017, pp. 1009–1024.<--size=8.0-->
[3]<--size=8.0-->
P. Jamshidi and G. Casale, “An uncertainty-aware approach to optimal<--size=8.0-->
conﬁguration of stream processing systems,” in Proc. Symp. Model.<--size=8.0-->
Anal. Simul. Comput. Telecommun. Syst., 2016, pp. 39–48.<--size=8.0-->
[4]<--size=8.0-->
N. Siegmund et al., “Predicting performance via automated<--size=8.0-->
feature-interaction detection,” in Proc. Int. Conf. Softw. Eng., 2012,<--size=8.0-->
pp. 167–177.<--size=8.0-->
[5]<--size=8.0-->
P. Valov, J. Guo, and K. Czarnecki, “Empirical comparison of<--size=8.0-->
regression methods for variability-aware performance prediction,”<--size=8.0-->
in Proc. 19th Int. Conf. Softw. Product Line, 2015, pp. 186–190.<--size=8.0-->
[6]<--size=8.0-->
N. Siegmund, A. Grebhahn, S. Apel, and C. K€astner, “Performance-<--size=8.0-->
inﬂuence models for highly conﬁgurable systems,” in Proc. 10th Joint<--size=8.0-->
Meeting Found. Softw. Eng., 2015, pp. 284–294.<--size=8.0-->
[7]<--size=8.0-->
A. Sarkar, J. Guo, N. Siegmund, S. Apel, and K. Czarnecki,<--size=8.0-->
“Cost-efﬁcient sampling for performance prediction of conﬁgu-<--size=8.0-->
rable systems (T),” in Proc. Int. Conf. Automated Softw. Eng., 2015,<--size=8.0-->
pp. 342–352.<--size=8.0-->
[8]<--size=8.0-->
J. Oh, D. Batory, M. Myers, and N. Siegmund, “Finding near-optimal<--size=8.0-->
conﬁgurations in product lines by random sampling,” in Proc. 11th<--size=8.0-->
Joint Meeting Found. Softw. Eng., 2017, pp. 61–71.<--size=8.0-->
[9]<--size=8.0-->
C. Tang, K. Sullivan, and B. Ray, “Searching for high-performing<--size=8.0-->
software conﬁgurations with metaheuristic algorithms,” in Proc.<--size=8.0-->
40th Int. Conf. Softw. Eng., 2018, pp. 354–355.<--size=8.0-->
[10] V. Nair, T. Menzies, N. Siegmund, and S. Apel, “Faster discovery<--size=8.0-->

of faster system conﬁgurations with spectral learning,” Automated<--size=8.0-->
Softw. Eng., vol. 25, pp. 247–277, 2018.<--size=8.0-->
[11] C.-J. Hsu, V. Nair, T. Menzies, and V. Freeh, “Micky: A cheaper<--size=8.0-->

alternative for selecting cloud instances,” in Proc. IEEE 11th Int.<--size=8.0-->
Conf. Cloud Comput., 2018, pp. 409–416.<--size=8.0-->
[12] V. Nair, T. Menzies, N. Siegmund, and S. Apel, “Using bad learn-<--size=8.0-->

ers to ﬁnd good conﬁgurations,” in Proc. 11th Joint Meeting Found.<--size=8.0-->
Softw. Eng., 2017, pp. 257–267.<--size=8.0-->
[13] J. Nam, S. J. Pan, and S. Kim, “Transfer defect learning,” in Proc.<--size=8.0-->

Int. Conf. Softw. Eng., 2013, pp. 382–391.<--size=8.0-->
[14] E. Kocaguneli and T. Menzies, “How to ﬁnd relevant data for<--size=8.0-->

effort estimation?” in Proc. Int. Symp. Empir. Softw. Eng. Meas.,<--size=8.0-->
2011, pp. 255–264.<--size=8.0-->
[15] E. Kocaguneli, T. Menzies, and E. Mendes, “Transfer learning in<--size=8.0-->

effort estimation,” Empir. Softw. Eng., vol. 20, pp. 813–843, 2015.<--size=8.0-->
[16] B. Turhan, T. Menzies, A. B. Bener, and J. Di Stefano, “On the rela-<--size=8.0-->

tive value of cross-company and within-company data for defect<--size=8.0-->
prediction,” Empir. Softw. Eng., vol. 14, pp. 540–578, 2009.<--size=8.0-->
[17] F. Peters, T. Menzies, and L. Layman, “LACE2: Better privacy-<--size=8.0-->

preserving data sharing for cross project defect prediction,” in<--size=8.0-->
Proc. Int. Conf. Softw. Eng., 2015, pp. 801–811.<--size=8.0-->
[18] R. Krishna and T. Menzies, “Bellwethers: A baseline method<--size=8.0-->

for transfer learning,” IEEE Trans. Softw. Eng., vol. 45, no. 11,<--size=8.0-->
pp. 1081–1105, Nov. 2019.<--size=8.0-->
[19] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Trans.<--size=8.0-->

Knowl. Data Eng., vol. 22, no. 10, pp. 1345–1359, Oct. 2010.<--size=8.0-->
[20] P. Jamshidi, N. Siegmund, M. Velez, C. K€astner, A. Patel, and<--size=8.0-->

Y. Agarwal, “Transfer learning for performance modeling of con-<--size=8.0-->
ﬁgurable systems: An exploratory analysis,” in Proc. Int. Conf.<--size=8.0-->
Automated Softw. Eng., 2017, pp. 497–508.<--size=8.0-->

2970<--size=7.0-->
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 47, NO. 12, DECEMBER 2021<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:15-->

<--page_start:16-->
[21] R. Krishna, T. Menzies, and W. Fu, “Too much automation? The<--size=8.0-->

bellwether effect and its implications for transfer learning,” in<--size=8.0-->
Proc. Int. Conf. Automated Softw. Eng., 2016, pp. 122–131.<--size=8.0-->
[22] S. Mensah, J. Keung, S. G. MacDonell, M. F. Bosu, and K. E. Bennin,<--size=8.0-->

“Investigating the signiﬁcance of bellwether effect to improve soft-<--size=8.0-->
ware effort estimation,” in Proc. Int. Conf. Softw. Qual. Rel. Secur.,<--size=8.0-->
2017, pp. 340–351.<--size=8.0-->
[23] S. Mensah, J. Keung, M. F. Bosu, K. E. Bennin, and P. K. Kudjo, “A<--size=8.0-->

stratiﬁcation and sampling model for bellwether moving win-<--size=8.0-->
dow,” in Proc. Int. Conf. Softw. Eng. Knowl. Eng., 2017, pp. 481–486.<--size=8.0-->
[24] P. Valov, J.-C. Petkovich, J. Guo, S. Fischmeister, and K. Czarnecki,<--size=8.0-->

“Transferring performance prediction models across different hard-<--size=8.0-->
ware platforms,” in Proc. Int. Conf. Perform. Eng., 2017, pp. 39–50.<--size=8.0-->
[25] P. Jamshidi, M. Velez, C. K€astner, N. Siegmund, and P. Kawthekar,<--size=8.0-->

“Transfer learning for improving model predictions in highly con-<--size=8.0-->
ﬁgurable software,” in Proc. Symp. Softw. Eng. Adaptive Self-Manag.<--size=8.0-->
Syst., 2017, pp. 31–41.<--size=8.0-->
[26] J. Guo, K. Czarnecki, S. Apel, N. Siegmund, and A. Wasowski,<--size=8.0-->

“Variability-aware performance prediction: A statistical learning<--size=8.0-->
approach,” in Proc. Int. Conf. Automated Softw. Eng., 2013, pp. 301–311.<--size=8.0-->
[27] V. Nair, Z. Yu, T. Menzies, N. Siegmund, and S. Apel, “Finding<--size=8.0-->

faster conﬁgurations using FLASH,” in IEEE Trans. Softw. Eng., to<--size=8.0-->
be published, doi: 10.1109/TSE.2018.2870895.<--size=8.0-->
[28] H. Herodotou et al., “Starﬁsh: A self-tuning system for big<--size=8.0-->

data analytics,” in Proc. Conf. Innovative Data Syst. Res., 2011,<--size=8.0-->
pp. 261–272.<--size=8.0-->
[29] A. Saltelli et al., Sensitivity Analysis, vol. 1. New York, NY, USA:<--size=8.0-->

Wiley, 2000.<--size=8.0-->
[30] J. Guo et al., “Data-efﬁcient performance learning for conﬁgurable<--size=8.0-->

systems,” Empir. Softw. Eng., vol. 23, pp. 1826–1867, 2018.<--size=8.0-->
[31] P. Valov, J. Guo, and K. Czarnecki, “Empirical comparison of<--size=8.0-->

regression methods for variability-aware performance prediction,”<--size=8.0-->
in Proc. Int. Conf. Softw. Product Line, 2015, pp. 186–190.<--size=8.0-->
[32] M. Birattari, T. St€utzle, L. Paquete, and K. Varrentrapp, “A racing<--size=8.0-->

algorithm for conﬁguring metaheuristics,” in Proc. 4th Annu. Conf.<--size=8.0-->
Genetic Evol. Comput., 2002, pp. 11–18.<--size=8.0-->
[33] L. Breiman, Classiﬁcation and Regression Trees. Evanston, IL, USA:<--size=8.0-->

Routledge, 2017.<--size=8.0-->
[34] P.-L. Loh and S. Nowozin, “Faster Hoeffding racing: Bernstein<--size=8.0-->

races via Jackknife estimates,” in Proc. Int. Conf. Algorithmic Learn.<--size=8.0-->
Theory, 2013, pp. 203–217.<--size=8.0-->
[35] T. Foss, E. Stensrud, B. Kitchenham, and I. Myrtveit, “A simula-<--size=8.0-->

tion study of the model evaluation criterion MMRE,” IEEE Trans.<--size=8.0-->
Softw. Eng., vol. 29, no. 11, pp. 985–995, Nov. 2003.<--size=8.0-->
[36] C. Trubiani, P. Jamshidi, J. Cito, W. Shang, Z. M. Jiang, and<--size=8.0-->

M. Borg, “Performance issues? Hey DevOps, mind the uncertainty!”<--size=8.0-->
IEEE Softw., vol. 36, no. 2, pp. 110–117, Mar./Apr. 2019.<--size=8.0-->
[37] S. Wang, S. Ali, T. Yue, Y. Li, and M. Liaaen, “A practical guide to<--size=8.0-->

select quality indicators for assessing pareto-based search algo-<--size=8.0-->
rithms in search-based software engineering,” in Proc. IEEE/ACM<--size=8.0-->
38th Int. Conf. Softw. Eng., 2016, pp. 631–642.<--size=8.0-->
[38] J. Chen, V. Nair, R. Krishna, and T. Menzies, ““Sampling” as a<--size=8.0-->

baseline optimizer for search-based software engineering,” IEEE<--size=8.0-->
Trans. Softw. Eng., vol. 45, no. 6, pp. 597–614, Jun. 2019.<--size=8.0-->
[39] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, “A fast and elit-<--size=8.0-->

ist multiobjective genetic algorithm: NSGA-II,” IEEE Trans. Evol.<--size=8.0-->
Comput., vol. 6, no. 2, pp. 182–197, Apr. 2002.<--size=8.0-->
[40] N. Mittas and L. Angelis, “Ranking and clustering software cost<--size=8.0-->

estimation models through a multiple comparisons algorithm,”<--size=8.0-->
IEEE Trans. Softw. Eng., vol. 39, no. 4, pp. 537–551, Apr. 2013.<--size=8.0-->
[41] R. J. Tibshirani and B. Efron, An Introduction to the Bootstrap,<--size=8.0-->

vol. 57. New York, NY, USA: Chapman and Hall, 1993.<--size=8.0-->
[42] A. Vargha and H. D. Delaney, “A critique and improvement of the<--size=8.0-->

CL common language effect size statistics of McGraw and Wong,”<--size=8.0-->
J. Educ. Behavioral Statist., vol. 25, pp. 101–132, 2000.<--size=8.0-->
[43] N. L. Leech and A. J. Onwuegbuzie, “A call for greater use of non-<--size=8.0-->

parametric statistics,” in Proc. Annu. Meeting Mid-South Educ. Res.<--size=8.0-->
Assoc., 2002, pp. 1–27.<--size=8.0-->
[44] S. Poulding and J. A. Clark, “Efﬁcient software veriﬁcation: Statis-<--size=8.0-->

tical testing using automated search,” IEEE Trans. Softw. Eng.,<--size=8.0-->
vol. 36, no. 6, pp. 763–777, Nov./Dec. 2010.<--size=8.0-->
[45] A. Arcuri and L. Briand, “A practical guide for using statistical<--size=8.0-->

tests to assess randomized algorithms in software engineering,”<--size=8.0-->
in Proc. 33rd Int. Conf. Softw. Eng., 2011, pp. 1–10.<--size=8.0-->
[46] M. J. Shepperd and S. G. MacDonell, “Evaluating prediction sys-<--size=8.0-->

tems in software project estimation,” Inf. Softw. Technol., vol. 54,<--size=8.0-->
pp. 820–827, 2012.<--size=8.0-->

[47] V. B. Kampenes, T. Dyba<--size=8.0-->

, J. E. Hannay, and D. I. K. Sjøberg, “A sys-<--size=8.0-->
tematic review of effect size in software engineering experiments,”<--size=8.0-->
Inf. Softw. Technol., vol. 49, pp. 1073–1086, 2007.<--size=8.0-->
[48] E. Kocaguneli, T. Zimmermann, C. Bird, N. Nagappan, and<--size=8.0-->

T. Menzies, “Distributed development considered harmful?” in<--size=8.0-->
Proc. Int. Conf. Softw. Eng., 2013, pp. 882–890.<--size=8.0-->
[49] C.-J. Hsu, V. Nair, T. Menzies, and V. W. Freeh, “Scout: An experi-<--size=8.0-->

enced guide to ﬁnd the best cloud conﬁguration,” 2018, arXiv:<--size=8.0-->
1803.01296.<--size=8.0-->
[50] C.-J. Hsu, V. Nair, V. W. Freeh, and T. Menzies, “Low-level aug-<--size=8.0-->

mented Bayesian optimization for ﬁnding the best cloud VM,” in<--size=8.0-->
Proc. Int. Conf. Distrib. Comput. Syst., 2018, pp. 660–670.<--size=8.0-->
[51] W. Fu, T. Menzies, and X. Shen, “Tuning for software analytics: Is<--size=8.0-->

it really necessary?” Inf. Softw. Technol., vol. 76, pp. 135–146, 2016.<--size=8.0-->
[52] M. J. Afridi, A. Ross, and E. M. Shapiro, “On automated source<--size=8.0-->

selection for transfer learning in convolutional neural networks,”<--size=8.0-->
J. Pattern Recognit., vol. 73, pp. 65–75, 2018.<--size=8.0-->
[53] K. Hoste, A. Phansalkar, L. Eeckhout, A. Georges, L. K. John, and<--size=8.0-->

K. De Bosschere, “Performance prediction based on inherent pro-<--size=8.0-->
gram similarity,” in Proc. Int. Conf. Parallel Archit. Compilation<--size=8.0-->
Techn., 2006, pp. 114–122.<--size=8.0-->
[54] F. Hutter, L. Xu, H. H. Hoos, and K. Leyton-Brown, “Algorithm<--size=8.0-->

runtime prediction: Methods & evaluation,” Artif. Intell., vol. 206,<--size=8.0-->
pp. 79–111, 2014.<--size=8.0-->
[55] E. Thereska, B. Doebel, A. X. Zheng, and P. Nobel, “Practical per-<--size=8.0-->

formance models for complex, popular applications,” ACM SIG-<--size=8.0-->
METRICS Perform. Eval. Rev., vol. 38, pp. 1–12, 2010.<--size=8.0-->
[56] D. Westermann, J. Happe, R. Krebs, and R. Farahbod, “Automated<--size=8.0-->

inference of goal-oriented performance prediction functions,” in<--size=8.0-->
Proc. Int. Conf. Automated Softw. Eng., 2012, pp. 190–199.<--size=8.0-->
[57] S. Wang, C. Li, H. Hoffmann, S. Lu, W. Sentosa, and A. I. Kistijantoro,<--size=8.0-->

“Understanding and auto-adjusting performance-sensitive conﬁgu-<--size=8.0-->
rations,” in Proc. 23rd Int. Conf. Archit. Support Program. Lang. Operat-<--size=8.0-->
ing Syst., 2018, pp. 154–168.<--size=8.0-->
[58] Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo, “Optimus: An efﬁ-<--size=8.0-->

cient dynamic resource scheduler for deep learning clusters,” in<--size=8.0-->
Proc. 13th EuroSys Conf., 2018, pp. 1–14.<--size=8.0-->
[59] Y. Zhu et al., “BestConﬁg: Tapping the performance potential of<--size=8.0-->

systems via automatic conﬁguration tuning,” in Proc. Symp. Cloud<--size=8.0-->
Comput., 2017, pp. 338–350.<--size=8.0-->
[60] S. W. C. Li, H. Hoffmann, S. Lu, S. W. Sentosa, and A. I. Kisti-<--size=8.0-->

jantoro, “Understanding and auto-adjusting performance-sen-<--size=8.0-->
sitive conﬁgurations,” in ACM SIGPLAN Notices, vol. 53, no. 2,<--size=8.0-->
pp. 154–168, 2018.<--size=8.0-->
[61] O. Alipourfard, H. H. Liu, J. Chen, S. Venkataraman, M. Yu, and<--size=8.0-->

M. Zhang, “CherryPick: Adaptively unearthing the best cloud<--size=8.0-->
conﬁgurations for big data analytics,” in Proc. Symp. Netw. Syst.<--size=8.0-->
Des. Implementation, 2017, pp. 469–482.<--size=8.0-->
[62] N. J. Yadwadkar, B. Hariharan, J. E. Gonzalez, B. Smith, and<--size=8.0-->

R. H. Katz, “Selecting the best VM across multiple public clouds:<--size=8.0-->
A data-driven performance modeling approach,” in Proc. Symp.<--size=8.0-->
Cloud Comput., 2017, pp. 452–465.<--size=8.0-->
[63] H. Chen, W. Zhang, and G. Jiang, “Experience transfer for the con-<--size=8.0-->

ﬁguration tuning in large-scale computing systems,” IEEE Trans.<--size=8.0-->
Knowl. Data Eng., vol. 23, no. 3, pp. 388–401, Mar. 2011.<--size=8.0-->
[64] D. Golovin, B. Solnik, S. Moitra, G. Kochanski, J. Karro, and<--size=8.0-->

D. Sculley, “Google Vizier: A service for black-box optimization,”<--size=8.0-->
in Proc. 23rd ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining,<--size=8.0-->
2017, pp. 1487–1495.<--size=8.0-->
[65] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable<--size=8.0-->

are features in deep neural networks?” in Proc. Int. Conf. Neural<--size=8.0-->
Inf. Process. Syst., 2014, pp. 3320–3328.<--size=8.0-->
[66] M. Long, Y. Cao, J. Wang, and M. Jordan, “Learning transferable<--size=8.0-->

features with deep adaptation networks,” in Proc. Int. Conf. Mach.<--size=8.0-->
Learn., 2015, pp. 97–105.<--size=8.0-->
[67] S. Ben-David and R. Schuller, “Exploiting task relatedness for<--size=8.0-->

multiple task learning,” in Learning Theory and Kernel Machines.<--size=8.0-->
Berlin, Germany: Springer, pp. 1–4, 2003.<--size=8.0-->
[68] M. T. Rosenstein, Z. Marx, L. P. Kaelbling, and T. G. Dietterich,<--size=8.0-->

“To transfer or not to transfer,” in Proc. NIPS Workshop Inductive<--size=8.0-->
Transfer, 2005, pp. 1–4.<--size=8.0-->
[69] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Trans.<--size=8.0-->

Knowl. Data Eng., vol. 22, no. 10, pp. 1345–1359, Oct. 2010.<--size=8.0-->
[70] C. E. Rasmussen, “Gaussian processes in machine learning,” in,<--size=8.0-->

Advanced Lectures on Machine Learning. Berlin, Germany: Springer,<--size=8.0-->
2004.<--size=8.0-->

KRISHNA ET AL.: WHENCE TO LEARN? TRANSFERRING KNOWLEDGE IN CONFIGURABLE SYSTEMS USING BEETLE<--size=7.0-->
2971<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:16-->

<--page_start:17-->
<--image width=300.0 height=375.0-->
Rahul Krishna received the PhD degree from<--size=8.0-->
NC State University, Raleigh, North Carolina. He<--size=8.0-->
is a postdoctoral researcher in computer science<--size=8.0-->
with Columbia University. His current research<--size=8.0-->
explores ways to use machine learning to gener-<--size=8.0-->
ate actionable insights for building reliable soft-<--size=8.0-->
ware<--size=8.0-->
systems.<--size=8.0-->
His<--size=8.0-->
other<--size=8.0-->
research<--size=8.0-->
interests<--size=8.0-->
include program analysis, artiﬁcial intelligence,<--size=8.0-->
and security. For more information, please visit<--size=8.0-->
http://rkrsn.us.<--size=8.0-->

<--image width=300.0 height=375.0-->
Vivek Nair received the graduate and PhD<--size=8.0-->
degrees from the Department of Computer Sci-<--size=8.0-->
ence, North Carolina State University, Raleigh,<--size=8.0-->
North Carolina. His primary interest lies in explor-<--size=8.0-->
ing possibilities of using multiobjective optimiza-<--size=8.0-->
tion to solve problems in software engineering. At<--size=8.0-->
NCSU, he was working on performance predic-<--size=8.0-->
tion models of highly conﬁgurable systems. He<--size=8.0-->
received the master’s degree and worked in the<--size=8.0-->
mobile industry for two years before returning to<--size=8.0-->
graduate school. For more information, please<--size=8.0-->
visit http://vivekaxl.com.<--size=8.0-->

<--image width=300.0 height=375.0-->
Pooyan Jamshidi is an assistant professor with the<--size=8.0-->
University of South Carolina. His general research<--size=8.0-->
interests include at the intersection of systems/soft-<--size=8.0-->
ware and machine learning. He directs the AISys<--size=8.0-->
Lab. For more information, please visit (https://<--size=8.0-->
pooyanjamshidi.github.io/AISys/), where he investi-<--size=8.0-->
gates the development of novel algorithmic and the-<--size=8.0-->
oretically principled methods for machine learning<--size=8.0-->
systems. Prior to his current position, he was a<--size=8.0-->
research associate with Carnegie Mellon University<--size=8.0-->
and Imperial College London, where he primarily<--size=8.0-->
worked on transfer learning for performance understanding of highly-<--size=8.0-->
conﬁgurable systems.<--size=8.0-->

<--image width=300.0 height=375.0-->
Tim Menzies (Fellow, IEEE) is a professor in CS<--size=8.0-->
at NcState. His research interests include soft-<--size=8.0-->
ware engineering (SE), data mining, artiﬁcial<--size=8.0-->
intelligence, search-based SE, and open access<--size=8.0-->
science. For more information, please visit http://<--size=8.0-->
menzies.us<--size=8.0-->

" For more information on this or any other computing topic,<--size=6.3-->
please visit our Digital Library at www.computer.org/csdl.<--size=8.0-->

2972<--size=7.0-->
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 47, NO. 12, DECEMBER 2021<--size=7.0-->

horized licensed use limited to: PONTIFICIA UNIVERSIDADE CATOLICA DO RIO DE JANEIRO. Downloaded on January 22,2025 at 21:47:17 UTC from IEEE Xplore.  Restrictions ap<--size=7.0-->

<--page_end:17-->

