1
INTRODUCTION
Configurable software systems offer a multitude of configuration
options that can be combined to tailor the systems’ functional behav-
ior and performance (e.g., execution time, memory consumption).
Options often have a significant influence on performance proper-
ties that are hard to know and model a priori. There are numerous
possible options values, logical constraints between options, and
subtle interactions among options [15, 25, 46, 51, 52] that can have
an effect while quantitative properties such as execution time are
themselves challenging to comprehend.
Measuring all configurations of a configurable system is the
most obvious path to e.g., find a well-suited configuration, but is
too costly or infeasible in practice. Machine-learning techniques
address this issue by measuring only a subset of configurations
(known as sample) and then using these configurations’ measure-
ments to build a performance model capable of predicting the per-
formance of other configurations (i.e., configurations not measured
before). Several works thus follow a "sampling, measuring, learn-
ing" process [3, 15, 20–23, 27, 40, 42, 43, 46, 51, 52, 60, 63, 64]. A
crucial step is the way the sampling is realized, since it can drasti-
cally affect the performance model accuracy [25, 46]. Ideally, the
sample should be small to reduce the cost of measurements and
representative of the configuration space to reduce prediction er-
rors. The sampling phase involves a number of difficult activities:
(1) picking configurations that are valid and conform to constraints
among options – one needs to resolve a satisfiability problem; (2)
instrumenting the executions and observations of software for a
variety of configurations – it might have a high computational
cost especially when measuring performance aspects of software;
(3) guaranteeing a coverage of the configuration space to obtain a
representative sample set. An ideal coverage includes all influential
configuration options by covering different kinds of interactions
SESSION 7: Performance Techniques
ICPE '20, April 20–24, 2020, Edmonton, AB, Canada
277
<--page_end:1--><--page_start:2-->ICPE ’20, April 20–24, 2020, Edmonton, AB, Canada
Juliana Alves Pereira, Mathieu Acher, Hugo Martin, and Jean-Marc Jézéquel
relevant to performance. Otherwise, the learning may hardly gen-
eralize to the whole configuration space.
With the promise to select a small and representative sample
set of valid configurations, several sampling strategies have been
devised in the last years [46]. For example, random sampling aims
to cover the configuration space uniformly while coverage-oriented
sampling selects the sample set according to a coverage criterion
(e.g.,t-wise sampling to cover all combinations oft selected options).
Recently, Kaltenecker et al. [25] analyzed 10 popular real-world
software systems and found that their novel proposed sampling
strategy, called diversified distance-based sampling, dominates five
other sampling strategies by decreasing the cost of labelling soft-
ware configurations while minimizing the prediction errors.
In this paper, we conduct a replication of this preliminary study
by exclusively considering the x264 case, a configurable video en-
coder. Though we only consider one particular configurable system,
we make vary its workloads (with the use of 17 input videos) and
measured two performance properties (encoding time and encoding
size) over 1,152 configurations. Interestingly, Kaltenecker et al. [25]
also analyzed the same 1,152 configurations of x264, but a fixed
input video was used and only the time was considered. The goal
of our experiments is to determine whether sampling strategies
considered in [25] over different subject systems are as effective on
the same configurable system, but with different factors possibly
influencing the distribution of the configuration space. A hypoth-
esis is that practitioners of a configurable system can rely on a
one-size-fits-all sampling strategy that is cost-effective whatever
the factors influencing the distribution of its configuration space.
On the contrary, another hypothesis is that practitioners should
change their sampling strategy each time an input (here: videos) or
a performance property are targeted.
We investigate to what extent sampling strategies are sensi-
tive to different workloads of the x264 configurable system and
different performance properties: What are the most effective sam-
pling strategies? Is random sampling a strong baseline? Is there
a dominant sampling strategy that practitioners can always rely
on? To this end, we systematically report the influence of sampling
strategies on the accuracy of performance predictions and on the
robustness of prediction accuracy.
Our contributions are as follows:
• We rank six sampling strategies based on a unified bench-
mark over seventeen different inputs and two non-functional
properties. We show that the ranking of non-random sam-
pling strategies is quite unstable, being heavily sensitive to
input video and targeted performance property. We gather
preliminary insights about the effectiveness of some sam-
pling strategies on some videos and performances;
• We compare our results to the previous study of x264 [25],
that was based on the use of a single input video and measure-
ments of encoding time. Through our experimental results,
we find that for the non-functional property encoding time
uniform random sampling dominates all the others sampling
strategies w.r.t. prediction accuracy for a majority of cases,
as in Kaltenecker et al. [25];
• We have made all the artifacts from our replication study pub-
licly available at https://github.com/jualvespereira/ICPE2020.
With respect to the categorized research methods by Stol et al. [57],
our paper mainly contributes to a knowledge-seeking study. Specif-
ically, we perform a field experiment of x264, a highly-configurable
system and mature project. We gain insights about performance’
properties using a large corpus of configurations and input videos.
Audience. Researchers and practitioners in configurable systems
and performance engineering shall benefit from our sampling exper-
iments and insights. We discuss impacts of our results for practition-
ers (How to choose a sampling strategy?) and for the research com-
munity (e.g., on the design and assessment of sampling strategies).

