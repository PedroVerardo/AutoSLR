{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from extrac_text import extract_text\n",
    "from ollama_tools import ask_llm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data-extration.xlsx\"\n",
    "xls = pd.ExcelFile(file_path)\n",
    "rq_sheets = [\"RQ2\", \"RQ3_Systems\", \"RQ4_Learning\", \"RQ5_Partition_Method\", \"RQ5_Metric\"]\n",
    "\n",
    "def extract_citation_key(cite):\n",
    "    if isinstance(cite, str):\n",
    "        match = re.search(r\"\\\\cite{(.+?)}\", cite)\n",
    "        return match.group(1) if match else cite\n",
    "    return cite\n",
    "\n",
    "processed_sheets = {}\n",
    "for sheet in rq_sheets:\n",
    "    df = xls.parse(sheet)\n",
    "    df.columns = [str(col).strip().lower() for col in df.columns]\n",
    "    \n",
    "    df = df.loc[:, ~df.columns.str.contains('^unnamed')]\n",
    "    df = df.loc[:, ~df.columns.str.contains('^comentÃ¡rio')]\n",
    "    df = df.loc[:, ~df.columns.str.contains('^obs:')]\n",
    "    \n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    if \"reference\" in df.columns:\n",
    "        df[\"reference\"] = df[\"reference\"].apply(extract_citation_key)\n",
    "        \n",
    "        agg_dict = {}\n",
    "        for col in df.columns:\n",
    "            if col != \"reference\":\n",
    "                agg_dict[col] = lambda x: list(x.dropna().unique()) if len(x.dropna()) > 0 else np.nan\n",
    "        \n",
    "        df = df.groupby(\"reference\", as_index=False).agg(agg_dict)\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if col != \"reference\":\n",
    "                df[col] = df[col].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "\n",
    "    processed_sheets[sheet] = df\n",
    "\n",
    "merged_df = processed_sheets[rq_sheets[0]]\n",
    "for sheet in rq_sheets[1:]:\n",
    "    merged_df = merged_df.merge(processed_sheets[sheet], on=\"reference\", how=\"outer\")\n",
    "\n",
    "merged_df.columns = [col.split('.')[-1] if '.' in col else col for col in merged_df.columns]\n",
    "\n",
    "merged_df.drop(columns=[\"related\", \"ref\", \"cite\"], inplace=True)\n",
    "merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df[\"reference\"] == \"yufei2024:jss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"reference\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "papers_folder = \"papers\"\n",
    "\n",
    "pdf_files = []\n",
    "for root, dirs, files in os.walk(papers_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdf_files.append(os.path.splitext(file)[0])\n",
    "\n",
    "unique_references = merged_df[\"reference\"].unique()\n",
    "size = len(unique_references)\n",
    "\n",
    "\n",
    "missing_papers = [pdf for pdf in pdf_files if pdf not in unique_references]\n",
    "\n",
    "if len(unique_references) == size:\n",
    "    print(\"All 64 papers are present in the merged_df.\")\n",
    "else:\n",
    "    print(f\"Number of unique papers in merged_df: {len(unique_references)}\")\n",
    "    print(f\"Missing papers: {missing_papers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = []\n",
    "for root, dirs, files in os.walk(papers_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdf_files.append(os.path.abspath(os.path.join(root, file)))\n",
    "\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_base = [\"using only the metodology part answer the following questions in topics: \", \"do not consider the related work section for answer this questions in topics: \"]\n",
    "questions = [\"What is the sampling method used?\",\n",
    "             \"What are the systems used?\",\n",
    "             \"Which is the domain of the systems used?\",\n",
    "             \"What is the learning method used?\",\n",
    "             \"What is the partition method used?\",\n",
    "             \"What is the Non-Functional Performance metric used?\",\n",
    "             \"Is there any reference for the dataset?\",\n",
    "             \"What are the machine learning algorithms used in this article?\",\n",
    "             \"What are the performance metrics used in this article?\",\n",
    "             \"What are the partition methods used?\",\n",
    "             \"What are the evaluation methods used in this article?\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_str = \" \".join(questions)\n",
    "final_prompts = [prompt  + \" \" + questions_str for prompt in prompts_base]\n",
    "\n",
    "final_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"mistral-nemo\", \"qwen:1.8b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "for model in models:\n",
    "    for prompt in final_prompts:\n",
    "        for pdf_path in pdf_files:\n",
    "            result = None\n",
    "\n",
    "            text = extract_text(pdf_path)\n",
    "\n",
    "            if text:\n",
    "                result = ask_llm(text, prompt, model)\n",
    "            else:\n",
    "                print(f\"Error extracting text from {pdf_path}\")\n",
    "\n",
    "            if result:\n",
    "                if pdf_path not in result_dict:\n",
    "                    result_dict[pdf_path] = {}\n",
    "                if model not in result_dict[pdf_path]:\n",
    "                    result_dict[pdf_path][model] = {}\n",
    "                result_dict[pdf_path][model][prompt] = result\n",
    "\n",
    "with open(\"result_dict.json\", \"w\") as json_file:\n",
    "    json.dump(result_dict, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
