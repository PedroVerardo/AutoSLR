{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>applicability</th>\n",
       "      <th>sampling</th>\n",
       "      <th>learning</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\cite{martinez2018}</td>\n",
       "      <td>A3</td>\n",
       "      <td>Genetic algorithm</td>\n",
       "      <td>Data Mining Interpolation</td>\n",
       "      <td>(Neighbors Density, Similarity) Confidence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\cite{valov2015}</td>\n",
       "      <td>A1</td>\n",
       "      <td>Random</td>\n",
       "      <td>Bagging</td>\n",
       "      <td>Closeness Range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\cite{valov2015}</td>\n",
       "      <td>A1</td>\n",
       "      <td>Random</td>\n",
       "      <td>CART</td>\n",
       "      <td>Closeness Range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\cite{valov2015}</td>\n",
       "      <td>A1</td>\n",
       "      <td>Random</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Closeness Range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\cite{valov2015}</td>\n",
       "      <td>A1</td>\n",
       "      <td>Random</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Closeness Range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>\\cite{xu2008}</td>\n",
       "      <td>A3</td>\n",
       "      <td>Interaction-wise</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>Tuning Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>\\cite{valov2015}</td>\n",
       "      <td>A1</td>\n",
       "      <td>Random</td>\n",
       "      <td>Bagging</td>\n",
       "      <td>Winner Probability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>\\cite{valov2015}</td>\n",
       "      <td>A1</td>\n",
       "      <td>Random</td>\n",
       "      <td>CART</td>\n",
       "      <td>Winner Probability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>\\cite{valov2015}</td>\n",
       "      <td>A1</td>\n",
       "      <td>Random</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Winner Probability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>\\cite{valov2015}</td>\n",
       "      <td>A1</td>\n",
       "      <td>Random</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Winner Probability</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               reference applicability           sampling  \\\n",
       "0    \\cite{martinez2018}            A3  Genetic algorithm   \n",
       "1       \\cite{valov2015}            A1             Random   \n",
       "2       \\cite{valov2015}            A1             Random   \n",
       "3       \\cite{valov2015}            A1             Random   \n",
       "4       \\cite{valov2015}            A1             Random   \n",
       "..                   ...           ...                ...   \n",
       "294        \\cite{xu2008}            A3   Interaction-wise   \n",
       "295     \\cite{valov2015}            A1             Random   \n",
       "296     \\cite{valov2015}            A1             Random   \n",
       "297     \\cite{valov2015}            A1             Random   \n",
       "298     \\cite{valov2015}            A1             Random   \n",
       "\n",
       "                      learning                                      metric  \n",
       "0    Data Mining Interpolation  (Neighbors Density, Similarity) Confidence  \n",
       "1                      Bagging                             Closeness Range  \n",
       "2                         CART                             Closeness Range  \n",
       "3                Random Forest                             Closeness Range  \n",
       "4                          SVM                             Closeness Range  \n",
       "..                         ...                                         ...  \n",
       "294           Ridge Regression                                 Tuning Time  \n",
       "295                    Bagging                          Winner Probability  \n",
       "296                       CART                          Winner Probability  \n",
       "297              Random Forest                          Winner Probability  \n",
       "298                        SVM                          Winner Probability  \n",
       "\n",
       "[299 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('jailmaReviewPdf/data.csv', sep=';').dropna()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'martinez2018': {'sampling': 'Genetic algorithm',\n",
       "  'learning': 'Data Mining Interpolation',\n",
       "  'evaluation_metric': 'MAE',\n",
       "  'note': ''},\n",
       " 'valov2015': {'sampling': 'Random',\n",
       "  'learning': 'SVM',\n",
       "  'evaluation_metric': 'Winner Probability',\n",
       "  'note': ''},\n",
       " 'song2013': {'sampling': 'iTree',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'Sampling Cost',\n",
       "  'note': ''},\n",
       " 'zhang2016': {'sampling': 'Random',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'Performance-Relevant Interactions',\n",
       "  'note': ''},\n",
       " 'safdar2017': {'sampling': 'NSGA-II',\n",
       "  'learning': 'PART',\n",
       "  'evaluation_metric': 'Mann-Whitney U-test',\n",
       "  'note': ''},\n",
       " 'gargantini2017': {'sampling': 'Unconstrained CIT',\n",
       "  'learning': 'CitLab',\n",
       "  'evaluation_metric': 'Equivalence, FID, TRM',\n",
       "  'note': ''},\n",
       " 'osogami2007': {'sampling': 'Random',\n",
       "  'learning': 'Quick Optimization via Guessing',\n",
       "  'evaluation_metric': 'Tuning Time',\n",
       "  'note': ''},\n",
       " 'aken2017': {'sampling': 'Historical dataset',\n",
       "  'learning': 'OLS',\n",
       "  'evaluation_metric': 'Tuning Time',\n",
       "  'note': ''},\n",
       " 'grebhahn2017': {'sampling': 'Random',\n",
       "  'learning': 'Multivariate regression',\n",
       "  'evaluation_metric': 'Sampling Cost',\n",
       "  'note': ''},\n",
       " 'queiroz2016': {'sampling': 'Arbitrarily chosen',\n",
       "  'learning': 'Random Forest',\n",
       "  'evaluation_metric': 'TP, FP, ROC',\n",
       "  'note': ''},\n",
       " 'amand2019': {'sampling': 'Cartesian product',\n",
       "  'learning': 'SVM',\n",
       "  'evaluation_metric': 'F-measure',\n",
       "  'note': ''},\n",
       " 'yilmaz2006': {'sampling': 'Covering arrays',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'F-measure',\n",
       "  'note': ''},\n",
       " 'yilmaz2014': {'sampling': 'Interaction-wise',\n",
       "  'learning': 'Traditional CIT',\n",
       "  'evaluation_metric': 't-masked ',\n",
       "  'note': ''},\n",
       " 'samreen2016': {'sampling': 'Arbitrarily chosen',\n",
       "  'learning': 'Ridge Regression',\n",
       "  'evaluation_metric': 'p-value, R2, RSE',\n",
       "  'note': ''},\n",
       " 'kaltenecker2019': {'sampling': 'Random',\n",
       "  'learning': 'Linear Regression',\n",
       "  'evaluation_metric': 'MRE',\n",
       "  'note': ''},\n",
       " 'etxeberria2014': {'sampling': 'Feature frequency',\n",
       "  'learning': 'Variance Analysis',\n",
       "  'evaluation_metric': 'GAP',\n",
       "  'note': ''},\n",
       " 'westermann2012': {'sampling': 'Random Breakdown',\n",
       "  'learning': 'MARS',\n",
       "  'evaluation_metric': 'MRE',\n",
       "  'note': ''},\n",
       " 'zhang2015': {'sampling': 'Random',\n",
       "  'learning': 'Fourier Learning',\n",
       "  'evaluation_metric': 'MRE',\n",
       "  'note': ''},\n",
       " 'kolesnikov2017': {'sampling': 'non-applicable',\n",
       "  'learning': 'Linear Regression',\n",
       "  'evaluation_metric': 'Precision, Recall',\n",
       "  'note': ''},\n",
       " 'jamshidi2017b': {'sampling': 'Random',\n",
       "  'learning': 'Multinomial Logistic Regression',\n",
       "  'evaluation_metric': 'KL, (Rank, Pearson, Spearman) Correlation',\n",
       "  'note': ''},\n",
       " 'murwantara2014': {'sampling': 'Arbitrarily chosen',\n",
       "  'learning': 'Multilayer Perceptron',\n",
       "  'evaluation_metric': 'MSE',\n",
       "  'note': ''},\n",
       " 'jamshidi2016': {'sampling': 'LHS',\n",
       "  'learning': 'GPM',\n",
       "  'evaluation_metric': 'MAE',\n",
       "  'note': ''},\n",
       " 'jamshidi2018': {'sampling': 'L2S',\n",
       "  'learning': 'GPM',\n",
       "  'evaluation_metric': 'MRE',\n",
       "  'note': ''},\n",
       " 'jamshidi2017a': {'sampling': 'Random',\n",
       "  'learning': 'GPM',\n",
       "  'evaluation_metric': 'MAE',\n",
       "  'note': ''},\n",
       " 'siegmund2013b': {'sampling': 'family-based',\n",
       "  'learning': 'Graph family-based simulator',\n",
       "  'evaluation_metric': 'MAE',\n",
       "  'note': ''},\n",
       " 'lillacka2013': {'sampling': 'Feature frequency',\n",
       "  'learning': 'Linear Regression',\n",
       "  'evaluation_metric': 'MAE',\n",
       "  'note': ''},\n",
       " 'bao2018': {'sampling': 'wLHS',\n",
       "  'learning': 'Random Forest',\n",
       "  'evaluation_metric': 'Rank Accuracy',\n",
       "  'note': ''},\n",
       " 'jehooh2017': {'sampling': 'Random',\n",
       "  'learning': 'Recursive Searching',\n",
       "  'evaluation_metric': 'MAE',\n",
       "  'note': ''},\n",
       " 'elafia2018': {'sampling': 'NI',\n",
       "  'learning': 'SVM',\n",
       "  'evaluation_metric': 'MSE',\n",
       "  'note': ''},\n",
       " 'ghamizi2019': {'sampling': 'PLEDGE diversity',\n",
       "  'learning': 'Tensorflow and Keras',\n",
       "  'evaluation_metric': 'MAE',\n",
       "  'note': ''},\n",
       " 'hutter2011': {'sampling': 'multi-start local search',\n",
       "  'learning': 'ROAR and SMAC',\n",
       "  'evaluation_metric': 'Mann-Whitney U-test',\n",
       "  'note': ''},\n",
       " 'nair2017': {'sampling': 'Random',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'MRD',\n",
       "  'note': ''},\n",
       " 'nair2018a': {'sampling': 'Random',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'Sampling Cost',\n",
       "  'note': ''},\n",
       " 'nair2018c': {'sampling': 'Random',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'Sampling Cost',\n",
       "  'note': ''},\n",
       " 'guo2013': {'sampling': 'Random',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'MRE',\n",
       "  'note': ''},\n",
       " 'guo2017': {'sampling': 'Random',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'MRE',\n",
       "  'note': ''},\n",
       " 'valov2017': {'sampling': \"Walker's alias\",\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'Structure of the model',\n",
       "  'note': ''},\n",
       " 'siegmund2012a': {'sampling': 'Hot-spot features',\n",
       "  'learning': \"Feature's Influence Delta\",\n",
       "  'evaluation_metric': 'MRE',\n",
       "  'note': ''},\n",
       " 'siegmund2011': {'sampling': 'Interaction-wise',\n",
       "  'learning': \"Feature's Influence Delta\",\n",
       "  'evaluation_metric': 'MRE',\n",
       "  'note': ''},\n",
       " 'siegmund2013': {'sampling': 'Interaction-wise',\n",
       "  'learning': \"Feature's Influence Delta\",\n",
       "  'evaluation_metric': 'MRE',\n",
       "  'note': ''},\n",
       " 'couto2017': {'sampling': 'non-applicable',\n",
       "  'learning': 'Implicit Path Enumeration',\n",
       "  'evaluation_metric': 'MRE',\n",
       "  'note': ''},\n",
       " 'duarte2018': {'sampling': 'Arbitrarily chosen',\n",
       "  'learning': 'k-plane',\n",
       "  'evaluation_metric': 'Sampling Cost',\n",
       "  'note': ''},\n",
       " 'siegmund2015': {'sampling': 'Random',\n",
       "  'learning': 'Linear Regression',\n",
       "  'evaluation_metric': 'MRE',\n",
       "  'note': ''},\n",
       " 'kolesnikov2018': {'sampling': 'non-applicable',\n",
       "  'learning': 'Linear Regression',\n",
       "  'evaluation_metric': 'Structure of the model',\n",
       "  'note': ''},\n",
       " 'jamshidi2019': {'sampling': 'Random',\n",
       "  'learning': 'Linear Regression',\n",
       "  'evaluation_metric': 'Rank Correlation',\n",
       "  'note': ''},\n",
       " 'chen2005': {'sampling': 'Arbitrarily chosen',\n",
       "  'learning': 'Mathematical Modelling',\n",
       "  'evaluation_metric': 'MRE',\n",
       "  'note': ''},\n",
       " 'thornton2013': {'sampling': 'Random',\n",
       "  'learning': 'TPE',\n",
       "  'evaluation_metric': 'MRE',\n",
       "  'note': ''},\n",
       " 'saleem2015': {'sampling': 'Historical dataset',\n",
       "  'learning': 'AdaRank ',\n",
       "  'evaluation_metric': 'MRR',\n",
       "  'note': ''},\n",
       " 'zheng2007': {'sampling': 'non-applicable',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'non-applicable',\n",
       "  'note': ''},\n",
       " 'siegmund2017': {'sampling': 'Random',\n",
       "  'learning': 'Kernel Density and NSGA-II',\n",
       "  'evaluation_metric': 'non-applicable',\n",
       "  'note': ''},\n",
       " 'siegmund2008': {'sampling': 'Random',\n",
       "  'learning': 'Linear Regression',\n",
       "  'evaluation_metric': 'non-applicable',\n",
       "  'note': ''},\n",
       " 'sincero2010': {'sampling': 'Arbitrarily chosen',\n",
       "  'learning': 'Covariance Analysis',\n",
       "  'evaluation_metric': 'NI',\n",
       "  'note': ''},\n",
       " 'siegmund2012b': {'sampling': 'Feature frequency',\n",
       "  'learning': \"Feature's Influence Delta\",\n",
       "  'evaluation_metric': 'NI',\n",
       "  'note': ''},\n",
       " 'sharifloo2016': {'sampling': 'NI',\n",
       "  'learning': 'Reinforcement Learning',\n",
       "  'evaluation_metric': 'NI',\n",
       "  'note': ''},\n",
       " 'xi2004': {'sampling': 'LHS',\n",
       "  'learning': 'Smart Hill-Climbing',\n",
       "  'evaluation_metric': 'NI',\n",
       "  'note': ''},\n",
       " 'zuluaga2016': {'sampling': 'Diameter uncertainty',\n",
       "  'learning': 'GPM',\n",
       "  'evaluation_metric': 'Pareto Prediction Error',\n",
       "  'note': ''},\n",
       " 'temple2017a': {'sampling': 'Random',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'Precision, Recall',\n",
       "  'note': ''},\n",
       " 'acher2018': {'sampling': 'Random',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'Precision, Recall',\n",
       "  'note': ''},\n",
       " 'temple2016': {'sampling': 'Random',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'Precision, Recall',\n",
       "  'note': ''},\n",
       " 'porter2007': {'sampling': 'nearest neighbor',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'Statistical Significance',\n",
       "  'note': ''},\n",
       " 'temple2018': {'sampling': 'Random',\n",
       "  'learning': 'SVM',\n",
       "  'evaluation_metric': 'Qualitative Analysis',\n",
       "  'note': ''},\n",
       " 'svogor2019': {'sampling': 'Random',\n",
       "  'learning': 'Simulated annealing',\n",
       "  'evaluation_metric': 'Rank Accuracy',\n",
       "  'note': ''},\n",
       " 'krismayer2017': {'sampling': 'STT with random differences',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'Ranking Constraints',\n",
       "  'note': ''},\n",
       " 'chen2009': {'sampling': 'Random',\n",
       "  'learning': 'Actor-Critic',\n",
       "  'evaluation_metric': 'Reward',\n",
       "  'note': ''},\n",
       " 'sarkar2015': {'sampling': 'Feature frequency',\n",
       "  'learning': 'CART',\n",
       "  'evaluation_metric': 'Sampling Cost',\n",
       "  'note': ''},\n",
       " 'alipourfard2017': {'sampling': 'Quasi-random',\n",
       "  'learning': 'GPM',\n",
       "  'evaluation_metric': 'Sampling Cost',\n",
       "  'note': ''},\n",
       " 'weckesser2018': {'sampling': 'Random',\n",
       "  'learning': 'Linear Regression',\n",
       "  'evaluation_metric': 'Sampling Cost',\n",
       "  'note': ''},\n",
       " 'ding2015': {'sampling': 'Clustering-based',\n",
       "  'learning': 'Exhaustive Feature Classifiers',\n",
       "  'evaluation_metric': 'Tuning Time',\n",
       "  'note': ''},\n",
       " 'xu2008': {'sampling': 'Interaction-wise',\n",
       "  'learning': 'Ridge Regression',\n",
       "  'evaluation_metric': 'Tuning Time',\n",
       "  'note': ''}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def extract_citation_key(cite):\n",
    "    match = re.search(r\"\\\\cite{(.+?)}\", cite)\n",
    "    return match.group(1) if match else cite\n",
    "\n",
    "df[\"reference\"] = df[\"reference\"].apply(extract_citation_key)\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    ref = row[\"reference\"]\n",
    "    result_dict[ref] = {\n",
    "        \"sampling\": row[\"sampling\"],\n",
    "        \"learning\": row[\"learning\"],\n",
    "        \"evaluation_metric\": row[\"metric\"]\n",
    "    }\n",
    "\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Documents/Rag_test/grpc/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-20 13:04:49,360 - INFO - HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from extrac_text import extract_sections, extract_text\n",
    "from quadrant_insertion import create_collection, insert_vector, query_vector\n",
    "from ollama_tools import ask_llm\n",
    "from chunking import ChunkText\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_segment_insertion(pdf_path: str, extraction_pattern: str):\n",
    "    segments = extract_sections(pdf_path, extraction_pattern)\n",
    "\n",
    "    if create_collection(pdf_path):\n",
    "        print(\"Collection already exists, skipping...\")\n",
    "    \n",
    "    cont = 0\n",
    "    doc = []\n",
    "    metadata = []\n",
    "    ids = []\n",
    "    for section in segments:\n",
    "        title = section[0]\n",
    "        text = section[1]\n",
    "        chunked_text = ChunkText.fixed_window_splitter(text, 300)\n",
    "\n",
    "        section = {\"section_title\": title}\n",
    "\n",
    "        for i, chunk in enumerate(chunked_text):\n",
    "            cont += 1\n",
    "            ids.append(cont)\n",
    "            doc.append(chunk)\n",
    "            metadata.append({\"section\": title, \"chunk\": i})\n",
    "        \n",
    "    insert_vector(pdf_path, doc, metadata, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 11:30:11,228 - INFO - Loading regex patterns from regex.json\n",
      "2025-02-20 11:30:11,229 - INFO - Loaded 9 patterns\n",
      "2025-02-20 11:30:11,229 - INFO - Time to clean text: 1740061811.229937\n",
      "2025-02-20 11:30:11,230 - INFO - Time to split reference: 1740061811.230482\n",
      "2025-02-20 11:30:11,231 - INFO - Time to split sections: 1740061811.2313173\n",
      "2025-02-20 11:30:11,231 - INFO - Time to extract segments: 1740061811.2316406\n",
      "2025-02-20 11:30:11,231 - INFO - DOCUMENT SECTION DEBUG:\n",
      "2025-02-20 11:30:11,232 - INFO - I. INTRODUCTION\n",
      "2025-02-20 11:30:11,232 - INFO - II. INTUITION\n",
      "2025-02-20 11:30:11,232 - INFO - III. RESEARCH QUESTIONS AND METHODOLOGY\n",
      "2025-02-20 11:30:11,232 - INFO - IV. PERFORMANCE BEHAVIOR CONSISTENCY (RQ1)\n",
      "2025-02-20 11:30:11,233 - INFO - V. SIMILARITY OF INFLUENTIAL OPTIONS (RQ2)\n",
      "2025-02-20 11:30:11,233 - INFO - VI. PRESERVATION OF OPTION INTERACTIONS (RQ3)\n",
      "2025-02-20 11:30:11,233 - INFO - VII. INVALID CONFIGURATIONS SIMILARITY (RQ4)\n",
      "2025-02-20 11:30:11,233 - INFO - VIII. LESSONS LEARNED AND DISCUSSION\n",
      "2025-02-20 11:30:11,234 - INFO - IX. RELATED WORK\n",
      "2025-02-20 11:30:11,234 - INFO - X. CONCLUSIONS\n",
      "2025-02-20 11:30:11,235 - INFO - HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 11:30:11,237 - INFO - HTTP Request: GET http://localhost:6333/collections/jamshidi2017b.pdf \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection already exists, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 11:30:11,826 - INFO - HTTP Request: PUT http://localhost:6333/collections/jamshidi2017b.pdf/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 11:30:11,987 - INFO - HTTP Request: PUT http://localhost:6333/collections/jamshidi2017b.pdf/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 11:30:12,121 - INFO - HTTP Request: PUT http://localhost:6333/collections/jamshidi2017b.pdf/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 11:30:12,298 - INFO - HTTP Request: PUT http://localhost:6333/collections/jamshidi2017b.pdf/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 11:30:12,412 - INFO - HTTP Request: PUT http://localhost:6333/collections/jamshidi2017b.pdf/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 11:30:12,732 - INFO - HTTP Request: PUT http://localhost:6333/collections/jamshidi2017b.pdf/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 11:30:12,882 - INFO - HTTP Request: PUT http://localhost:6333/collections/jamshidi2017b.pdf/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 11:30:12,891 - INFO - HTTP Request: POST http://localhost:6333/collections/jamshidi2017b.pdf/points/search \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[QueryResponse(id=169, embedding=None, sparse_embedding=None, metadata={'document': 've metrics (e.g., different ways to establish inﬂuential\\noptions), but settled usually on the simplest and most reli-\\nable metric we could identify to keep the paper accessible\\nand within reasonable length. In addition, we only partially\\nused statistical tests, as needed, and often compared metrics\\n', 'section': 'VIII. LESSONS LEARNED AND DISCUSSION', 'chunk': 21}, document='ve metrics (e.g., different ways to establish inﬂuential\\noptions), but settled usually on the simplest and most reli-\\nable metric we could identify to keep the paper accessible\\nand within reasonable length. In addition, we only partially\\nused statistical tests, as needed, and often compared metrics\\n', score=0.87867635),\n",
       " QueryResponse(id=85, embedding=None, sparse_embedding=None, metadata={'document': 's where they trained a linear transformation to\\nderive a target model for hardware changes [51].\\nMetric. We investigate whether f(c, et) = α × f(c, es) +\\nβ, ∀c ∈C. We use metric M1: Pearson linear correlation [4]\\nbetween f(c, es) and f(c, et) to evaluate the hypothesis. If\\nthe correlation is 1, we c', 'section': 'IV. PERFORMANCE BEHAVIOR CONSISTENCY (RQ1)', 'chunk': 4}, document='s where they trained a linear transformation to\\nderive a target model for hardware changes [51].\\nMetric. We investigate whether f(c, et) = α × f(c, es) +\\nβ, ∀c ∈C. We use metric M1: Pearson linear correlation [4]\\nbetween f(c, es) and f(c, et) to evaluate the hypothesis. If\\nthe correlation is 1, we c', score=0.8764775),\n",
       " QueryResponse(id=144, embedding=None, sparse_embedding=None, metadata={'document': 'nﬁgurations before measuring them, thus decrease cost.\\nMetric. We learn a classiﬁer using multinomial logistic regres-\\nsion [4]. It is a model that is used to predict the probabilities\\nof being invalid, given a set of conﬁguration options. We\\nmeasure M18: the correlation between the coefﬁcients (i.e', 'section': 'VII. INVALID CONFIGURATIONS SIMILARITY (RQ4)', 'chunk': 6}, document='nﬁgurations before measuring them, thus decrease cost.\\nMetric. We learn a classiﬁer using multinomial logistic regres-\\nsion [4]. It is a model that is used to predict the probabilities\\nof being invalid, given a set of conﬁguration options. We\\nmeasure M18: the correlation between the coefﬁcients (i.e', score=0.86994314),\n",
       " QueryResponse(id=171, embedding=None, sparse_embedding=None, metadata={'document': '\\nFor building the performance models, calculating impor-\\ntance of conﬁguration options, and classifying the invalid\\nconﬁgurations, we elected to use different machine learn-\\ning models: step-wise linear regression, regression trees, and\\nmultinomial logistic regression. We chose these learner mainly\\n', 'section': 'VIII. LESSONS LEARNED AND DISCUSSION', 'chunk': 23}, document='\\nFor building the performance models, calculating impor-\\ntance of conﬁguration options, and classifying the invalid\\nconﬁgurations, we elected to use different machine learn-\\ning models: step-wise linear regression, regression trees, and\\nmultinomial logistic regression. We chose these learner mainly\\n', score=0.8671607),\n",
       " QueryResponse(id=44, embedding=None, sparse_embedding=None, metadata={'document': '\\nvalues, De = {yi}, e ∈E, using kernel density estimation [4]\\n(in the same way as histograms are constructed in statistics).\\n4) Inﬂuential Option: At the level of individual conﬁg-\\nuration options, we will be interested in exploring whether\\noptions have an inﬂuence on the performance of the system\\ni', 'section': 'II. INTUITION', 'chunk': 19}, document='\\nvalues, De = {yi}, e ∈E, using kernel density estimation [4]\\n(in the same way as histograms are constructed in statistics).\\n4) Inﬂuential Option: At the level of individual conﬁg-\\nuration options, we will be interested in exploring whether\\noptions have an inﬂuence on the performance of the system\\ni', score=0.862445),\n",
       " QueryResponse(id=134, embedding=None, sparse_embedding=None, metadata={'document': 'tize regions in the conﬁgu-\\nration space based on the importance of the interactions.\\nMetric. We measure M14: the correlation between the coef-\\nﬁcients of the pairwise interaction terms in the linear model\\nlearned independently on the source and target environments\\nusing step-wise linear regression ', 'section': 'VI. PRESERVATION OF OPTION INTERACTIONS (RQ3)', 'chunk': 9}, document='tize regions in the conﬁgu-\\nration space based on the importance of the interactions.\\nMetric. We measure M14: the correlation between the coef-\\nﬁcients of the pairwise interaction terms in the linear model\\nlearned independently on the source and target environments\\nusing step-wise linear regression ', score=0.8587191),\n",
       " QueryResponse(id=204, embedding=None, sparse_embedding=None, metadata={'document': 'M2: Kullback-Leibler (KL) divergence; M3: Spearman correlation; M4/M5: Perc. of top/bottom conf.; M6/M7: Number of inﬂuential options;\\nM8/M9: Number of options agree/disagree; M10: Correlation btw importance of options; M11/M12: Number of interactions; M13: Number of interactions agree on effects;\\nM', 'section': 'X. CONCLUSIONS', 'chunk': 20}, document='M2: Kullback-Leibler (KL) divergence; M3: Spearman correlation; M4/M5: Perc. of top/bottom conf.; M6/M7: Number of inﬂuential options;\\nM8/M9: Number of options agree/disagree; M10: Correlation btw importance of options; M11/M12: Number of interactions; M13: Number of interactions agree on effects;\\nM', score=0.8555181),\n",
       " QueryResponse(id=41, embedding=None, sparse_embedding=None, metadata={'document': 'e ∈E\\non various conﬁgurations ci ∈C, and record the resulting\\nperformance values yi = f(ci, e) + ϵi where ϵi ∼N(0, σi) is\\nthe measurement noise corresponding to a normal distribution\\nwith zero mean and variance σ2\\ni . The training data for learning\\na performance model for system A in environment e i', 'section': 'II. INTUITION', 'chunk': 16}, document='e ∈E\\non various conﬁgurations ci ∈C, and record the resulting\\nperformance values yi = f(ci, e) + ϵi where ϵi ∼N(0, σi) is\\nthe measurement noise corresponding to a normal distribution\\nwith zero mean and variance σ2\\ni . The training data for learning\\na performance model for system A in environment e i', score=0.85550404),\n",
       " QueryResponse(id=168, embedding=None, sparse_embedding=None, metadata={'document': '\\nmiss effects in parts of the conﬁguration space that we did\\nnot sample. We did not encounter any surprisingly different\\nobservation in our exhaustively measured SPEAR dataset.\\nWe operationalized a large number of different measures\\nthrough metrics. For each measure, we considered multiple\\nalternati', 'section': 'VIII. LESSONS LEARNED AND DISCUSSION', 'chunk': 20}, document='\\nmiss effects in parts of the conﬁguration space that we did\\nnot sample. We did not encounter any surprisingly different\\nobservation in our exhaustively measured SPEAR dataset.\\nWe operationalized a large number of different measures\\nthrough metrics. For each measure, we considered multiple\\nalternati', score=0.855448),\n",
       " QueryResponse(id=109, embedding=None, sparse_embedding=None, metadata={'document': ' is a relaxed hypothesis comparing to H1.3.\\nMetric. We measure M4/M5: the percentage of (10th per-\\ncentile) top/bottom conﬁgurations in the source that are also\\ntop/bottom performers in the target.\\nResults. The results in Table II show that top/bottom con-\\nﬁgurations are common across hardware and s', 'section': 'IV. PERFORMANCE BEHAVIOR CONSISTENCY (RQ1)', 'chunk': 28}, document=' is a relaxed hypothesis comparing to H1.3.\\nMetric. We measure M4/M5: the percentage of (10th per-\\ncentile) top/bottom conﬁgurations in the source that are also\\ntop/bottom performers in the target.\\nResults. The results in Table II show that top/bottom con-\\nﬁgurations are common across hardware and s', score=0.8549571)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pdf_name = \"jamshidi2017b.pdf\"\n",
    "# pdf_path = \"jailmaReviewPdf/\" + pdf_name\n",
    "# extracted_text = extract_text(pdf_path)\n",
    "\n",
    "# segments = extract_sections(extracted_text, \"rome_point_section\")\n",
    "\n",
    "# if create_collection(pdf_name):\n",
    "#         print(\"Collection already exists, skipping...\")\n",
    "\n",
    "# cont = 0\n",
    "# doc = []\n",
    "# metadata = []\n",
    "# ids = []\n",
    "# for section in segments:\n",
    "#     title = section[0]\n",
    "#     text = section[1]\n",
    "#     chunked_text = ChunkText.fixed_window_splitter(text, 300)\n",
    "\n",
    "#     qtd_chunks = len(chunked_text)\n",
    "\n",
    "#     section = {\"section_title\": title}\n",
    "\n",
    "#     for i, chunk in enumerate(chunked_text):\n",
    "#         cont += 1\n",
    "#         ids.append(cont)\n",
    "#         doc.append(chunk)\n",
    "#         metadata.append({\"section\": title, \"chunk\": i})\n",
    "    \n",
    "# insert_vector(pdf_name, doc, metadata, ids)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {\"guo2013\": {\n",
    "                    \"sampling\": \"random\",\n",
    "                    \"learning\": \"CART\",\n",
    "                    \"evaluation_metric\": \"MRE\",\n",
    "                    \"note\": \"\"\n",
    "                },\n",
    "                \"jamshidi2017b\": {\n",
    "                    \"sampling\": \"random\",\n",
    "                    \"learning\": \"CART\",\n",
    "                    \"evaluation_metric\": \"KL\",\n",
    "                    \"note\": \"pearson Spearman correlation\"\n",
    "                }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Which sample method is used on this paper?\", \"Which evaluation metric is used on this paper?\", \"Which learning algorithm is used on this paper?\"]\n",
    "models = [\"mistral-nemo\", \"qwen:1.8b\"]\n",
    "pdf_names = [\"guo2013.pdf\", \"jamshidi2017b.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 13:05:32,185 - INFO - HTTP Request: POST http://localhost:6333/collections/guo2013.pdf/points/search \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 13:05:32,195 - INFO - HTTP Request: POST http://localhost:6333/collections/guo2013.pdf/points/search \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 13:05:32,203 - INFO - HTTP Request: POST http://localhost:6333/collections/guo2013.pdf/points/search \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 13:05:32,210 - INFO - HTTP Request: POST http://localhost:6333/collections/guo2013.pdf/points/search \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 13:05:32,217 - INFO - HTTP Request: POST http://localhost:6333/collections/jamshidi2017b.pdf/points/search \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 13:05:32,225 - INFO - HTTP Request: POST http://localhost:6333/collections/jamshidi2017b.pdf/points/search \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 13:05:32,233 - INFO - HTTP Request: POST http://localhost:6333/collections/jamshidi2017b.pdf/points/search \"HTTP/1.1 200 OK\"\n",
      "2025-02-20 13:05:32,239 - INFO - HTTP Request: POST http://localhost:6333/collections/jamshidi2017b.pdf/points/search \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "context_dict = {}\n",
    "for pdf_name in pdf_names:\n",
    "    for question in questions:\n",
    "        response_list = query_vector(pdf_name, question)\n",
    "        context = ''\n",
    "        for response in response_list:\n",
    "            context += response.metadata['document']\n",
    "        context_dict[pdf_name + question] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request data: {'model': 'mistral-nemo', 'prompt': 'Context:\\nrs deﬁned in Equation 4.\\nMoreover, the time of at most 90ms needed for all six subject\\nsystems and for any sample size from N to M demonstrates\\nthat our approach is highly efﬁcient for variability-aware\\nperformance prediction.\\nE. Comparative Analysis of Performance Distributions\\nThe previous experimI(x14 = 1, x7 = 1)\\n+ 402 ∗I(x14 = 0, x15 = 1, x3 = 0)\\n+ 508 ∗I(x14 = 0, x15 = 1, x3 = 1)\\n+ 571 ∗I(x14 = 0, x15 = 0, x3 = 1)\\n+ 626 ∗I(x14 = 0, x15 = 0, x3 = 0)\\nWe can derive a set of decision rules from a global perfor-\\nmance model to provide direct performance predictions for\\nusers. Each branch in t\\nrepeated each measurement (the prediction fault rate or the\\ntime cost) for each system and each sample size ﬁve times,\\nand took only the average of these measurements for analysis.\\nTo increase external validity, we used a public dataset with\\nsix systems spanning different domains, with different si built performance models and generate\\ndecision rules. We also experimented with two CART variants,\\nRandom Forests and Boosting [26], which try to enhance the\\nprediction effects of CART, but we observed similar prediction\\nimprovements on our evaluated case studies through parameter\\ntuning. Thus, we = 571\\n{x1, x6, x11}\\nSRRR\\nℓSRRR = 626\\nyes\\nno\\nSR\\nx15 = 1?\\nyes\\nno\\nx14 = 1?\\nyes\\nno\\nFigure 2. Example performance model of X264 generated by CART based on the random sample of Table I\\nexhaustively searching over all feature-selection variables in X\\nfor the best split that minimizes the total prediction eting\\nand overﬁtting for our case studies. Moreover, to implement a\\nfully-automated process of performance modeling by CART,\\nwe aim at setting the two parameters automatically in terms of\\nthe size of the input sample, i.e., |S|. Since the size of most of\\nthe samples used in our case studies is less t We compare our method with an existing technique\\nthat relies on heuristics and feature coverage [19]. In\\nparticular, we discuss the strengths and weaknesses of\\nthe two approaches and guide users to choose one or the\\nother in practice.\\nThe implementation of the approach and all experimental\\ndata are feature selections and\\nperformance and to guide the conﬁguration process [15]. In\\nfuture work, we aim at performing systematic parameter tuning\\nfor CART and trying other regression techniques (e.g., Sup-\\nport Vector Machines [4]). Moreover, we aim at quantifying\\nthe similarity between a sample and We implemented our approach using R 2.15.1 and JAVA\\n(ECLIPSE 4.2 with JVM 1.7). R is a language and environ-\\nment for statistical computing and graphics.4 We used the R\\npackages RATTLE and RPART to implement CART and to\\ngenerate the performance models [26]. We developed a rule\\ngenerator to parse the●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\\n3\\n3\\n3\\n3\\n0\\n10\\n20\\n30\\n40\\nFault Rate (x100%)\\nN\\n2N\\n3N\\nM\\n(b) Including outliers\\nFigure 3. Boxplots of the prediction fault rates for the six systems (1 to 6\\nlisted in Table II) and the four sample sizes (N to M listed in Table II)\\n“Fault Rate”) lists the mean and sta\\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich metrics and ML models are used on this paper?'}\n",
      "Request data: {'model': 'mistral-nemo', 'prompt': 'Context:\\nhe prediction process? (Section VI-D)\\nRQ 4: Is it possible to make accurate predictions using only\\nsmall random samples? (Section VI-E)\\nRQ 5: What are the strengths and weaknesses of our\\napproach compared to existing techniques? (Sec-\\ntion VI-G)\\nA. Subject Systems\\nWe performed our case studies on a  whole\\npopulation can be found on the random sample of size N.\\n3) Discussion: The comparative analysis of performance\\ndistributions between random samples and the whole popula-\\ntions reveals that our approach works well with a small random\\nsample when the sample has a similar performance distributioall random samples. Moreover, our approach\\nshows a robust increasing trend of prediction accuracy as the\\nsample size increases. A comparative analysis of performance\\ndistributions revealed that our approach works well when the\\ncorresponding sample has a similar performance distribution as\\nthe whole esis.\\n1) Hypothesis: Our approach works well with a small\\nrandom sample when the sample has a similar performance\\ndistribution as the whole population.\\n2) Results: For each subject system, we collected all the\\nrandom samples generated in the previous experiments. Then,\\nwe visualized the average perfes.\\n• We conduct a comparative analysis of performance dis-\\ntributions on the evaluated case studies and empirically\\nexplore why the approach works with small random\\nsamples.1 A key ﬁnding is that it works well when the\\nsample it uses has a performance distribution similar to\\nthe whole population.\\n• We compare our method with an existing technique\\nthat relies on heuristics and feature coverage [19]. In\\nparticular, we discuss the strengths and weaknesses of\\nthe two approaches and guide users to choose one or the\\nother in practice.\\nThe implementation of the approach and all experimental\\ndata are.\\nTo assess the effectiveness of our approach working with\\nrandom samples of different sizes, we use four sizes for the\\ntraining sample of each subject system: N, 2N, 3N, and M,\\nwhere N is the number of all features of each system, and\\nM is the number of all speciﬁc conﬁgurations required by the\\npaion,\\nshall be conducted in future work.\\nF. Threats to Validity\\nTo enhance internal validity, we performed automated ran-\\ndom sampling avoiding misleading effects of speciﬁc-selected\\ntraining samples and test samples. We randomly selected\\nsamples of four sizes (N to M) respectively from the whole\\npopulation of each subject system as the training sample, and\\nall of the rest as the test sample. We repeated each random\\nsampling ﬁve times with freshly generated training samples\\nand test samples of the same size. The exception is the test\\nsample of SQLITE, in which the original authors could not\\nmeashen we apply\\nthem to the (random or speciﬁc) samples of size N and M\\nfor the six subject systems listed in Table II.\\nOur approach produces a prediction fault rate of 8 % or\\nless based on a random sample of size N for three systems\\n(Rows 2, 5, and 6 in Table IV); and it produces an average\\nof 6.2 % p\\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich sample method is used on this paper?'}\n",
      "Request data: {'model': 'mistral-nemo', 'prompt': 'Context:\\nrs deﬁned in Equation 4.\\nMoreover, the time of at most 90ms needed for all six subject\\nsystems and for any sample size from N to M demonstrates\\nthat our approach is highly efﬁcient for variability-aware\\nperformance prediction.\\nE. Comparative Analysis of Performance Distributions\\nThe previous experim\\nrepeated each measurement (the prediction fault rate or the\\ntime cost) for each system and each sample size ﬁve times,\\nand took only the average of these measurements for analysis.\\nTo increase external validity, we used a public dataset with\\nsix systems spanning different domains, with different si●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\\n3\\n3\\n3\\n3\\n0\\n10\\n20\\n30\\n40\\nFault Rate (x100%)\\nN\\n2N\\n3N\\nM\\n(b) Including outliers\\nFigure 3. Boxplots of the prediction fault rates for the six systems (1 to 6\\nlisted in Table II) and the four sample sizes (N to M listed in Table II)\\n“Fault Rate”) lists the mean and staI(x14 = 1, x7 = 1)\\n+ 402 ∗I(x14 = 0, x15 = 1, x3 = 0)\\n+ 508 ∗I(x14 = 0, x15 = 1, x3 = 1)\\n+ 571 ∗I(x14 = 0, x15 = 0, x3 = 1)\\n+ 626 ∗I(x14 = 0, x15 = 0, x3 = 0)\\nWe can derive a set of decision rules from a global perfor-\\nmance model to provide direct performance predictions for\\nusers. Each branch in teen XS and\\nYS and that makes each conﬁguration’s predicted performance\\nf(x) as close as possible to its actual performance y, i.e.:\\nf : X →R such that\\nX\\nx,y∈S\\nL(y, f(x)) is minimal\\n(1)\\nwhere L is a loss function to penalize errors in prediction.\\nAn assumption of our approach is that the sample S andes.\\n• We conduct a comparative analysis of performance dis-\\ntributions on the evaluated case studies and empirically\\nexplore why the approach works with small random\\nsamples.1 A key ﬁnding is that it works well when the\\nsample it uses has a performance distribution similar to\\nthe whole population.\\n•ed it to our\\napproach and discussed the strengths and weaknesses of both\\napproaches in Section VI-G.\\nSincero et al. [22] used existing conﬁgurations and measure-\\nments to predict a conﬁguration’s non-functional properties.\\nThey designed the Feedback approach to ﬁnd the correlation\\nbetween feature se= 571\\n{x1, x6, x11}\\nSRRR\\nℓSRRR = 626\\nyes\\nno\\nSR\\nx15 = 1?\\nyes\\nno\\nx14 = 1?\\nyes\\nno\\nFigure 2. Example performance model of X264 generated by CART based on the random sample of Table I\\nexhaustively searching over all feature-selection variables in X\\nfor the best split that minimizes the total prediction eormance distribution for each\\nsample size from N to M, to mitigate the inﬂuence of a\\nspeciﬁc performance distribution of a certain random sample.\\nFurthermore, we visualized the performance distribution of the\\nwhole population of each system, to be able to compare it to\\nthe performance distribution o built performance models and generate\\ndecision rules. We also experimented with two CART variants,\\nRandom Forests and Boosting [26], which try to enhance the\\nprediction effects of CART, but we observed similar prediction\\nimprovements on our evaluated case studies through parameter\\ntuning. Thus, we \\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich evaluation metric is used on this paper?'}\n",
      "Request data: {'model': 'mistral-nemo', 'prompt': 'Context:\\nn-linearly on one or more predictors [11]. We compare our method with an existing technique\\nthat relies on heuristics and feature coverage [19]. In\\nparticular, we discuss the strengths and weaknesses of\\nthe two approaches and guide users to choose one or the\\nother in practice.\\nThe implementation of the approach and all experimental\\ndata are built performance models and generate\\ndecision rules. We also experimented with two CART variants,\\nRandom Forests and Boosting [26], which try to enhance the\\nprediction effects of CART, but we observed similar prediction\\nimprovements on our evaluated case studies through parameter\\ntuning. Thus, we een XS and\\nYS and that makes each conﬁguration’s predicted performance\\nf(x) as close as possible to its actual performance y, i.e.:\\nf : X →R such that\\nX\\nx,y∈S\\nL(y, f(x)) is minimal\\n(1)\\nwhere L is a loss function to penalize errors in prediction.\\nAn assumption of our approach is that the sample S and= 571\\n{x1, x6, x11}\\nSRRR\\nℓSRRR = 626\\nyes\\nno\\nSR\\nx15 = 1?\\nyes\\nno\\nx14 = 1?\\nyes\\nno\\nFigure 2. Example performance model of X264 generated by CART based on the random sample of Table I\\nexhaustively searching over all feature-selection variables in X\\nfor the best split that minimizes the total prediction eI(x14 = 1, x7 = 1)\\n+ 402 ∗I(x14 = 0, x15 = 1, x3 = 0)\\n+ 508 ∗I(x14 = 0, x15 = 1, x3 = 1)\\n+ 571 ∗I(x14 = 0, x15 = 0, x3 = 1)\\n+ 626 ∗I(x14 = 0, x15 = 0, x3 = 0)\\nWe can derive a set of decision rules from a global perfor-\\nmance model to provide direct performance predictions for\\nusers. Each branch in ting starts with a random sample. We use the sample to\\nbuild a performance model automatically by statistical learn-\\ning. From the performance model, we derive a set of decision\\nrules to enable fast, direct question answering for performance\\nprediction. To validate the current performance model, userFault Rate\\nCART has been proved effective for many non-linear re-\\ngression problems [4], [5]. Moreover, most statistical learning\\ntechniques can make more accurate predictions when more da-\\nta are available [11]. Hence, the hypotheses of this experiment\\nare as follows.\\n1) Hypotheses: Our CART-based  and generality by experiments on six real-world\\nconﬁgurable software systems. The results show that the\\napproach produces an average prediction accuracy of\\n94 %, based on only small random samples. Moreover,\\nwe observe a desirable increasing trend of prediction\\naccuracy when the sample size increas feature selections and\\nperformance and to guide the conﬁguration process [15]. In\\nfuture work, we aim at performing systematic parameter tuning\\nfor CART and trying other regression techniques (e.g., Sup-\\nport Vector Machines [4]). Moreover, we aim at quantifying\\nthe similarity between a sample and \\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich learning algorithm is used on this paper?'}\n",
      "Request data: {'model': 'mistral-nemo', 'prompt': 'Context:\\nve metrics (e.g., different ways to establish inﬂuential\\noptions), but settled usually on the simplest and most reli-\\nable metric we could identify to keep the paper accessible\\nand within reasonable length. In addition, we only partially\\nused statistical tests, as needed, and often compared metrics\\ns where they trained a linear transformation to\\nderive a target model for hardware changes [51].\\nMetric. We investigate whether f(c, et) = α × f(c, es) +\\nβ, ∀c ∈C. We use metric M1: Pearson linear correlation [4]\\nbetween f(c, es) and f(c, et) to evaluate the hypothesis. If\\nthe correlation is 1, we cnﬁgurations before measuring them, thus decrease cost.\\nMetric. We learn a classiﬁer using multinomial logistic regres-\\nsion [4]. It is a model that is used to predict the probabilities\\nof being invalid, given a set of conﬁguration options. We\\nmeasure M18: the correlation between the coefﬁcients (i.e\\nFor building the performance models, calculating impor-\\ntance of conﬁguration options, and classifying the invalid\\nconﬁgurations, we elected to use different machine learn-\\ning models: step-wise linear regression, regression trees, and\\nmultinomial logistic regression. We chose these learner mainly\\n\\nvalues, De = {yi}, e ∈E, using kernel density estimation [4]\\n(in the same way as histograms are constructed in statistics).\\n4) Inﬂuential Option: At the level of individual conﬁg-\\nuration options, we will be interested in exploring whether\\noptions have an inﬂuence on the performance of the system\\nitize regions in the conﬁgu-\\nration space based on the importance of the interactions.\\nMetric. We measure M14: the correlation between the coef-\\nﬁcients of the pairwise interaction terms in the linear model\\nlearned independently on the source and target environments\\nusing step-wise linear regression M2: Kullback-Leibler (KL) divergence; M3: Spearman correlation; M4/M5: Perc. of top/bottom conf.; M6/M7: Number of inﬂuential options;\\nM8/M9: Number of options agree/disagree; M10: Correlation btw importance of options; M11/M12: Number of interactions; M13: Number of interactions agree on effects;\\nMe ∈E\\non various conﬁgurations ci ∈C, and record the resulting\\nperformance values yi = f(ci, e) + ϵi where ϵi ∼N(0, σi) is\\nthe measurement noise corresponding to a normal distribution\\nwith zero mean and variance σ2\\ni . The training data for learning\\na performance model for system A in environment e i\\nmiss effects in parts of the conﬁguration space that we did\\nnot sample. We did not encounter any surprisingly different\\nobservation in our exhaustively measured SPEAR dataset.\\nWe operationalized a large number of different measures\\nthrough metrics. For each measure, we considered multiple\\nalternati is a relaxed hypothesis comparing to H1.3.\\nMetric. We measure M4/M5: the percentage of (10th per-\\ncentile) top/bottom conﬁgurations in the source that are also\\ntop/bottom performers in the target.\\nResults. The results in Table II show that top/bottom con-\\nﬁgurations are common across hardware and s\\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich metrics and ML models are used on this paper?'}\n",
      "Request data: {'model': 'mistral-nemo', 'prompt': 'Context:\\nze the amount of measurement efforts.\\nSampling strategies based on experimental design (such as\\nPlackett-Burman) have been applied in the domain of con-\\nﬁgurable systems [15], [43], [44]. The aim of these sampling\\napproaches is to ensure that we gain a high level of information\\nfrom sparse sampling \\nvalues, De = {yi}, e ∈E, using kernel density estimation [4]\\n(in the same way as histograms are constructed in statistics).\\n4) Inﬂuential Option: At the level of individual conﬁg-\\nuration options, we will be interested in exploring whether\\noptions have an inﬂuence on the performance of the system\\nisampling. Our empirical study demonstrates the ex-\\nistence of diverse forms of transferable knowledge across\\nenvironments that can contribute to learning faster, better,\\nreliable, and more important, less costly performance models.\\nACKNOWLEDGMENT\\nThis work has been supported by AFRL and DARPA\\n(FA875imization and evolu-\\ntionary algorithms [16], [54] have also been used.\\nOur work is related to the performance analysis research\\nmentioned above. However, we do not perform a comparison\\nof different models, conﬁguration optimization or sampling\\nstrategies. Instead, we concentrate on transferring perve metrics (e.g., different ways to establish inﬂuential\\noptions), but settled usually on the simplest and most reli-\\nable metric we could identify to keep the paper accessible\\nand within reasonable length. In addition, we only partially\\nused statistical tests, as needed, and often compared metrics\\nexpect research opportunities regarding:\\n1) Sampling strategies to exploit the relatedness of environ-\\nments to select informative samples using the importance\\nof speciﬁc regions [40] or avoiding invalid conﬁgurations.\\n2) Learning mechanisms to exploit the relatedness across\\nenvironments and learn eategies can take existing domain knowledge into\\naccount and could beneﬁt from knowledge about inﬂuen-\\ntial options and interactions [44], [45], it is less obvious\\nhow to effectively incorporate such knowledge into sampling\\nstrategies and how to build more effective learners based on\\nlimited transfere ∈E\\non various conﬁgurations ci ∈C, and record the resulting\\nperformance values yi = f(ci, e) + ϵi where ϵi ∼N(0, σi) is\\nthe measurement noise corresponding to a normal distribution\\nwith zero mean and variance σ2\\ni . The training data for learning\\na performance model for system A in environment e it with the presented\\ndata and did not provide additional insights.\\n2) Internal and Construct Validity: Due to the size of\\nconﬁguration spaces, we could only measure conﬁgurations\\nexhaustively in one subject system and had to rely on sampling\\n(with substantial sampling size) for the others, which may\\nsubsequently relax the hypothesis to test whether and when\\nthe performance distributions are similar (H1.2), whether the\\nranking of conﬁgurations (H1.3), and the top/bottom conﬁgu-\\nrations (H1.4) stay consistent. Table II summarizes the results.\\nH1.1: The relation of the source response to the targ\\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich sample method is used on this paper?'}\n",
      "Request data: {'model': 'mistral-nemo', 'prompt': 'Context:\\nve metrics (e.g., different ways to establish inﬂuential\\noptions), but settled usually on the simplest and most reli-\\nable metric we could identify to keep the paper accessible\\nand within reasonable length. In addition, we only partially\\nused statistical tests, as needed, and often compared metrics\\nnﬁgurations before measuring them, thus decrease cost.\\nMetric. We learn a classiﬁer using multinomial logistic regres-\\nsion [4]. It is a model that is used to predict the probabilities\\nof being invalid, given a set of conﬁguration options. We\\nmeasure M18: the correlation between the coefﬁcients (i.e exponential\\nin the number of options and it is computationally infeasible\\nto get measurements aiming at learning an exhaustive number\\nof interactions. Prior work has shown that a very large portion\\nof potential interactions has no inﬂuence [29], [45].\\nMetric. One key objective here is to evaluate ts where they trained a linear transformation to\\nderive a target model for hardware changes [51].\\nMetric. We investigate whether f(c, et) = α × f(c, es) +\\nβ, ∀c ∈C. We use metric M1: Pearson linear correlation [4]\\nbetween f(c, es) and f(c, et) to evaluate the hypothesis. If\\nthe correlation is 1, we c\\nvalues, De = {yi}, e ∈E, using kernel density estimation [4]\\n(in the same way as histograms are constructed in statistics).\\n4) Inﬂuential Option: At the level of individual conﬁg-\\nuration options, we will be interested in exploring whether\\noptions have an inﬂuence on the performance of the system\\nidirectly using more informal comparisons and some ad-hoc\\nthreshold for detecting common patterns across environments.\\nA different operationalization may lead to different results,\\nbut since our results are consistent across a large number of\\nmeasures, we do not expect any changes to the big picture.\\nmiss effects in parts of the conﬁguration space that we did\\nnot sample. We did not encounter any surprisingly different\\nobservation in our exhaustively measured SPEAR dataset.\\nWe operationalized a large number of different measures\\nthrough metrics. For each measure, we considered multiple\\nalternatitions across environments to avoid exploration of\\ninvalid regions and reduce measurement effort.\\nMetric. We measure M15/M16: percentage of invalid conﬁg-\\nurations in the source and target, M17: percentage of invalid\\nconﬁgurations, which are common between environments.\\nResults. The results in Table inear mapping\\nbetween the two responses, but, there might be a more\\nsophisticated relationship between the two environments that\\ncan be captured by a non-linear transfer function.\\nMetric. We measure M2: Kullback-Leibler (KL) divergence\\n[8] to compare the similarity between the performance dis-\\ntribu\\nbecause we already know that these are the key options\\ninﬂuencing performance.\\nMetric. In order to investigate the option-speciﬁc effects,\\nwe use a paired t-test [4] to test if an option leads to any\\nsigniﬁcant performance change and whether this change is\\nsimilar across environments. That is, when\\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich evaluation metric is used on this paper?'}\n",
      "Request data: {'model': 'mistral-nemo', 'prompt': 'Context:\\nboth environments [51].\\nMore sophisticated transfer learning exists that reuses source\\ndata using learners such as Gaussian Processes (GP) [25]. the context of MapReduce applications [61],\\nperformance-anomaly detection [46], micro-benchmarking on\\ndifferent hardware [22], parameter dependencies [64], and\\nperformance prediction based on similarity search [48].\\nRecently, transfer learning is used in systems and soft-\\nware engineering. For exam\\nFor building the performance models, calculating impor-\\ntance of conﬁguration options, and classifying the invalid\\nconﬁgurations, we elected to use different machine learn-\\ning models: step-wise linear regression, regression trees, and\\nmultinomial logistic regression. We chose these learner mainly\\nsampling. Our empirical study demonstrates the ex-\\nistence of diverse forms of transferable knowledge across\\nenvironments that can contribute to learning faster, better,\\nreliable, and more important, less costly performance models.\\nACKNOWLEDGMENT\\nThis work has been supported by AFRL and DARPA\\n(FA875The aim of optimization\\napproaches is to ﬁnd the optimal conﬁguration in a highly\\ndimensional space using only a limited sampling budget.\\nMachine learning techniques, such as support-vector ma-\\nchines [61], decision trees [33], Fourier sparse functions [63],\\nactive learning [44] and search-based optnce models, showing broad possibilities of\\napplying transfer learning beyond the relatively small changes\\nexplored in existing work (e.g., small hardware changes [51],\\nlow ﬁdelity simulations [25], similar systems [7]).\\nOverall, our contributions are the following:\\n• We formulate a series of hypotheari-\\nables. Intuitively, the Spearman correlation will be high when\\nobservations have a similar rank. We consider rank correlations\\nhigher than 0.9 as strong and suitable for transfer learning.\\nResults. The results in Table II show that the rank correlations\\nare high across hardware changes and smalansfer learning in the context\\nof select hardware changes [7], [25], [51], we more broadly\\nempirically investigate why and when transfer learning works.\\nThat is, we provide evidence why and when other techniques\\nare applicable for which environmental changes.\\nTransfer learning has also been applied an linearly transform performance\\nmodels. Due to measurement noise, we do not expect perfect\\ncorrelation, but we expect, for correlations higher than 0.9,\\nsimple transfer learning can produce good predictions.\\nResults. The result in Table II show very high correlations\\nfor about a third of all studie ∈E\\non various conﬁgurations ci ∈C, and record the resulting\\nperformance values yi = f(ci, e) + ϵi where ϵi ∼N(0, σi) is\\nthe measurement noise corresponding to a normal distribution\\nwith zero mean and variance σ2\\ni . The training data for learning\\na performance model for system A in environment e i\\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich learning algorithm is used on this paper?'}\n",
      "Request data: {'model': 'qwen:1.8b', 'prompt': 'Context:\\nrs deﬁned in Equation 4.\\nMoreover, the time of at most 90ms needed for all six subject\\nsystems and for any sample size from N to M demonstrates\\nthat our approach is highly efﬁcient for variability-aware\\nperformance prediction.\\nE. Comparative Analysis of Performance Distributions\\nThe previous experimI(x14 = 1, x7 = 1)\\n+ 402 ∗I(x14 = 0, x15 = 1, x3 = 0)\\n+ 508 ∗I(x14 = 0, x15 = 1, x3 = 1)\\n+ 571 ∗I(x14 = 0, x15 = 0, x3 = 1)\\n+ 626 ∗I(x14 = 0, x15 = 0, x3 = 0)\\nWe can derive a set of decision rules from a global perfor-\\nmance model to provide direct performance predictions for\\nusers. Each branch in t\\nrepeated each measurement (the prediction fault rate or the\\ntime cost) for each system and each sample size ﬁve times,\\nand took only the average of these measurements for analysis.\\nTo increase external validity, we used a public dataset with\\nsix systems spanning different domains, with different si built performance models and generate\\ndecision rules. We also experimented with two CART variants,\\nRandom Forests and Boosting [26], which try to enhance the\\nprediction effects of CART, but we observed similar prediction\\nimprovements on our evaluated case studies through parameter\\ntuning. Thus, we = 571\\n{x1, x6, x11}\\nSRRR\\nℓSRRR = 626\\nyes\\nno\\nSR\\nx15 = 1?\\nyes\\nno\\nx14 = 1?\\nyes\\nno\\nFigure 2. Example performance model of X264 generated by CART based on the random sample of Table I\\nexhaustively searching over all feature-selection variables in X\\nfor the best split that minimizes the total prediction eting\\nand overﬁtting for our case studies. Moreover, to implement a\\nfully-automated process of performance modeling by CART,\\nwe aim at setting the two parameters automatically in terms of\\nthe size of the input sample, i.e., |S|. Since the size of most of\\nthe samples used in our case studies is less t We compare our method with an existing technique\\nthat relies on heuristics and feature coverage [19]. In\\nparticular, we discuss the strengths and weaknesses of\\nthe two approaches and guide users to choose one or the\\nother in practice.\\nThe implementation of the approach and all experimental\\ndata are feature selections and\\nperformance and to guide the conﬁguration process [15]. In\\nfuture work, we aim at performing systematic parameter tuning\\nfor CART and trying other regression techniques (e.g., Sup-\\nport Vector Machines [4]). Moreover, we aim at quantifying\\nthe similarity between a sample and We implemented our approach using R 2.15.1 and JAVA\\n(ECLIPSE 4.2 with JVM 1.7). R is a language and environ-\\nment for statistical computing and graphics.4 We used the R\\npackages RATTLE and RPART to implement CART and to\\ngenerate the performance models [26]. We developed a rule\\ngenerator to parse the●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\\n3\\n3\\n3\\n3\\n0\\n10\\n20\\n30\\n40\\nFault Rate (x100%)\\nN\\n2N\\n3N\\nM\\n(b) Including outliers\\nFigure 3. Boxplots of the prediction fault rates for the six systems (1 to 6\\nlisted in Table II) and the four sample sizes (N to M listed in Table II)\\n“Fault Rate”) lists the mean and sta\\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich metrics and ML models are used on this paper?'}\n",
      "Request data: {'model': 'qwen:1.8b', 'prompt': 'Context:\\nhe prediction process? (Section VI-D)\\nRQ 4: Is it possible to make accurate predictions using only\\nsmall random samples? (Section VI-E)\\nRQ 5: What are the strengths and weaknesses of our\\napproach compared to existing techniques? (Sec-\\ntion VI-G)\\nA. Subject Systems\\nWe performed our case studies on a  whole\\npopulation can be found on the random sample of size N.\\n3) Discussion: The comparative analysis of performance\\ndistributions between random samples and the whole popula-\\ntions reveals that our approach works well with a small random\\nsample when the sample has a similar performance distributioall random samples. Moreover, our approach\\nshows a robust increasing trend of prediction accuracy as the\\nsample size increases. A comparative analysis of performance\\ndistributions revealed that our approach works well when the\\ncorresponding sample has a similar performance distribution as\\nthe whole esis.\\n1) Hypothesis: Our approach works well with a small\\nrandom sample when the sample has a similar performance\\ndistribution as the whole population.\\n2) Results: For each subject system, we collected all the\\nrandom samples generated in the previous experiments. Then,\\nwe visualized the average perfes.\\n• We conduct a comparative analysis of performance dis-\\ntributions on the evaluated case studies and empirically\\nexplore why the approach works with small random\\nsamples.1 A key ﬁnding is that it works well when the\\nsample it uses has a performance distribution similar to\\nthe whole population.\\n• We compare our method with an existing technique\\nthat relies on heuristics and feature coverage [19]. In\\nparticular, we discuss the strengths and weaknesses of\\nthe two approaches and guide users to choose one or the\\nother in practice.\\nThe implementation of the approach and all experimental\\ndata are.\\nTo assess the effectiveness of our approach working with\\nrandom samples of different sizes, we use four sizes for the\\ntraining sample of each subject system: N, 2N, 3N, and M,\\nwhere N is the number of all features of each system, and\\nM is the number of all speciﬁc conﬁgurations required by the\\npaion,\\nshall be conducted in future work.\\nF. Threats to Validity\\nTo enhance internal validity, we performed automated ran-\\ndom sampling avoiding misleading effects of speciﬁc-selected\\ntraining samples and test samples. We randomly selected\\nsamples of four sizes (N to M) respectively from the whole\\npopulation of each subject system as the training sample, and\\nall of the rest as the test sample. We repeated each random\\nsampling ﬁve times with freshly generated training samples\\nand test samples of the same size. The exception is the test\\nsample of SQLITE, in which the original authors could not\\nmeashen we apply\\nthem to the (random or speciﬁc) samples of size N and M\\nfor the six subject systems listed in Table II.\\nOur approach produces a prediction fault rate of 8 % or\\nless based on a random sample of size N for three systems\\n(Rows 2, 5, and 6 in Table IV); and it produces an average\\nof 6.2 % p\\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich sample method is used on this paper?'}\n",
      "Request data: {'model': 'qwen:1.8b', 'prompt': 'Context:\\nrs deﬁned in Equation 4.\\nMoreover, the time of at most 90ms needed for all six subject\\nsystems and for any sample size from N to M demonstrates\\nthat our approach is highly efﬁcient for variability-aware\\nperformance prediction.\\nE. Comparative Analysis of Performance Distributions\\nThe previous experim\\nrepeated each measurement (the prediction fault rate or the\\ntime cost) for each system and each sample size ﬁve times,\\nand took only the average of these measurements for analysis.\\nTo increase external validity, we used a public dataset with\\nsix systems spanning different domains, with different si●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\\n3\\n3\\n3\\n3\\n0\\n10\\n20\\n30\\n40\\nFault Rate (x100%)\\nN\\n2N\\n3N\\nM\\n(b) Including outliers\\nFigure 3. Boxplots of the prediction fault rates for the six systems (1 to 6\\nlisted in Table II) and the four sample sizes (N to M listed in Table II)\\n“Fault Rate”) lists the mean and staI(x14 = 1, x7 = 1)\\n+ 402 ∗I(x14 = 0, x15 = 1, x3 = 0)\\n+ 508 ∗I(x14 = 0, x15 = 1, x3 = 1)\\n+ 571 ∗I(x14 = 0, x15 = 0, x3 = 1)\\n+ 626 ∗I(x14 = 0, x15 = 0, x3 = 0)\\nWe can derive a set of decision rules from a global perfor-\\nmance model to provide direct performance predictions for\\nusers. Each branch in teen XS and\\nYS and that makes each conﬁguration’s predicted performance\\nf(x) as close as possible to its actual performance y, i.e.:\\nf : X →R such that\\nX\\nx,y∈S\\nL(y, f(x)) is minimal\\n(1)\\nwhere L is a loss function to penalize errors in prediction.\\nAn assumption of our approach is that the sample S andes.\\n• We conduct a comparative analysis of performance dis-\\ntributions on the evaluated case studies and empirically\\nexplore why the approach works with small random\\nsamples.1 A key ﬁnding is that it works well when the\\nsample it uses has a performance distribution similar to\\nthe whole population.\\n•ed it to our\\napproach and discussed the strengths and weaknesses of both\\napproaches in Section VI-G.\\nSincero et al. [22] used existing conﬁgurations and measure-\\nments to predict a conﬁguration’s non-functional properties.\\nThey designed the Feedback approach to ﬁnd the correlation\\nbetween feature se= 571\\n{x1, x6, x11}\\nSRRR\\nℓSRRR = 626\\nyes\\nno\\nSR\\nx15 = 1?\\nyes\\nno\\nx14 = 1?\\nyes\\nno\\nFigure 2. Example performance model of X264 generated by CART based on the random sample of Table I\\nexhaustively searching over all feature-selection variables in X\\nfor the best split that minimizes the total prediction eormance distribution for each\\nsample size from N to M, to mitigate the inﬂuence of a\\nspeciﬁc performance distribution of a certain random sample.\\nFurthermore, we visualized the performance distribution of the\\nwhole population of each system, to be able to compare it to\\nthe performance distribution o built performance models and generate\\ndecision rules. We also experimented with two CART variants,\\nRandom Forests and Boosting [26], which try to enhance the\\nprediction effects of CART, but we observed similar prediction\\nimprovements on our evaluated case studies through parameter\\ntuning. Thus, we \\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich evaluation metric is used on this paper?'}\n",
      "Request data: {'model': 'qwen:1.8b', 'prompt': 'Context:\\nn-linearly on one or more predictors [11]. We compare our method with an existing technique\\nthat relies on heuristics and feature coverage [19]. In\\nparticular, we discuss the strengths and weaknesses of\\nthe two approaches and guide users to choose one or the\\nother in practice.\\nThe implementation of the approach and all experimental\\ndata are built performance models and generate\\ndecision rules. We also experimented with two CART variants,\\nRandom Forests and Boosting [26], which try to enhance the\\nprediction effects of CART, but we observed similar prediction\\nimprovements on our evaluated case studies through parameter\\ntuning. Thus, we een XS and\\nYS and that makes each conﬁguration’s predicted performance\\nf(x) as close as possible to its actual performance y, i.e.:\\nf : X →R such that\\nX\\nx,y∈S\\nL(y, f(x)) is minimal\\n(1)\\nwhere L is a loss function to penalize errors in prediction.\\nAn assumption of our approach is that the sample S and= 571\\n{x1, x6, x11}\\nSRRR\\nℓSRRR = 626\\nyes\\nno\\nSR\\nx15 = 1?\\nyes\\nno\\nx14 = 1?\\nyes\\nno\\nFigure 2. Example performance model of X264 generated by CART based on the random sample of Table I\\nexhaustively searching over all feature-selection variables in X\\nfor the best split that minimizes the total prediction eI(x14 = 1, x7 = 1)\\n+ 402 ∗I(x14 = 0, x15 = 1, x3 = 0)\\n+ 508 ∗I(x14 = 0, x15 = 1, x3 = 1)\\n+ 571 ∗I(x14 = 0, x15 = 0, x3 = 1)\\n+ 626 ∗I(x14 = 0, x15 = 0, x3 = 0)\\nWe can derive a set of decision rules from a global perfor-\\nmance model to provide direct performance predictions for\\nusers. Each branch in ting starts with a random sample. We use the sample to\\nbuild a performance model automatically by statistical learn-\\ning. From the performance model, we derive a set of decision\\nrules to enable fast, direct question answering for performance\\nprediction. To validate the current performance model, userFault Rate\\nCART has been proved effective for many non-linear re-\\ngression problems [4], [5]. Moreover, most statistical learning\\ntechniques can make more accurate predictions when more da-\\nta are available [11]. Hence, the hypotheses of this experiment\\nare as follows.\\n1) Hypotheses: Our CART-based  and generality by experiments on six real-world\\nconﬁgurable software systems. The results show that the\\napproach produces an average prediction accuracy of\\n94 %, based on only small random samples. Moreover,\\nwe observe a desirable increasing trend of prediction\\naccuracy when the sample size increas feature selections and\\nperformance and to guide the conﬁguration process [15]. In\\nfuture work, we aim at performing systematic parameter tuning\\nfor CART and trying other regression techniques (e.g., Sup-\\nport Vector Machines [4]). Moreover, we aim at quantifying\\nthe similarity between a sample and \\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich learning algorithm is used on this paper?'}\n",
      "Request data: {'model': 'qwen:1.8b', 'prompt': 'Context:\\nve metrics (e.g., different ways to establish inﬂuential\\noptions), but settled usually on the simplest and most reli-\\nable metric we could identify to keep the paper accessible\\nand within reasonable length. In addition, we only partially\\nused statistical tests, as needed, and often compared metrics\\ns where they trained a linear transformation to\\nderive a target model for hardware changes [51].\\nMetric. We investigate whether f(c, et) = α × f(c, es) +\\nβ, ∀c ∈C. We use metric M1: Pearson linear correlation [4]\\nbetween f(c, es) and f(c, et) to evaluate the hypothesis. If\\nthe correlation is 1, we cnﬁgurations before measuring them, thus decrease cost.\\nMetric. We learn a classiﬁer using multinomial logistic regres-\\nsion [4]. It is a model that is used to predict the probabilities\\nof being invalid, given a set of conﬁguration options. We\\nmeasure M18: the correlation between the coefﬁcients (i.e\\nFor building the performance models, calculating impor-\\ntance of conﬁguration options, and classifying the invalid\\nconﬁgurations, we elected to use different machine learn-\\ning models: step-wise linear regression, regression trees, and\\nmultinomial logistic regression. We chose these learner mainly\\n\\nvalues, De = {yi}, e ∈E, using kernel density estimation [4]\\n(in the same way as histograms are constructed in statistics).\\n4) Inﬂuential Option: At the level of individual conﬁg-\\nuration options, we will be interested in exploring whether\\noptions have an inﬂuence on the performance of the system\\nitize regions in the conﬁgu-\\nration space based on the importance of the interactions.\\nMetric. We measure M14: the correlation between the coef-\\nﬁcients of the pairwise interaction terms in the linear model\\nlearned independently on the source and target environments\\nusing step-wise linear regression M2: Kullback-Leibler (KL) divergence; M3: Spearman correlation; M4/M5: Perc. of top/bottom conf.; M6/M7: Number of inﬂuential options;\\nM8/M9: Number of options agree/disagree; M10: Correlation btw importance of options; M11/M12: Number of interactions; M13: Number of interactions agree on effects;\\nMe ∈E\\non various conﬁgurations ci ∈C, and record the resulting\\nperformance values yi = f(ci, e) + ϵi where ϵi ∼N(0, σi) is\\nthe measurement noise corresponding to a normal distribution\\nwith zero mean and variance σ2\\ni . The training data for learning\\na performance model for system A in environment e i\\nmiss effects in parts of the conﬁguration space that we did\\nnot sample. We did not encounter any surprisingly different\\nobservation in our exhaustively measured SPEAR dataset.\\nWe operationalized a large number of different measures\\nthrough metrics. For each measure, we considered multiple\\nalternati is a relaxed hypothesis comparing to H1.3.\\nMetric. We measure M4/M5: the percentage of (10th per-\\ncentile) top/bottom conﬁgurations in the source that are also\\ntop/bottom performers in the target.\\nResults. The results in Table II show that top/bottom con-\\nﬁgurations are common across hardware and s\\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich metrics and ML models are used on this paper?'}\n",
      "Request data: {'model': 'qwen:1.8b', 'prompt': 'Context:\\nze the amount of measurement efforts.\\nSampling strategies based on experimental design (such as\\nPlackett-Burman) have been applied in the domain of con-\\nﬁgurable systems [15], [43], [44]. The aim of these sampling\\napproaches is to ensure that we gain a high level of information\\nfrom sparse sampling \\nvalues, De = {yi}, e ∈E, using kernel density estimation [4]\\n(in the same way as histograms are constructed in statistics).\\n4) Inﬂuential Option: At the level of individual conﬁg-\\nuration options, we will be interested in exploring whether\\noptions have an inﬂuence on the performance of the system\\nisampling. Our empirical study demonstrates the ex-\\nistence of diverse forms of transferable knowledge across\\nenvironments that can contribute to learning faster, better,\\nreliable, and more important, less costly performance models.\\nACKNOWLEDGMENT\\nThis work has been supported by AFRL and DARPA\\n(FA875imization and evolu-\\ntionary algorithms [16], [54] have also been used.\\nOur work is related to the performance analysis research\\nmentioned above. However, we do not perform a comparison\\nof different models, conﬁguration optimization or sampling\\nstrategies. Instead, we concentrate on transferring perve metrics (e.g., different ways to establish inﬂuential\\noptions), but settled usually on the simplest and most reli-\\nable metric we could identify to keep the paper accessible\\nand within reasonable length. In addition, we only partially\\nused statistical tests, as needed, and often compared metrics\\nexpect research opportunities regarding:\\n1) Sampling strategies to exploit the relatedness of environ-\\nments to select informative samples using the importance\\nof speciﬁc regions [40] or avoiding invalid conﬁgurations.\\n2) Learning mechanisms to exploit the relatedness across\\nenvironments and learn eategies can take existing domain knowledge into\\naccount and could beneﬁt from knowledge about inﬂuen-\\ntial options and interactions [44], [45], it is less obvious\\nhow to effectively incorporate such knowledge into sampling\\nstrategies and how to build more effective learners based on\\nlimited transfere ∈E\\non various conﬁgurations ci ∈C, and record the resulting\\nperformance values yi = f(ci, e) + ϵi where ϵi ∼N(0, σi) is\\nthe measurement noise corresponding to a normal distribution\\nwith zero mean and variance σ2\\ni . The training data for learning\\na performance model for system A in environment e it with the presented\\ndata and did not provide additional insights.\\n2) Internal and Construct Validity: Due to the size of\\nconﬁguration spaces, we could only measure conﬁgurations\\nexhaustively in one subject system and had to rely on sampling\\n(with substantial sampling size) for the others, which may\\nsubsequently relax the hypothesis to test whether and when\\nthe performance distributions are similar (H1.2), whether the\\nranking of conﬁgurations (H1.3), and the top/bottom conﬁgu-\\nrations (H1.4) stay consistent. Table II summarizes the results.\\nH1.1: The relation of the source response to the targ\\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich sample method is used on this paper?'}\n",
      "Request data: {'model': 'qwen:1.8b', 'prompt': 'Context:\\nve metrics (e.g., different ways to establish inﬂuential\\noptions), but settled usually on the simplest and most reli-\\nable metric we could identify to keep the paper accessible\\nand within reasonable length. In addition, we only partially\\nused statistical tests, as needed, and often compared metrics\\nnﬁgurations before measuring them, thus decrease cost.\\nMetric. We learn a classiﬁer using multinomial logistic regres-\\nsion [4]. It is a model that is used to predict the probabilities\\nof being invalid, given a set of conﬁguration options. We\\nmeasure M18: the correlation between the coefﬁcients (i.e exponential\\nin the number of options and it is computationally infeasible\\nto get measurements aiming at learning an exhaustive number\\nof interactions. Prior work has shown that a very large portion\\nof potential interactions has no inﬂuence [29], [45].\\nMetric. One key objective here is to evaluate ts where they trained a linear transformation to\\nderive a target model for hardware changes [51].\\nMetric. We investigate whether f(c, et) = α × f(c, es) +\\nβ, ∀c ∈C. We use metric M1: Pearson linear correlation [4]\\nbetween f(c, es) and f(c, et) to evaluate the hypothesis. If\\nthe correlation is 1, we c\\nvalues, De = {yi}, e ∈E, using kernel density estimation [4]\\n(in the same way as histograms are constructed in statistics).\\n4) Inﬂuential Option: At the level of individual conﬁg-\\nuration options, we will be interested in exploring whether\\noptions have an inﬂuence on the performance of the system\\nidirectly using more informal comparisons and some ad-hoc\\nthreshold for detecting common patterns across environments.\\nA different operationalization may lead to different results,\\nbut since our results are consistent across a large number of\\nmeasures, we do not expect any changes to the big picture.\\nmiss effects in parts of the conﬁguration space that we did\\nnot sample. We did not encounter any surprisingly different\\nobservation in our exhaustively measured SPEAR dataset.\\nWe operationalized a large number of different measures\\nthrough metrics. For each measure, we considered multiple\\nalternatitions across environments to avoid exploration of\\ninvalid regions and reduce measurement effort.\\nMetric. We measure M15/M16: percentage of invalid conﬁg-\\nurations in the source and target, M17: percentage of invalid\\nconﬁgurations, which are common between environments.\\nResults. The results in Table inear mapping\\nbetween the two responses, but, there might be a more\\nsophisticated relationship between the two environments that\\ncan be captured by a non-linear transfer function.\\nMetric. We measure M2: Kullback-Leibler (KL) divergence\\n[8] to compare the similarity between the performance dis-\\ntribu\\nbecause we already know that these are the key options\\ninﬂuencing performance.\\nMetric. In order to investigate the option-speciﬁc effects,\\nwe use a paired t-test [4] to test if an option leads to any\\nsigniﬁcant performance change and whether this change is\\nsimilar across environments. That is, when\\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich evaluation metric is used on this paper?'}\n",
      "Request data: {'model': 'qwen:1.8b', 'prompt': 'Context:\\nboth environments [51].\\nMore sophisticated transfer learning exists that reuses source\\ndata using learners such as Gaussian Processes (GP) [25]. the context of MapReduce applications [61],\\nperformance-anomaly detection [46], micro-benchmarking on\\ndifferent hardware [22], parameter dependencies [64], and\\nperformance prediction based on similarity search [48].\\nRecently, transfer learning is used in systems and soft-\\nware engineering. For exam\\nFor building the performance models, calculating impor-\\ntance of conﬁguration options, and classifying the invalid\\nconﬁgurations, we elected to use different machine learn-\\ning models: step-wise linear regression, regression trees, and\\nmultinomial logistic regression. We chose these learner mainly\\nsampling. Our empirical study demonstrates the ex-\\nistence of diverse forms of transferable knowledge across\\nenvironments that can contribute to learning faster, better,\\nreliable, and more important, less costly performance models.\\nACKNOWLEDGMENT\\nThis work has been supported by AFRL and DARPA\\n(FA875The aim of optimization\\napproaches is to ﬁnd the optimal conﬁguration in a highly\\ndimensional space using only a limited sampling budget.\\nMachine learning techniques, such as support-vector ma-\\nchines [61], decision trees [33], Fourier sparse functions [63],\\nactive learning [44] and search-based optnce models, showing broad possibilities of\\napplying transfer learning beyond the relatively small changes\\nexplored in existing work (e.g., small hardware changes [51],\\nlow ﬁdelity simulations [25], similar systems [7]).\\nOverall, our contributions are the following:\\n• We formulate a series of hypotheari-\\nables. Intuitively, the Spearman correlation will be high when\\nobservations have a similar rank. We consider rank correlations\\nhigher than 0.9 as strong and suitable for transfer learning.\\nResults. The results in Table II show that the rank correlations\\nare high across hardware changes and smalansfer learning in the context\\nof select hardware changes [7], [25], [51], we more broadly\\nempirically investigate why and when transfer learning works.\\nThat is, we provide evidence why and when other techniques\\nare applicable for which environmental changes.\\nTransfer learning has also been applied an linearly transform performance\\nmodels. Due to measurement noise, we do not expect perfect\\ncorrelation, but we expect, for correlations higher than 0.9,\\nsimple transfer learning can produce good predictions.\\nResults. The result in Table II show very high correlations\\nfor about a third of all studie ∈E\\non various conﬁgurations ci ∈C, and record the resulting\\nperformance values yi = f(ci, e) + ϵi where ϵi ∼N(0, σi) is\\nthe measurement noise corresponding to a normal distribution\\nwith zero mean and variance σ2\\ni . The training data for learning\\na performance model for system A in environment e i\\nEnd of context\\nAnswer the question with only Context information, anything besides that write MYTHINKING:\\nWhich learning algorithm is used on this paper?'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"results.csv\", \"w\") as f:\n",
    "    f.write(\"pdf,question,model,response,specialist_result\\n\")\n",
    "    for model in models:\n",
    "        for pdf in pdf_names:\n",
    "            for question in questions:\n",
    "                context = context_dict[pdf + question]\n",
    "                result = ask_llm(question, [context], model)\n",
    "                f.write(f\"{pdf},\\\"{question}\\\",\\\"{model}\\\",\\\"{result}\\\", \\\"{result_dict2[pdf]}\\\"\\n\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
