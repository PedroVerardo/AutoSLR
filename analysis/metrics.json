{
    "sampling method":{
        "question": "What sampling method is used in the paper?",
        "answer_prefix": "The sampling method used in the paper is:",
        "context": "sampling methods are used to select a representative subset from a dataset. They are often used to reduce the size of the dataset while maintaining its diversity and characteristics.",
        "some_possible_answers": "Randon sampling, Stratified sampling, Systematic sampling, Cluster sampling, Convenience sampling, Purposive sampling, Snowball sampling, Oversampling, Undersampling and etc... .",
        "text_example": "The preliminary analysis entails choosing an acceptable sample strategy because the dataset is severely imbalanced. Using some technique, such as changing the amount of the original data, sampling methods will convert an imbalanced dataset into a balanced distribution. The difficulty with imbalanced datasets is that most machine learning approaches will disregard the minority class, which is fraud, and hence perform poorly on it. Undersampling, Oversampling, and ROSE sampling approaches were tested for this objective. The performance of each sampling technique on the dataset is measured using ROC-AUC performance measure. The ROC-AUC gives you a number between 0 and 1, with one being the best and 0 being the worst. We chose the sampling approach with the maximum area under the curve (AUC) based on the study of the three sample methods since it will deliver the best performance. For our dataset, oversampling produced the greatest AUC.",
        "text_example_answare": "Undersampling, Oversampling, and ROSE",
        "output_format": "comma separated list" 
    },
    "dataset":{
        "question": "What dataset is used in the paper?",
        "answer_prefix": "The dataset used in the paper is:",
        "context": "Datasets are collections of data that are used for analysis, training machine learning models, or other purposes. They can vary widely in size, structure, and content.",
        "some_possible_answers": "Image datasets, Text datasets, Audio datasets, Video datasets, Tabular datasets and etc..., they could have different formats such as CSV, JSON, XML, or SQL databases.",
        "text_example": "This paper introduces the EMNIST dataset and then ap- plies two ELM-based neural networks to the classification tasks made possible with the complete NIST dataset. The classification systems demonstrate the structure and nature of the classification tasks involving both digits and letters, and additionally provides benchmark results for this new dataset. The purpose of the EMNIST dataset is to function as a drop- in replacement for the MNIST dataset in existing classification systems. As a result, the focus of the methodology is on reproducing the steps used to convert the NIST dataset into the MNIST dataset and apply them to the entire NIST SpecialDatabase 19. This conversion process and the modifications implemented to better convert the letters is described in Section II-B. The classifier used in this work are introduced and described in Section II-C and were applied to three different classifi- cation tasks based on combinations of letters, digits and the full classification task using the NIST dataset. In addition, a subset of the EMNIST digits were extracted and used to train a classifier. The results were then compared to an identical network applied to the MNIST digits in order to explore and validate the conversion process used in this work. The code used to extract the data from the original NIST dataset, the conversion code from 128 × 128 pixel binary images to the converted gray-scale and the code to perform the training and testing splits are provided as supplementary material to this paper.",
        "text_example_answare": "EMNIST, NIST, MNIST",
        "output_format": "comma separated list"
    },
    "technique":{
        "question": "What learning algorithms are used in the paper?",
        "answer_prefix": "The learning algorithms used in the paper are:",
        "context": "learning algorithms refer to the methods or approaches used in the Machine Learning field to solve specific problems or tasks. They are normally categorized in supervised learning (classification, regression), unsupervised learning (clustering, dimensionality reduction)",
        "some_possible_answers": "Decision Trees, Support Vector Machines, Neural Networks, k-Nearest Neighbors, Random Forests, Gradient Boosting Machines",
        "text_example": "Support Vector Machine (SVM) is a tool to find the hyper-plane that can be used for classification; it is based onkernel functions17,34,36. The Gaussian kernel is the mostversatile kernels17,37. By the width parameter σof theGaussian kernel function, one cancontrol the flexibility ofSVM classifier results. The Gaussian function can be usednot only as a kernel for SVM, butalso for some excitingneuro-fuzzy classifiers38.Decision trees are classifiers expressed as a recursivepart of the instance space.Classification and RegressionTrees (CART) model is a flexible method to describe howthe variable Y distributes after assigning the forecast vec-tor X of the metric. The CART model usesa binary tree todivide the forecast space into certain subsets on which Ydistribution is assumed continuously39,40.Artificial Neural Networks (ANNs)constitute a non-linear statistic model based on the function of thehuman brain41. ANNs provide powerful tools of datamining techniquesfor data analyst relationship model-ing.ANNs are able to recognize the complex patternsininput dataand alsothey can predict the outcome of thenew independent input data precisely42. ANNs have theremarkable ability to derive meaning from complicateddata or imprecise data. It can be used to extract patternsand detect trends using specific techniques43. ANNs arevery suitablefor identifying patterns, and they are alsovery well suited for prediction or forecasting data44.One ofthe most well-known ANNs is the Multi-layer Perceptron(MLP)45 named also as the Back-Propagation NeuralNetwork (BPN); its algorithm is based on the computa-tion of the errors of each output neuron after processingan input data35. It is a general technique called automaticdifferentiation. BPN is characterized by backward propa-gation of output errors, namely these errors are computedat the output layer and the training is distributed back tothe weights of the previous layers to reduce the outputerrors46.Survival Analysis method is a new technique of creditscoring model. A common way that banks can differenti-ate customer information when they apply for a loan fromthe bank. Banks canseparate the good information fromthe bad information regarding the loan application. Thesystem can calculate the profitability of customers andalso it can evaluate the profit scoring from the customers35.Worthwhile Survival analysis can predict the duration ofthe event will take place in advance and forecast the prob-ability of occurrence of an event to occur43.The H2O team discovered these famous data miningtechniquesto analyze the group datasets. These techniquesare Generalized Linear Models (GLM),Gradient BoostingMethod (GBM) and Distributed Random Forest (DRF).GLM are similar with the linear regression model. Datamining techniques are used for regression analysis andThitimanan Damrongsakmethee and Victor-Emil NeagoeIndian Journal of Science and Technology5Vol 10 (39) | October2017 | www.indjst.orgdata classification. GLM modelisvery popular because itis easy to be interpreted and it is also a high-speed pro-cessing stage when used for the large datasets28. GBMmodel is a tool for prediction using regression or classifi-cation. It is an ensemble of the tree models and providesconsiderably accurate results. GBM model applies weakclassification algorithms to incrementally change data, tocreate a series of decision trees28. Finally, the DRF is anensemble of tree models, where each tree is related fromother trees. DRF is the most powerful technique for clas-sification and regression. DRF can generate a big forestof classification or regression trees rather than a singleclassification or regression tree28. In addition, DRF buildshalf as many trees for binomial problems with a singletree to estimate class 0 by probability(p0) then computesthe probability of another class 1 as (p1). For multiclassproblems, DRF is used to estimate the probability of eachclass separately47.",
        "text_example_answare": "suport vector machine(SVM), Generalized Linear Modeling (GLM), Gradient Boosting Method (GBM), Distributed Random Forest(DRF), Classification and Regression Trees (CART), Artificial Neural Networks (ANNs), Multi-layer Perceptron (MLP), Back-Propagation Neural Network (BPN), Survival Analysis method ",
        "output_format": "comma separated list"
    },
    "partition method":{
        "question": "What partition method is used in the paper?",
        "answer_prefix": "The partition method used in the paper is:",
        "context": "partition methods are used to divide a dataset into subsets for training, validation, and testing. They help in evaluating the performance of machine learning models by ensuring that the model is tested on unseen data.",
        "some_possible_answers": "Holdout method, k-Fold Cross-Validation, Stratified k-Fold Cross-Validation, Leave-One-Out Cross-Validation (LOOCV), Time Series Split and etc... .",
        "text_example": "2.3.2. Random ForestRandom Forest is an ensemble learning method for both classification and regressiontasks. It constructs multiple decision trees during training, each using a subset of the dataand features. The final prediction is an average (regression) or majority vote (classification)of individual tree outputs. This technique reduces overfitting and enhances accuracy.2.3.3. XGBoostXGBoost, short for eXtreme Gradient Boosting, is an ensemble learning algorithmknown for its speed and performance. It builds a sequence of decision trees, each correctingthe errors of its predecessor. It employs a gradient boosting framework, optimizing auser-specified loss function. Regularization techniques and parallel processing contributeto its efficiency and effectiveness in various machine learning tasks. The detailed pseudocode of XGBoost is provided in the reference [21].2.4. Forced Partition MethodsInstead of treating the prediction of reb as a regression problem, this study categorizessamples into different clusters based on reb. Two forced partition methods were employed:the percentile method and the k-means method.Appl. Sci. 2024, 14, 326 6 of 132.4.1. Percentile MethodFor the percentile method, the desired number of clusters (f) was initially determined,and labels range from 0 to f − 1. Subsequently, samples were divided into f clusters basedon their reb percentiles. Each cluster contained either floor (total number of samples/f) orfloor (total number of samples/f) + 1 samples. This method ensured an even distributionof the sample count across each cluster.2.4.2. K-means MethodFor the k-means method, samples underwent k-means unsupervised learning, andthe means of energy, interval time, and reb of each initial cluster were calculated. Thesethree mean values were analyzed to ensure sufficient distinctiveness among the initialclusters. Following this, a forced partition method was introduced. This method establishedboundaries using the arithmetic mean of the reb means of two adjacent initial clusters. It isconceivable that initial clusters with closely aligned reb values might merge into a singleinitial cluster. Samples falling within the same boundary were subsequently categorizedinto the same final cluster.",
        "text_example_answare": "percentile method, k-means method",
        "output_format": "comma separated list"
    },
    "evaluation metric":{
        "question": "What evaluation metric is used in the paper?",
        "answer_prefix": "The evaluation metric used in the paper is:",
        "context": "evaluation metrics are used to assess the performance of machine learning models. They provide a quantitative measure of how well a model performs on a given task.",
        "some_possible_answers": "accuracy, precision, recall, F1-score, ROC-AUC, mean absolute error(MAE), and mean squared error(MSE). and etc... .",
        "text_example": "A. Evaluation IndicatorsWidely used indicators are Precision as in (1) and Recallas in (2).Precision=TP/(TP+FP) (1)Recall=TP(TP+FN) (2)where:TP: corresponds to the number of positive examplescorrectly predicted by classifier.FN: corresponds to the number of positive exampleswrongly predicted as negative by classifier.FP: corresponds to the number of negative exampleswrongly predicted as positive by classifier.TN: corresponds to the number of negative examplescorrectly predicted as negative by classifier.",
        "text_example_answare": "precision, recall",
        "output_format": "comma separated list"
    }
}