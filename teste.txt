Extracted Text: <--size:17.21500015258789-->Calculon: a Methodology and Tool for High-Level Codesign of<--size:17.21500015258789-->
<--size:17.21500015258789-->Systems and Large Language Models<--size:17.21500015258789-->
<--size:11.954999923706055-->Mikhail Isaev<--size:11.954999923706055-->
<--size:9.963000297546387-->michael.v.isaev@gatech.edu<--size:9.963000297546387-->
<--size:9.963000297546387-->Georgia Institute of Technology<--size:9.963000297546387-->
<--size:9.963000297546387-->Atlanta, Georgia, USA<--size:9.963000297546387-->
<--size:11.954999923706055-->Nic McDonald<--size:11.954999923706055-->
<--size:9.963000297546387-->nimcdonald@nvidia.com<--size:9.963000297546387-->
<--size:9.963000297546387-->NVIDIA<--size:9.963000297546387-->
<--size:9.963000297546387-->Salt Lake City, Utah, USA<--size:9.963000297546387-->
<--size:11.954999923706055-->Larry Dennison<--size:11.954999923706055-->
<--size:9.963000297546387-->ldennison@nvidia.com<--size:9.963000297546387-->
<--size:9.963000297546387-->NVIDIA<--size:9.963000297546387-->
<--size:9.963000297546387-->Westford, Massachusetts, USA<--size:9.963000297546387-->
<--size:11.954999923706055-->Richard Vuduc<--size:11.954999923706055-->
<--size:9.963000297546387-->richie@cc.gatech.edu<--size:9.963000297546387-->
<--size:9.963000297546387-->Georgia Institute of Technology<--size:9.963000297546387-->
<--size:9.963000297546387-->Atlanta, Georgia, USA<--size:9.963000297546387-->
<--size:10.909000396728516-->ABSTRACT<--size:10.909000396728516-->
<--size:8.8894624710083-->This paper presents a parameterized analytical performance model<--size:8.8894624710083-->
<--size:9.024090766906738-->of transformer-based Large Language Models (LLMs) for guiding<--size:9.024090766906738-->
<--size:8.939061164855957-->high-level algorithm-architecture codesign studies. This model de-<--size:8.939061164855957-->
<--size:8.979438781738281-->rives from an extensive survey of performance optimizations that<--size:8.979438781738281-->
<--size:9.055215835571289-->have been proposed for the training and inference of LLMs; the<--size:9.055215835571289-->
<--size:9.055215835571289-->model’s parameters capture application characteristics, the hard-<--size:9.055215835571289-->
<--size:8.875886917114258-->ware system, and the space of implementation strategies. With such<--size:8.875886917114258-->
<--size:9.028543472290039-->a model, we can systematically explore a joint space of hardware<--size:9.028543472290039-->
<--size:8.893982887268066-->and software configurations to identify optimal system designs un-<--size:8.893982887268066-->
<--size:8.893982887268066-->der given constraints, like the total amount of system memory. We<--size:8.893982887268066-->
<--size:8.875886917114258-->implemented this model and methodology in a Python-based open-<--size:8.875886917114258-->
<--size:9.055215835571289-->source tool called Calculon. Using it, we identified novel system<--size:9.055215835571289-->
<--size:8.880414009094238-->designs that look significantly different from current inference and<--size:8.880414009094238-->
<--size:8.875886917114258-->training systems, showing quantitatively the estimated potential to<--size:8.875886917114258-->
<--size:8.965999603271484-->achieve higher efficiency, lower cost, and better scalability.<--size:8.965999603271484-->
<--size:7.96999979019165-->ACM Reference Format:<--size:7.96999979019165-->
<--size:8.049304962158203-->Mikhail Isaev, Nic McDonald, Larry Dennison, and Richard Vuduc. 2023.<--size:8.049304962158203-->
<--size:8.049304962158203-->Calculon: a Methodology and Tool for High-Level Codesign of Systems<--size:8.049304962158203-->
<--size:8.017677307128906-->and Large Language Models. In The International Conference for High Per-<--size:8.017677307128906-->
<--size:8.049304962158203-->formance Computing, Networking, Storage and Analysis (SC ’23), Novem-<--size:8.049304962158203-->
<--size:8.049304962158203-->ber 12–17, 2023, Denver, CO, USA. ACM, New York, NY, USA, 13 pages.<--size:8.049304962158203-->
<--size:7.96999979019165-->https://doi.org/10.1145/3581784.3607102<--size:7.96999979019165-->
<--size:10.909000396728516-->1 INTRODUCTION<--size:10.909000396728516-->
<--size:8.875886917114258-->We consider the task of conducting high-level analyses for algorithm-<--size:8.875886917114258-->
<--size:8.875886917114258-->architecture codesign of distributed clusters and transformer-based<--size:8.875886917114258-->
<--size:8.875886917114258-->Large Language Models (LLMs) [ 3 , 4 , 36 , 39 , 40 ]. By “high level,” we<--size:8.875886917114258-->
<--size:9.055215835571289-->mean focusing on developing and using fast and coarse-grained<--size:9.055215835571289-->
<--size:8.939061164855957-->analytical or semi-empirical models of the software and hardware,<--size:8.939061164855957-->
<--size:9.055215835571289-->a stage of design that precedes detailed simulation or implemen-<--size:9.055215835571289-->
<--size:9.055215835571289-->tation on actual hardware. The goal is to estimate the best-case<--size:9.055215835571289-->
<--size:9.032994270324707-->relative improvements that might come from significant changes<--size:9.032994270324707-->
<--size:9.037443161010742-->to the system or software, as well as combinations of system and<--size:9.037443161010742-->
<--size:6.973999977111816-->SC ’23, November 12–17, 2023, Denver, CO, USA<--size:6.973999977111816-->
<--size:6.973999977111816-->© 2023 Copyright held by the owner/author(s).<--size:6.973999977111816-->
<--size:6.973999977111816-->ACM ISBN 979-8-4007-0109-2/23/11.<--size:6.973999977111816-->
<--size:6.973999977111816-->https://doi.org/10.1145/3581784.3607102<--size:6.973999977111816-->
<--size:8.93006420135498-->software configurations that might be unusual or otherwise costly<--size:8.93006420135498-->
<--size:8.907529830932617-->and difficult to implement. Since high level models are expected to<--size:8.907529830932617-->
<--size:9.055215835571289-->be much cheaper than detailed simulation, they should facilitate<--size:9.055215835571289-->
<--size:8.8894624710083-->rapid exploration of a potentially large parameter space during the<--size:8.8894624710083-->
<--size:8.965999603271484-->early phases of codesign.<--size:8.965999603271484-->
<--size:9.055215835571289-->In this work, our starting point is Megatron, a large family of<--size:9.055215835571289-->
<--size:9.055215835571289-->open-source LLM instances developed by NVIDIA [ 44 ]. The cost<--size:9.055215835571289-->
<--size:9.055215835571289-->of training such models is high: a version of Megatron having<--size:9.055215835571289-->
<--size:9.019635200500488-->one trillion (1T) parameters was recently trained over 84 days on<--size:9.019635200500488-->
<--size:9.024090766906738-->450 billion tokens using 3,072 NVIDIA A100 Graphics Processing<--size:9.024090766906738-->
<--size:9.055215835571289-->Unit ( GPU ) and executing more than 1,000 zetta FLOP ( 1 × 10 21<--size:9.055215835571289-->
<--size:8.961515426635742-->floating-point operations) [ 29 , 30 ]. This cost roughly equals seven<--size:8.961515426635742-->
<--size:9.055215835571289-->hundred years on a single GPU and over six million dollars (US)<--size:9.055215835571289-->
<--size:8.979438781738281-->assuming a single GPU at $1 per hour cloud- GPU rates. Incurring<--size:8.979438781738281-->
<--size:8.884939193725586-->such costs is commonplace; the PaLM-540B model recently trained<--size:8.884939193725586-->
<--size:9.055215835571289-->by Google used 2,572 zetta FLOP with similar numbers of Tensor<--size:9.055215835571289-->
<--size:9.055215835571289-->Processing Units (TPUs) and more than 8 million TPU hours [ 6 ].<--size:9.055215835571289-->
<--size:8.907529830932617-->Such high costs strongly motivate any combination of algorithmic,<--size:8.907529830932617-->
<--size:8.965999603271484-->software, or hardware redesign that can reduce them.<--size:8.965999603271484-->
<--size:8.988387107849121-->Consequently, there are proposed enhancements in algorithms<--size:8.988387107849121-->
<--size:8.875886917114258-->and software [ 29 , 37 , 38 ] and options for hardware acceleration [ 18 ,<--size:8.875886917114258-->
<--size:8.965999603271484-->31 ]. However, selecting a good configuration in practice has relied<--size:8.965999603271484-->
<--size:8.970481872558594-->primarily on heuristic reasoning [ 29 ]. While these proposals have<--size:8.970481872558594-->
<--size:8.875886917114258-->yielded impressive results, it has also been observed that distributed<--size:8.875886917114258-->
<--size:9.01071834564209-->training runs at a FLOP /s rate well below 50% of peak despite the<--size:9.01071834564209-->
<--size:8.965999603271484-->prevalence of matrix multiply (GEMM) operations [34].<--size:8.965999603271484-->
<--size:8.8894624710083-->The challenge is that the codesign landscape is quite large, mak-<--size:8.8894624710083-->
<--size:9.055215835571289-->ing it hard to reason about the impact of major changes to arbi-<--size:9.055215835571289-->
<--size:9.055215835571289-->trary combinations of hardware and software. For example, con-<--size:9.055215835571289-->
<--size:8.988387107849121-->sider that limited GPU memory capacity requires dividing a large<--size:8.988387107849121-->
<--size:8.875886917114258-->model among processors. Doing so can be achieved via model paral-<--size:8.875886917114258-->
<--size:8.884939193725586-->lelism, which combines two strategies known as tensor parallelism<--size:8.884939193725586-->
<--size:8.93006420135498-->and pipeline parallelism [ 29 ]. However, when using NVIDIA A100<--size:8.93006420135498-->
<--size:8.965999603271484-->GPU s, the size of the NVLink domain is 8, which can limit tensor<--size:8.965999603271484-->
<--size:8.8894624710083-->parallelism performance [ 32 ]. To compensate, one can increase the<--size:8.8894624710083-->
<--size:8.916550636291504-->degree of pipeline parallelism—but that may in turn produce other<--size:8.916550636291504-->
<--size:9.055215835571289-->inefficiencies such as reduced utilization due to pipeline bubbles<--size:9.055215835571289-->
<--size:8.875886917114258-->and needing to recompute intermediate features in light of memory<--size:8.875886917114258-->
<--size:8.875886917114258-->constraints [ 13 , 29 ]. Alternatively, one might improve the computer<--size:8.875886917114258-->
<--size:8.988387107849121-->network to support larger tensor parallelism domains; companies<--size:8.988387107849121-->
<--size:8.875886917114258-->and researchers have indeed considered doing so [ 17 , 32 ]. However,<--size:8.875886917114258-->
<--size:9.055215835571289-->if memory capacity is the root issue, then a more cost-effective<--size:9.055215835571289-->
<--size:7.96999979019165-->This work is licensed under a Creative Commons Attribution International<--size:7.96999979019165-->
<--size:7.96999979019165-->4.0 License.<--size:7.96999979019165-->
<--size:6.973999977111816-->SC ’23, November 12–17, 2023, Denver, CO, USA<--size:6.973999977111816-->
<--size:6.973999977111816-->Isaev, et al.<--size:6.973999977111816-->
<--size:4.873879909515381-->V i<--size:4.873879909515381-->
<--size:4.873879909515381-->V i<--size:4.873879909515381-->
<--size:4.873879909515381-->V i<--size:4.873879909515381-->
<--size:4.873879909515381-->V i<--size:4.873879909515381-->
<--size:4.873879909515381-->V i<--size:4.873879909515381-->
<--size:4.873879909515381-->V i<--size:4.873879909515381-->
<--size:6.49850606918335-->X Kj<--size:5.686192512512207-->
<--size:5.686192512512207-->Q j<--size:5.686192512512207-->
<--size:5.686192512512207-->V j<--size:5.686192512512207-->
<--size:5.686192512512207-->W V<--size:5.686192512512207-->
<--size:5.686192512512207-->W Q<--size:5.686192512512207-->
<--size:5.686192512512207-->W K<--size:5.686192512512207-->
<--size:4.873879909515381-->Q j x K j T<--size:4.873879909515381-->
<--size:6.49850606918335-->SoftMax<--size:6.49850606918335-->
<--size:6.49850606918335-->DropOut<--size:6.49850606918335-->
<--size:6.49850606918335-->Y j<--size:6.49850606918335-->
<--size:5.686192512512207-->W O<--size:5.686192512512207-->
<--size:4.873879909515381-->#heads<--size:4.873879909515381-->
<--size:5.686192512512207-->Layer Norm<--size:5.686192512512207-->
<--size:6.49850606918335-->Z<--size:6.49850606918335-->
<--size:5.686192512512207-->DropOut<--size:5.686192512512207-->
<--size:6.49850606918335-->X W B<--size:5.686192512512207-->
<--size:6.49850606918335-->Y W A<--size:5.686192512512207-->
<--size:6.49850606918335-->GeLU<--size:6.49850606918335-->
<--size:5.686192512512207-->Layer Norm<--size:5.686192512512207-->
<--size:6.49850606918335-->Z<--size:6.49850606918335-->
<--size:5.686192512512207-->DropOut<--size:5.686192512512207-->
<--bold--><--size:5.686192512512207-->Legend:<--size:5.686192512512207--><--bold-->
<--size:5.686192512512207-->element-wise layers<--size:5.686192512512207-->
<--size:5.686192512512207-->intermediate tensors<--size:5.686192512512207-->
<--size:5.686192512512207-->weights<--size:5.686192512512207-->
<--size:5.686192512512207-->– matrix multiplication<--size:5.686192512512207-->
<--size:5.686192512512207-->– element-wise addition<--size:5.686192512512207-->
<--size:8.965999603271484-->Figure 1: The transformer block structure of a typical LLM, such as GPT-3 or Megatron.<--size:8.965999603271484-->
<--size:9.055215835571289-->strategy is to increase capacity. Overall, this example shows that<--size:9.055215835571289-->
<--size:8.92556095123291-->codesign should carefully consider and delicately balance memory<--size:8.92556095123291-->
<--size:8.898500442504883-->capacity, memory bandwidth, processing throughput (i.e., FLOP/s ),<--size:8.898500442504883-->
<--size:8.979438781738281-->network bandwidth, and network scalability, all of which interact<--size:8.979438781738281-->
<--size:9.055215835571289-->with choices made in software. Therefore, to reason about these<--size:9.055215835571289-->
<--size:9.055215835571289-->trade-offs we seek a principled analysis framework that can be<--size:9.055215835571289-->
<--size:8.961515426635742-->extended or adapted in a hardware and software landscape under-<--size:8.961515426635742-->
<--size:8.965999603271484-->going rapid and continual evolution.<--size:8.965999603271484-->
<--size:8.875886917114258-->We propose one such model-driven approach for high-level code-<--size:8.875886917114258-->
<--size:9.055215835571289-->sign of LLM training and inference systems. We first identify a<--size:9.055215835571289-->
<--size:9.055215835571289-->parameterized space of possible configurations that span major<--size:9.055215835571289-->
<--size:8.94355583190918-->system features and common algorithmic and software implemen-<--size:8.94355583190918-->
<--size:9.028543472290039-->tation strategies (Section 2). We then develop a unified analytical<--size:9.028543472290039-->
<--size:8.957029342651367-->model to estimate the end-to-end performance of LLM training as<--size:8.957029342651367-->
<--size:8.875886917114258-->a function of the configuration parameters. This model allows us to<--size:8.875886917114258-->
<--size:8.884939193725586-->pose, mathematically, a constrained optimization problem: find the<--size:8.884939193725586-->
<--size:8.965999603271484-->configuration that yields the best performance given fixed system<--size:8.965999603271484-->
<--size:8.965999603271484-->constraints such as memory capacity and system or network size.<--size:8.965999603271484-->
<--size:9.055215835571289-->We encode this performance model and model-optimizer in a<--size:9.055215835571289-->
<--size:9.055215835571289-->tool called Calculon. The parameter space includes the structure<--size:9.055215835571289-->
<--size:9.055215835571289-->and number of weights in the LLM model, the implementation<--size:9.055215835571289-->
<--size:8.997325897216797-->strategy, and a schematic description of the hardware system (Ta-<--size:8.997325897216797-->
<--size:9.055215835571289-->ble 1). Since Calculon’s model is analytical, it can calculate and<--size:9.055215835571289-->
<--size:9.055215835571289-->return a complete breakdown of projected training or inference<--size:9.055215835571289-->
<--size:8.98391342163086-->time quickly, typically in much less than a 1 ms per configuration.<--size:8.98391342163086-->
<--size:9.055215835571289-->It thus becomes possible to search an entire configuration space<--size:9.055215835571289-->
<--size:8.952540397644043-->having many millions of combinations in only a few minutes on a<--size:8.952540397644043-->
<--size:8.965999603271484-->standard desktop computer.<--size:8.965999603271484-->
<--size:9.024090766906738-->This paper includes several analyses we have conducted using<--size:9.024090766906738-->
<--size:8.903016090393066-->Calculon. The modeling formulae themselves are complex to write<--size:8.903016090393066-->
<--size:9.055215835571289-->out in full; therefore, to save space, we focus on the analyses as<--size:9.055215835571289-->
<--size:8.92556095123291-->“proof-of-concept,” with detailed formulas appearing in Calculon’s<--size:8.92556095123291-->
<--size:9.024090766906738-->open-source repository, where our formulas and assumptions ap-<--size:9.024090766906738-->
<--size:8.875886917114258-->pear in full, allowing replication of our results or modification of our<--size:8.875886917114258-->
<--size:8.875886917114258-->assumptions. 1 The analyses showcase Calculon’s potential to facili-<--size:8.875886917114258-->
<--size:8.875886917114258-->tate high-level codesign, revealing several system and optimization<--size:8.875886917114258-->
<--size:8.965999603271484-->insights that may contradict conventional wisdom:<--size:8.965999603271484-->
<--size:8.965999603271484-->(1) None of the existing software-parallelism strategies is uniformly<--size:8.965999603271484-->
<--size:8.98391342163086-->the “best.” However, there is an optimal split-parallelism strat-<--size:8.98391342163086-->
<--size:8.875886917114258-->egy that balances system resources well, with the exact optimum<--size:8.875886917114258-->
<--size:8.965999603271484-->depending on system parameters, as we show.<--size:8.965999603271484-->
<--size:8.965999603271484-->(2) The speed of LLM training can be a sensitive function of system<--size:8.965999603271484-->
<--size:9.055215835571289-->size, with performance variability (ratio of highest to lowest<--size:9.055215835571289-->
<--size:9.055215835571289-->performer) exceeding 6 × . These “efficiency cliffs” come from<--size:9.055215835571289-->
<--size:5.480000019073486-->1 https://github.com/calculon-ai/calculon<--size:5.480000019073486-->
<--size:8.992857933044434-->difficulties mapping LLM structures to a given number of pro-<--size:8.992857933044434-->
<--size:8.965999603271484-->cessors when sizes do not “divide evenly.”<--size:8.965999603271484-->
<--size:8.965999603271484-->(3) Adding a second high-capacity tier of memory for tensor of-<--size:8.965999603271484-->
<--size:9.055215835571289-->floading reduces performance variability across various LLM<--size:9.055215835571289-->
<--size:9.055215835571289-->configurations and sizes, enabling efficient training of larger<--size:9.055215835571289-->
<--size:8.988387107849121-->models. Moreover, the bandwidth requirement for efficient of-<--size:8.988387107849121-->
<--size:8.965999603271484-->floading is within current technological capabilities.<--size:8.965999603271484-->
<--size:8.875886917114258-->While these findings are estimates, they suggest quantitatively what<--size:8.875886917114258-->
<--size:8.875886917114258-->performance improvements are possible , a critical first step for LLM<--size:8.875886917114258-->
<--size:9.055215835571289-->codesign. Our methodology is systematic and rigorous, enabling<--size:9.055215835571289-->
<--size:8.961515426635742-->future exploration via more detailed experiments and simulation.<--size:8.961515426635742-->
<--size:10.909000396728516-->2 ANALYTICAL MODEL<--size:10.909000396728516-->
<--size:8.903016090393066-->Calculon is a Python-based analytical performance model for LLM<--size:8.903016090393066-->
<--size:8.893982887268066-->training and inference on large-scale distributed systems. The core<--size:8.893982887268066-->
<--size:8.875886917114258-->analytical model performs a single calculation of time and resource<--size:8.875886917114258-->
<--size:9.055215835571289-->usage. This model is given 3 specifications: the LLM , the system<--size:9.055215835571289-->
<--size:8.93006420135498-->the LLM is running on, and the execution strategy describing how<--size:8.93006420135498-->
<--size:8.965999603271484-->the LLM is run on the system.<--size:8.965999603271484-->
<--size:10.909000396728516-->2.1<--size:10.909000396728516-->
<--size:10.909000396728516-->LLM Configuration<--size:10.909000396728516-->
<--size:8.8894624710083-->We adopt the framework of Megatron [ 44 ] for describing the struc-<--size:8.8894624710083-->
<--size:8.875886917114258-->ture of transformer-based [ 49 ] LLMs. Megatron can be used to reim-<--size:8.875886917114258-->
<--size:9.055215835571289-->plement models such as GPT2 [ 36 ] or GPT3 [ 3 ], Chinchilla [ 11 ],<--size:9.055215835571289-->
<--size:8.965999603271484-->LLaMa [48], and many other popular LLMs.<--size:8.965999603271484-->
<--size:8.94355583190918-->For training, Megatron uses synchronous mini-batch stochastic<--size:8.94355583190918-->
<--size:8.875886917114258-->gradient descent and the Adam optimizer for weight updates [ 2 , 19 ].<--size:8.875886917114258-->
<--size:8.988387107849121-->It may employ transformer-based encoder blocks, decoder blocks,<--size:8.988387107849121-->
<--size:8.875886917114258-->or both. Each transformer block has the same structure and consists<--size:8.875886917114258-->
<--size:8.875886917114258-->of a multi-head attention block followed by a multilayer perceptron<--size:8.875886917114258-->
<--size:8.934563636779785-->( MLP ) block, with normalization and residual connection between<--size:8.934563636779785-->
<--size:8.893982887268066-->them, as shown in Fig. 1 (adapted from Shoeybi et al. [ 44 ]). Several<--size:8.893982887268066-->
<--size:8.912040710449219-->hyperparameters define the blocks: the hidden size ( hidden ) is the<--size:8.912040710449219-->
<--size:9.055215835571289-->size of the embeddings and MLP layers, the number of attention<--size:9.055215835571289-->
<--size:8.875886917114258-->heads ( attn ) of certain size, the sequence size ( seq ) of the input text,<--size:8.875886917114258-->
<--size:9.055215835571289-->training batch size ( batch ), micro-batch size ( m ), and the number<--size:9.055215835571289-->
<--size:8.965999603271484-->of transformer blocks ( blocks ).<--size:8.965999603271484-->
<--size:10.909000396728516-->2.2<--size:10.909000396728516-->
<--size:10.909000396728516-->Hardware Configuration<--size:10.909000396728516-->
<--size:9.055215835571289-->Calculon models a processor-based distributed system in which<--size:9.055215835571289-->
<--size:9.055215835571289-->computation is assigned to either “matrix” execution or “vector”<--size:9.055215835571289-->
<--size:9.037443161010742-->execution. The performance of each type of computation may be<--size:9.037443161010742-->
<--size:9.055215835571289-->parameterized by input size in a specification file. This feature is<--size:9.055215835571289-->
<--size:8.875886917114258-->useful when, for instance, smaller general matrix multiply ( GEMM )<--size:8.875886917114258-->
<--size:8.965999603271484-->operations run at a lower fraction of peak than larger ones [33].<--size:8.965999603271484-->
<--size:6.973999977111816-->Calculon: a Methodology and Tool for High-Level Codesign of Systems and Large Language Models<--size:6.973999977111816-->
<--size:6.973999977111816-->SC ’23, November 12–17, 2023, Denver, CO, USA<--size:6.973999977111816-->
<--size:9.028543472290039-->Table 1: Optimizations related to LLM training grouped in families related to a system component or particular parallelism<--size:9.028543472290039-->
<--size:9.055215835571289-->they target. Families and optimizations within families are sorted by year. Arrow upward/downward direction represents<--size:9.055215835571289-->
<--size:9.01071834564209-->an increase/decrease in the metric. Color represents an increase (green) or decrease (red) in the performance. The presented<--size:9.01071834564209-->
<--size:9.01071834564209-->relative significance of each optimization’s effect is based on experimental evaluation of the search space. Range represents<--size:9.01071834564209-->
<--size:8.965999603271484-->space of Calculon’s input parameters.<--size:8.965999603271484-->
<--size:8.965999603271484-->Optimization<--size:8.965999603271484-->
<--size:8.965999603271484-->Year<--size:8.965999603271484-->
<--size:8.965999603271484-->Related<--size:8.965999603271484-->
<--size:8.965999603271484-->system<--size:8.965999603271484-->
<--size:8.965999603271484-->Comp<--size:8.965999603271484-->
<--size:8.965999603271484-->time<--size:8.965999603271484-->
<--size:8.965999603271484-->Comp<--size:8.965999603271484-->
<--size:8.965999603271484-->util<--size:8.965999603271484-->
<--size:8.965999603271484-->Mem<--size:8.965999603271484-->
<--size:8.965999603271484-->ti m e<--size:8.965999603271484-->
<--size:8.965999603271484-->Mem<--size:8.965999603271484-->
<--size:8.965999603271484-->cap<--size:8.965999603271484-->
<--size:8.965999603271484-->Mem<--size:8.965999603271484-->
<--size:8.965999603271484-->BW<--size:8.965999603271484-->
<--size:8.965999603271484-->Net<--size:8.965999603271484-->
<--size:8.965999603271484-->ti m e<--size:8.965999603271484-->
<--size:8.965999603271484-->Net<--size:8.965999603271484-->
<--size:8.965999603271484-->BW<--size:8.965999603271484-->
<--size:8.965999603271484-->Range<--size:8.965999603271484-->
<--size:8.965999603271484-->Data parallelism (DP) [55]<--size:8.965999603271484-->
<--size:8.965999603271484-->1989<--size:8.965999603271484-->
<--size:8.965999603271484-->network<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↑<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↑↑↑<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↑<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ 1 .. batch<--size:8.965999603271484-->
<--size:8.965999603271484-->DP overlap [23]<--size:8.965999603271484-->
<--size:8.965999603271484-->2017<--size:8.965999603271484-->
<--size:8.965999603271484-->network<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ ↓<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↓↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->– true/false<--size:8.965999603271484-->
<--size:8.965999603271484-->Optimizer sharding [22]<--size:8.965999603271484-->
<--size:8.965999603271484-->2019<--size:8.965999603271484-->
<--size:8.965999603271484-->network<--size:8.965999603271484-->
<--size:8.965999603271484-->↓ –<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->– tr u e/ f alse<--size:8.965999603271484-->
<--size:8.965999603271484-->Recompute [5, 10]<--size:8.965999603271484-->
<--size:8.965999603271484-->2000<--size:8.965999603271484-->
<--size:8.965999603271484-->compute<--size:8.965999603271484-->
<--size:8.965999603271484-->↑↑<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->↓↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->– full/attn/none<--size:8.965999603271484-->
<--size:8.965999603271484-->Fused layers [26]<--size:8.965999603271484-->
<--size:8.965999603271484-->2018<--size:8.965999603271484-->
<--size:8.965999603271484-->compute<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↑↑<--size:8.965999603271484-->
<--size:8.965999603271484-->↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->↓ –<--size:8.965999603271484-->
<--size:8.965999603271484-->– true/false<--size:8.965999603271484-->
<--size:8.965999603271484-->Microbatch training [13]<--size:8.965999603271484-->
<--size:8.965999603271484-->2019<--size:8.965999603271484-->
<--size:8.965999603271484-->compute<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↑↑<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↑↑↑<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->– 1 .. b a t c h /DP<--size:8.965999603271484-->
<--size:8.965999603271484-->Pipeline parallelism (PP) [7, 13]<--size:8.965999603271484-->
<--size:8.965999603271484-->2012<--size:8.965999603271484-->
<--size:8.965999603271484-->network<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ ↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↑<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ 1 .. blocks<--size:8.965999603271484-->
<--size:8.965999603271484-->PP 1F1B schedule [7, 28]<--size:8.965999603271484-->
<--size:8.965999603271484-->2012<--size:8.965999603271484-->
<--size:8.965999603271484-->network<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->– true/false<--size:8.965999603271484-->
<--size:8.965999603271484-->PP interleaving [29]<--size:8.965999603271484-->
<--size:8.965999603271484-->2021<--size:8.965999603271484-->
<--size:8.965999603271484-->network<--size:8.965999603271484-->
<--size:8.965999603271484-->↓ ↑↑<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↑<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↑<--size:8.965999603271484-->
<--size:8.965999603271484-->↑↑<--size:8.965999603271484-->
<--size:8.965999603271484-->1 .. blocks /PP<--size:8.965999603271484-->
<--size:8.965999603271484-->PP RS + AG [20]<--size:8.965999603271484-->
<--size:8.965999603271484-->2022<--size:8.965999603271484-->
<--size:8.965999603271484-->network<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↓<--size:8.965999603271484-->
<--size:8.965999603271484-->↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->tr u e/ f al se<--size:8.965999603271484-->
<--size:8.965999603271484-->Tensor parallelism (TP) [7, 21, 43]<--size:8.965999603271484-->
<--size:8.965999603271484-->2012<--size:8.965999603271484-->
<--size:8.965999603271484-->network<--size:8.965999603271484-->
<--size:8.965999603271484-->↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->↓ –<--size:8.965999603271484-->
<--size:8.965999603271484-->↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->↑↑↑<--size:8.965999603271484-->
<--size:8.965999603271484-->↑↑↑<--size:8.965999603271484-->
<--size:8.965999603271484-->1 .. attn<--size:8.965999603271484-->
<--size:8.965999603271484-->TP RS + AG instead AR [29]<--size:8.965999603271484-->
<--size:8.965999603271484-->2021<--size:8.965999603271484-->
<--size:8.965999603271484-->network<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ ↑<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↓<--size:8.965999603271484-->
<--size:8.965999603271484-->↓ true/false<--size:8.965999603271484-->
<--size:8.965999603271484-->Sequence parallelism (SP) [20]<--size:8.965999603271484-->
<--size:8.965999603271484-->2022<--size:8.965999603271484-->
<--size:8.965999603271484-->network<--size:8.965999603271484-->
<--size:8.965999603271484-->↓ –<--size:8.965999603271484-->
<--size:8.965999603271484-->↓ ↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->↓ ↑<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ true/false<--size:8.965999603271484-->
<--size:8.965999603271484-->TP redo for SP [20]<--size:8.965999603271484-->
<--size:8.965999603271484-->2022<--size:8.965999603271484-->
<--size:8.965999603271484-->network<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↓<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↑<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ true/false<--size:8.965999603271484-->
<--size:8.965999603271484-->TP overlap [52]<--size:8.965999603271484-->
<--size:8.965999603271484-->2022<--size:8.965999603271484-->
<--size:8.965999603271484-->network<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ ↓<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->– ↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->– none/ p ipe/ring<--size:8.965999603271484-->
<--size:8.965999603271484-->Weight offload [42]<--size:8.965999603271484-->
<--size:8.965999603271484-->2021<--size:8.965999603271484-->
<--size:8.965999603271484-->memory<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ ↓↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ –<--size:8.965999603271484-->
<--size:8.965999603271484-->– true/false<--size:8.965999603271484-->
<--size:8.965999603271484-->Activation offload [42]<--size:8.965999603271484-->
<--size:8.965999603271484-->2021<--size:8.965999603271484-->
<--size:8.965999603271484-->memory<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ ↓↓↓<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ –<--size:8.965999603271484-->
<--size:8.965999603271484-->– true/false<--size:8.965999603271484-->
<--size:8.965999603271484-->Optimizer of fl oad [42]<--size:8.965999603271484-->
<--size:8.965999603271484-->2021<--size:8.965999603271484-->
<--size:8.965999603271484-->memory<--size:8.965999603271484-->
<--size:8.965999603271484-->– –<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ ↓<--size:8.965999603271484-->
<--size:8.965999603271484-->↑ –<--size:8.965999603271484-->
<--size:8.965999603271484-->– tr u e/ f alse<--size:8.965999603271484-->
<--size:8.974961280822754-->The memory system of the processor is modeled as a two-level<--size:8.974961280822754-->
<--size:9.055215835571289-->hierarchy wherein the first level is used for direct computation<--size:9.055215835571289-->
<--size:9.055215835571289-->and the second level is used for stashing bulk data until a later<--size:9.055215835571289-->
<--size:8.875886917114258-->time when it is needed (i.e., offloading). Both memory systems have<--size:8.875886917114258-->
<--size:8.965999603271484-->programmable capacities, bandwidths, and size-based efficiencies.<--size:8.965999603271484-->
<--size:8.875886917114258-->Each computational operation (e.g., GEMM) is fed to a processing<--size:8.875886917114258-->
<--size:8.92556095123291-->model that determines how long it will take. This model considers<--size:8.92556095123291-->
<--size:8.875886917114258-->both the time spent in raw compute (i.e., FLOP s) and the time spent<--size:8.875886917114258-->
<--size:8.965999603271484-->in raw memory accesses [33].<--size:8.965999603271484-->
<--size:9.055215835571289-->Each processor is able to connect to an arbitrary number of<--size:9.055215835571289-->
<--size:9.055215835571289-->networks. Each network is programmed with a size, bandwidth,<--size:9.055215835571289-->
<--size:9.024090766906738-->latency, and efficiency. A network also has a specification of how<--size:9.024090766906738-->
<--size:9.055215835571289-->it handles each specific operation, which is also the mechanism<--size:9.055215835571289-->
<--size:8.875886917114258-->that models the performance benefits of in-network collectives [ 32 ].<--size:8.875886917114258-->
<--size:9.055215835571289-->Each network also has a value of how much processing power is<--size:9.055215835571289-->
<--size:8.875886917114258-->taken from the processor while the network is operating at full band-<--size:8.875886917114258-->
<--size:8.875886917114258-->width. This value is explicitly used when modeling the performance<--size:8.875886917114258-->
<--size:8.965999603271484-->degradation of overlapping communication with computation.<--size:8.965999603271484-->
<--size:10.909000396728516-->2.3<--size:10.909000396728516-->
<--size:10.909000396728516-->Execution Configuration<--size:10.909000396728516-->
<--size:9.037443161010742-->Many performance optimization techniques and implementation<--size:9.037443161010742-->
<--size:8.992857933044434-->strategies have been proposed for transformer-based model train-<--size:8.992857933044434-->
<--size:8.957029342651367-->ing. We surveyed these methods and present them in Table 1. The<--size:8.957029342651367-->
<--size:9.055215835571289-->large compute requirements of LLMs dictate distributed training<--size:9.055215835571289-->
<--size:9.055215835571289-->using many processors or accelerators and using various modes<--size:9.055215835571289-->
<--size:9.037443161010742-->of parallelism. Megatron employs three parallelization strategies:<--size:9.037443161010742-->
<--size:9.055215835571289-->data parallelism ( DP ), pipeline parallelism ( PP ), and tensor par-<--size:9.055215835571289-->
<--size:9.055215835571289-->allelism ( TP ). These strategies can lead to a complex execution<--size:9.055215835571289-->
<--size:8.939061164855957-->schedule (Fig. 2). In this schedule, execution of transformer blocks<--size:8.939061164855957-->
<--size:8.965999603271484-->is intertwined with communication related to TP, PP, and DP.<--size:8.965999603271484-->
<--size:8.875886917114258-->We implement TP as presented in Megatron [ 44 ]. While other TP<--size:8.875886917114258-->
<--size:8.875886917114258-->partitioning schemes are possible, the selected one tries to minimize<--size:8.875886917114258-->
<--size:8.94355583190918-->the number of communication instructions per single transformer<--size:8.94355583190918-->
<--size:9.055215835571289-->block, which in the case of small TP partition sizes also reduces<--size:9.055215835571289-->
<--size:9.019635200500488-->the amount of communication traffic. We implement PP using an<--size:9.019635200500488-->
<--size:9.055215835571289-->interleaved schedule presented in [ 29 ], as shown in Fig. 2. DP is<--size:9.055215835571289-->
<--size:8.992857933044434-->implemented with an optional overlap. In this case, DP communi-<--size:8.992857933044434-->
<--size:8.934563636779785-->cation for each layer is scheduled as soon as the last microbatch is<--size:8.934563636779785-->
<--size:8.965999603271484-->propagated through that layer, as shown in Fig. 2(b).<--size:8.965999603271484-->
<--size:8.875886917114258-->Calculon implements all of optimizations from the Table 1. Each<--size:8.875886917114258-->
<--size:9.055215835571289-->optimization is parameterized with its acceptable input-range in<--size:9.055215835571289-->
<--size:8.875886917114258-->the column “range” . The space of techniques that Calculon captures<--size:8.875886917114258-->
<--size:8.997325897216797-->grows combinatorially, a major challenge for the implementation.<--size:8.997325897216797-->
<--size:8.974961280822754-->While individual techniques can be described with formulae, they<--size:8.974961280822754-->
<--size:8.875886917114258-->must be combined carefully to ensure their interactions are captured<--size:8.875886917114258-->
<--size:8.965999603271484-->and feasibility constraints are accounted for, as Calculon does.<--size:8.965999603271484-->
<--size:8.875886917114258-->We based the Calculon performance model on many transformer-<--size:8.875886917114258-->
<--size:8.875886917114258-->specific optimizations described in the open literature. The starting<--size:8.875886917114258-->
<--size:8.875886917114258-->point is Megatron [ 44 ], which combines many optimization families<--size:8.875886917114258-->
<--size:9.037443161010742-->including DP [ 55 ], activation recompute [ 5 , 10 ], and TP [ 44 ]. The<--size:9.037443161010742-->
<--size:9.055215835571289-->Megatron team later added many PP -related features[ 13 , 28 , 29 ],<--size:9.055215835571289-->
<--size:9.055215835571289-->including micro-batching, 1F1B and interleaved scheduling, and<--size:9.055215835571289-->
<--size:8.965999603271484-->TP -related optimizations [ 20 , 29 ], such as splitting all-reduce com-<--size:8.965999603271484-->
<--size:8.875886917114258-->munication into reduce-scatter and all-gather to optimize PP traffic<--size:8.875886917114258-->
<--size:9.015177726745605-->and adding sequence parallelism. Calculon also implements most<--size:9.015177726745605-->
<--size:9.055215835571289-->optimizations from DeepSpeed [ 41 ], including optimizer shard-<--size:9.055215835571289-->
<--size:9.055215835571289-->ing [ 22 , 37 ], and tensor offloading [ 38 , 42 ]. Several optimizations<--size:9.055215835571289-->
<--size:6.973999977111816-->SC ’23, November 12–17, 2023, Denver, CO, USA<--size:6.973999977111816-->
<--size:6.973999977111816-->Isaev, et al.<--size:6.973999977111816-->
<--bold--><--size:4.4996747970581055-->L1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L4<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L7<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L8<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L7<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L8<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L7<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L8<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L9<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L10<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L4<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L4<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L9<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L10<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L9<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L10<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L11<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L12<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L12<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L11<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L11<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L12<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L12<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L11<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L11<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L12<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L10<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L9<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L10<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L8<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L7<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m1<--size:4.4996747970581055--><--bold-->
<--size:6.74951171875-->1F1B schedule<--size:6.74951171875-->
<--size:6.74951171875-->interleaving factor 2<--size:6.74951171875-->
<--size:5.999566078186035-->Forward<--size:5.999566078186035-->
<--size:6.74951171875-->block<--size:6.74951171875-->
<--size:6.74951171875-->Backward<--size:6.74951171875-->
<--size:6.74951171875-->block<--size:6.74951171875-->
<--size:6.74951171875-->PP comm TP comm<--size:6.74951171875-->
<--size:6.74951171875-->Pipeline bubble<--size:6.74951171875-->
<--bold--><--size:4.4996747970581055-->L12 DP<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L10 DP<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L4 DP<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L6 DP<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L11<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L12<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L12<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L11<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m4<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m4<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L10<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L9<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L4<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m4<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m4<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L4<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L4<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L3<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L10<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L9<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L8<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L7<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m4<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m4<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L2<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L1<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L8<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L7<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L7<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m4<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L1 DP<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L2 DP<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L3 DP<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L5 DP<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L7 DP<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L8 DP<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L11 DP<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L9 DP<--size:4.4996747970581055--><--bold-->
<--size:6.74951171875-->Chunk of consecutive blocks<--size:6.74951171875-->
<--size:6.74951171875-->DP overlap section for a single chunk<--size:6.74951171875-->
<--size:6.74951171875-->last chunk DP overlap<--size:6.74951171875-->
<--size:6.74951171875-->DP comm<--size:6.74951171875-->
<--bold--><--size:4.4996747970581055-->L11<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m5<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L9<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L10<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m6<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->L8<--size:4.4996747970581055--><--bold-->
<--bold--><--size:4.4996747970581055-->m4<--size:4.4996747970581055--><--bold-->
<--size:8.249403953552246-->(a) Pipeline schedule, prologue<--size:8.249403953552246-->
<--size:8.249403953552246-->(b) Pipeline schedule, epilogue<--size:8.249403953552246-->
<--size:9.055215835571289-->Figure 2: A pipeline schedule of LLM training with an in-<--size:9.055215835571289-->
<--size:9.046334266662598-->terleaved schedule [ 29 ]. Every transformer block (Fig. 1) is<--size:9.046334266662598-->
<--size:8.875886917114258-->identified by its block number starting with 𝐿 and microbatch<--size:8.875886917114258-->
<--size:8.875886917114258-->id starting with 𝑚 . Several consecutive blocks in each proces-<--size:8.875886917114258-->
<--size:9.055215835571289-->sor are grouped into chunks. The backward pass for every<--size:9.055215835571289-->
<--size:9.037443161010742-->block-microbatch combination is scheduled right after the<--size:9.037443161010742-->
<--size:8.875886917114258-->data for it becomes available. A pipeline schedule consists of<--size:8.875886917114258-->
<--size:8.961515426635742-->a prologue phase shown in (a) and epilogue phase shown in<--size:8.961515426635742-->
<--size:8.875886917114258-->(b), with the in-between steady phase omitted for brevity. The<--size:8.875886917114258-->
<--size:8.875886917114258-->epilogue demonstrates data parallelism ( DP ) communication<--size:8.875886917114258-->
<--size:8.884939193725586-->overlapped with backward pass of other transformer blocks.<--size:8.884939193725586-->
<--size:8.875886917114258-->developed for inference are included as well [ 1 , 35 ]. Finally, we also<--size:8.875886917114258-->
<--size:8.875886917114258-->included activation fusion for some layers [ 26 ] and overlapping DP<--size:8.875886917114258-->
<--size:8.965999603271484-->and TP communication with compute [23, 52].<--size:8.965999603271484-->
<--size:9.055215835571289-->We do not consider many techniques that make LLM train-<--size:9.055215835571289-->
<--size:9.055215835571289-->ing available on smaller systems with costs of significant slow-<--size:9.055215835571289-->
<--size:8.875886917114258-->downs [ 38 , 46 ], nor those that target arbitrary networks with worse<--size:8.875886917114258-->
<--size:9.055215835571289-->performance for regular transformer structures [ 12 , 15 , 16 , 51 ].<--size:9.055215835571289-->
<--size:9.019635200500488-->We also do not consider implementation strategies that may non-<--size:9.019635200500488-->
<--size:8.898500442504883-->trivially affect model fidelity, including compression and asynchro-<--size:8.898500442504883-->
<--size:8.965999603271484-->nous training techniques.<--size:8.965999603271484-->
<--size:9.055215835571289-->Many techniques implemented in Calculon are de facto stan-<--size:9.055215835571289-->
<--size:9.055215835571289-->dards for LLMs, such as optimizer sharding [ 22 ] and microbatch<--size:9.055215835571289-->
<--size:9.055215835571289-->training [ 13 ]. Other techniques are scattered among many code<--size:9.055215835571289-->
<--size:9.055215835571289-->bases, such as interleaved schedule for PP only implemented in<--size:9.055215835571289-->
<--size:9.015177726745605-->Megatron [ 29 ]. A strength of Calculon is estimating performance<--size:9.015177726745605-->
<--size:8.965999603271484-->for combinations of techniques that have not yet been attempted.<--size:8.965999603271484-->
<--size:10.909000396728516-->2.4<--size:10.909000396728516-->
<--size:10.909000396728516-->Performance Calculation<--size:10.909000396728516-->
<--size:8.921056747436523-->LLMs are implemented in a series of replicated transformer blocks<--size:8.921056747436523-->
<--size:9.055215835571289-->(Fig. 1). Taking advantage of the regular structure, Calculon an-<--size:9.055215835571289-->
<--size:9.055215835571289-->alyzes the minimum number of unique block structures to make<--size:9.055215835571289-->
<--size:9.028543472290039-->its performance prediction, reusing the results when appropriate.<--size:9.028543472290039-->
<--size:8.98391342163086-->In contrast, many non- LLM specific performance tools needlessly<--size:8.98391342163086-->
<--size:9.055215835571289-->replicate the performance prediction of all blocks independently.<--size:9.055215835571289-->
<--size:8.875886917114258-->This modeling-time optimization allows Calculon to complete a full<--size:8.875886917114258-->
<--size:8.965999603271484-->analysis in under a millisecond.<--size:8.965999603271484-->
<--size:8.92556095123291-->Calculon distinguishes edge blocks with point-to-point commu-<--size:8.92556095123291-->
<--size:8.875886917114258-->nication for PP and separates the effects of each technique between<--size:8.875886917114258-->
<--size:9.055215835571289-->layers in the transformer block and across the execution stages,<--size:9.055215835571289-->
<--size:8.875886917114258-->such as forward and backward pass, optimizer step, communication<--size:8.875886917114258-->
<--size:8.921056747436523-->phases, etc. Doing so permits distinguishing the effects of interact-<--size:8.921056747436523-->
<--size:8.875886917114258-->ing model components and a faithful expression of how techniques<--size:8.875886917114258-->
<--size:8.884939193725586-->interact. For example, Calculon forbids DP communication overlap<--size:8.884939193725586-->
<--size:9.055215835571289-->during the optimizer step if optimizer sharding is enabled, and it<--size:9.055215835571289-->
<--size:8.875886917114258-->throttles offloading when high bandwidth memory ( HBM ) memory<--size:8.875886917114258-->
<--size:8.957029342651367-->is in active use while allowing it during compute-only or network<--size:8.957029342651367-->
<--size:8.965999603271484-->communication phases.<--size:8.965999603271484-->
<--size:9.037443161010742-->Given a description of the LLM , the system, and the execution<--size:9.037443161010742-->
<--size:9.055215835571289-->strategy, Calculon performs a complete performance estimation<--size:9.055215835571289-->
<--size:9.055215835571289-->using its analytical model and outputs the statistics which con-<--size:9.055215835571289-->
<--size:8.893982887268066-->tain relevant information about total performance (e.g., batch time,<--size:8.893982887268066-->
<--size:9.055215835571289-->sample rate, etc.) as well as a breakdown of where the time was<--size:9.055215835571289-->
<--size:9.055215835571289-->spent and how much of the available resources were used. The<--size:9.055215835571289-->
<--size:9.024090766906738-->time breakdown includes forward pass, backward pass, recompu-<--size:9.024090766906738-->
<--size:9.037443161010742-->tation (if used), optimizer, and more. For network time, it reports<--size:9.037443161010742-->
<--size:9.055215835571289-->the amount of time each type of parallelism was communicating<--size:9.055215835571289-->
<--size:8.92556095123291-->on the network and reports how much time was exposed blocking<--size:8.92556095123291-->
<--size:8.875886917114258-->computation. When offloading is used, Calculon reports the amount<--size:8.875886917114258-->
<--size:8.979438781738281-->of total time performing offloading over the second level memory<--size:8.979438781738281-->
<--size:8.875886917114258-->system and also the amount of time exposed (if any) in this process.<--size:8.875886917114258-->
<--size:9.055215835571289-->For memory systems, Calculon reports how much memory was<--size:9.055215835571289-->
<--size:9.055215835571289-->used and which types of data used them (e.g., weights, optimizer<--size:9.055215835571289-->
<--size:9.055215835571289-->state, activations, etc.). Finally, Calculon reports efficiency of the<--size:9.055215835571289-->
<--size:8.965999603271484-->LLM execution.<--size:8.965999603271484-->
<--size:8.921056747436523-->Fig. 3 shows an example output of running GPT3 175B on 4,096<--size:8.921056747436523-->
<--size:8.98391342163086-->NVIDIA A100 GPUs each with 80 GiB of memory connected over<--size:8.98391342163086-->
<--size:8.903016090393066-->NVLink clusters of 8 and InfiniBand HDR networks between them.<--size:8.903016090393066-->
<--size:9.055215835571289-->The execution specifies TP=8, PP=64, and DP=8. The total batch<--size:9.055215835571289-->
<--size:9.050776481628418-->time was 16.7 seconds and 20% of that was spent in recomputing<--size:9.050776481628418-->
<--size:8.875886917114258-->the activations during the backward pass. Of the available 80 GiB of<--size:8.875886917114258-->
<--size:8.970481872558594-->HBM memory, 17.4 GiB was used and 29% of the utilized memory<--size:8.970481872558594-->
<--size:8.965999603271484-->was used for optimizer state.<--size:8.965999603271484-->
<--size:6.497200012207031-->0<--size:6.497200012207031-->
<--size:6.497200012207031-->5<--size:6.497200012207031-->
<--size:6.497200012207031-->10<--size:6.497200012207031-->
<--size:6.497200012207031-->15<--size:6.497200012207031-->
<--size:6.497200012207031-->20<--size:6.497200012207031-->
<--size:6.497200012207031-->Time, s<--size:6.497200012207031-->
<--size:7.796640396118164-->Batch time<--size:7.796640396118164-->
<--size:6.497200012207031-->FW pass<--size:6.497200012207031-->
<--size:6.497200012207031-->BW pass<--size:6.497200012207031-->
<--size:6.497200012207031-->Optim step<--size:6.497200012207031-->
<--size:6.497200012207031-->PP bubble<--size:6.497200012207031-->
<--size:6.497200012207031-->FW recompute<--size:6.497200012207031-->
<--size:6.497200012207031-->TP comm<--size:6.497200012207031-->
<--size:6.497200012207031-->PP comm<--size:6.497200012207031-->
<--size:6.497200012207031-->DP comm<--size:6.497200012207031-->
<--size:6.497200012207031-->0<--size:6.497200012207031-->
<--size:6.497200012207031-->5<--size:6.497200012207031-->
<--size:6.497200012207031-->10<--size:6.497200012207031-->
<--size:6.497200012207031-->15<--size:6.497200012207031-->
<--size:6.497200012207031-->20<--size:6.497200012207031-->
<--size:6.497200012207031-->Size, GB<--size:6.497200012207031-->
<--size:7.796640396118164-->HBM consumption<--size:7.796640396118164-->
<--size:6.497200012207031-->Weight<--size:6.497200012207031-->
<--size:6.497200012207031-->Activation<--size:6.497200012207031-->
<--size:6.497200012207031-->Weight gradients<--size:6.497200012207031-->
<--size:6.497200012207031-->Activation gradients<--size:6.497200012207031-->
<--size:6.497200012207031-->Optimizer space<--size:6.497200012207031-->
<--size:8.903016090393066-->Figure 3: Time and memory consumption for running GPT3<--size:8.903016090393066-->
<--size:8.965999603271484-->175B across 4,096 GPUs using TP=8, PP=64, DP=8.<--size:8.965999603271484-->
<--size:6.973999977111816-->Calculon: a Methodology and Tool for High-Level Codesign of Systems and Large Language Models<--size:6.973999977111816-->
<--size:6.973999977111816-->SC ’23, November 12–17, 2023, Denver, CO, USA<--size:6.973999977111816-->
<--size:10.909000396728516-->2.5<--size:10.909000396728516-->
<--size:10.909000396728516-->Validation<--size:10.909000396728516-->
<--size:8.875886917114258-->Table 2: Calculon’s validation comparing performance predic-<--size:8.875886917114258-->
<--size:8.94355583190918-->tion to performance measured on the A100-based Selene su-<--size:8.94355583190918-->
<--size:8.875886917114258-->percomputer for full activation recomputation and sequence<--size:8.875886917114258-->
<--size:8.970481872558594-->parallelism plus selective activation recomputation. Perfor-<--size:8.970481872558594-->
<--size:8.965999603271484-->mance is batch time measured in seconds.<--size:8.965999603271484-->
<--size:8.965999603271484-->22B<--size:8.965999603271484-->
<--size:8.965999603271484-->175B<--size:8.965999603271484-->
<--size:8.965999603271484-->530B<--size:8.965999603271484-->
<--size:8.965999603271484-->1T<--size:8.965999603271484-->
<--size:8.965999603271484-->Full<--size:8.965999603271484-->
<--size:8.965999603271484-->Selene<--size:8.965999603271484-->
<--size:8.965999603271484-->1.42<--size:8.965999603271484-->
<--size:8.965999603271484-->18.13<--size:8.965999603271484-->
<--size:8.965999603271484-->49.05<--size:8.965999603271484-->
<--size:8.965999603271484-->94.42<--size:8.965999603271484-->
<--size:8.965999603271484-->Calculon<--size:8.965999603271484-->
<--size:8.965999603271484-->1.40<--size:8.965999603271484-->
<--size:8.965999603271484-->18.03<--size:8.965999603271484-->
<--size:8.965999603271484-->49.89<--size:8.965999603271484-->
<--size:8.965999603271484-->90.08<--size:8.965999603271484-->
<--size:8.965999603271484-->Delta<--size:8.965999603271484-->
<--size:8.965999603271484-->1.72%<--size:8.965999603271484-->
<--size:8.965999603271484-->0.56%<--size:8.965999603271484-->
<--size:8.965999603271484-->-1.72%<--size:8.965999603271484-->
<--size:8.965999603271484-->4.60%<--size:8.965999603271484-->
<--size:8.965999603271484-->Seq+Sel<--size:8.965999603271484-->
<--size:8.965999603271484-->Selene<--size:8.965999603271484-->
<--size:8.965999603271484-->1.10<--size:8.965999603271484-->
<--size:8.965999603271484-->13.75<--size:8.965999603271484-->
<--size:8.965999603271484-->37.83<--size:8.965999603271484-->
<--size:8.965999603271484-->71.49<--size:8.965999603271484-->
<--size:8.965999603271484-->Calculon<--size:8.965999603271484-->
<--size:8.965999603271484-->1.14<--size:8.965999603271484-->
<--size:8.965999603271484-->13.64<--size:8.965999603271484-->
<--size:8.965999603271484-->34.47<--size:8.965999603271484-->
<--size:8.965999603271484-->66.04<--size:8.965999603271484-->
<--size:8.965999603271484-->Delta<--size:8.965999603271484-->
<--size:8.965999603271484-->-3.33%<--size:8.965999603271484-->
<--size:8.965999603271484-->0.81%<--size:8.965999603271484-->
<--size:8.965999603271484-->8.87%<--size:8.965999603271484-->
<--size:8.965999603271484-->7.62%<--size:8.965999603271484-->
<--size:9.055215835571289-->We validated Calculon’s modeling accuracy against measured<--size:9.055215835571289-->
<--size:8.970481872558594-->performance of various LLMs on NVIDIA’s A100-based Selene su-<--size:8.970481872558594-->
<--size:8.94355583190918-->percomputer [ 20 ]. We compare the results for modeling Megatron<--size:8.94355583190918-->
<--size:9.046334266662598-->applications sized 22B, 175B, 530B, and 1T, while performing full<--size:9.046334266662598-->
<--size:9.006256103515625-->activation recomputation as well as when applying sequence par-<--size:9.006256103515625-->
<--size:9.055215835571289-->allelism with attention-only activation recomputation. The com-<--size:9.055215835571289-->
<--size:8.992857933044434-->parison appears in Table 2. For these 8 runs on Selene, Calculon’s<--size:8.992857933044434-->
<--size:8.965999603271484-->prediction averages 3.65% error, and the maximum error is 8.87%.<--size:8.965999603271484-->
<--size:10.909000396728516-->3 RELATED WORK<--size:10.909000396728516-->
<--size:9.037443161010742-->For codesign, one expects event-driven hardware modeling to be<--size:9.037443161010742-->
<--size:9.055215835571289-->more accurate than analytical modeling but also orders of mag-<--size:9.055215835571289-->
<--size:9.032994270324707-->nitude slower. NVArchSim [ 50 ], a state-of-the-art GPU simulator,<--size:9.032994270324707-->
<--size:8.903016090393066-->can take a day to model 1 second of a single GPU ’s execution time.<--size:8.903016090393066-->
<--size:8.992857933044434-->Network simulators such as SuperSim takes on the order of a day<--size:8.992857933044434-->
<--size:9.055215835571289-->to model 1 second of network communication between 128 end-<--size:9.055215835571289-->
<--size:9.050776481628418-->points [ 14 , 27 ]. SST, a state-of-the-art parallel network simulator,<--size:9.050776481628418-->
<--size:9.055215835571289-->shows a similar speed with single-core modeling [ 53 ] and, there-<--size:9.055215835571289-->
<--size:8.903016090393066-->fore, requires significant time to simulate the training of Megatron<--size:8.903016090393066-->
<--size:9.055215835571289-->on thousands of accelerators. On the application modeling side,<--size:9.055215835571289-->
<--size:8.875886917114258-->compiler-based models like ParaGraph can extract both application<--size:8.875886917114258-->
<--size:8.921056747436523-->models and optimizations from the compiled representation of the<--size:8.921056747436523-->
<--size:9.032994270324707-->applications [ 14 ]. However, it might not be possible to deduce all<--size:9.032994270324707-->
<--size:9.055215835571289-->of the performance optimizations and implementation strategies<--size:9.055215835571289-->
<--size:8.957029342651367-->developed for Megatron, and introducing them explicitly is a very<--size:8.957029342651367-->
<--size:8.903016090393066-->time-consuming process. Furthermore, compiler-based approaches<--size:8.903016090393066-->
<--size:8.965999603271484-->generally work well only for existing hardware.<--size:8.965999603271484-->
<--size:8.875886917114258-->On the execution side, several projects consider automated model<--size:8.875886917114258-->
<--size:9.037443161010742-->parallelization and partitioning. FlexFlow splits a single iteration<--size:9.037443161010742-->
<--size:9.055215835571289-->along samples, operators, attributes, and parameters dimensions<--size:9.055215835571289-->
<--size:9.055215835571289-->but does not consider pipelining [ 25 ]. DAPPLE, PipeDream, and<--size:9.055215835571289-->
<--size:9.055215835571289-->Tarnawski et al. focus on pipeline parallelism scheduling [ 8 , 28 ,<--size:9.055215835571289-->
<--size:8.965999603271484-->47 ]. These efforts do not consider TP or parallelization strategies<--size:8.965999603271484-->
<--size:8.875886917114258-->separate from a broader space of execution optimization techniques.<--size:8.875886917114258-->
<--size:8.988387107849121-->GShard [ 24 ] and GSPMD [ 54 ] provide a way to implement par-<--size:8.988387107849121-->
<--size:8.948049545288086-->allel transformer or automatically parallelize transformer via XLA<--size:8.948049545288086-->
<--size:9.015177726745605-->compiler [ 9 ] after code annotation. XLA compiler supports many<--size:9.015177726745605-->
<--size:8.875886917114258-->other optimizations as well. GSPMD focuses on finding the optimal<--size:8.875886917114258-->
<--size:8.875886917114258-->configuration for existing systems. Alpa [ 56 ] builds on top of these<--size:8.875886917114258-->
<--size:8.893982887268066-->projects adding an automated search for optimal combined TP and<--size:8.893982887268066-->
<--size:8.965999603271484-->PP model split. Unlike Calculon, they do not consider a unified space<--size:8.965999603271484-->
<--size:8.98391342163086-->of hardware and software configurations and do not target future<--size:8.98391342163086-->
<--size:8.875886917114258-->system design, both being key features of Calculon. Also, Calculon,<--size:8.875886917114258-->
<--size:9.041889190673828-->being an analytical performance model with fast prediction time,<--size:9.041889190673828-->
<--size:9.055215835571289-->can explore vast configuration spaces. Other solutions, typically<--size:9.055215835571289-->
<--size:8.898500442504883-->working as part of compiler infrastructure or runtime, either cover<--size:8.898500442504883-->
<--size:9.055215835571289-->a smaller space or use heuristics to limit the search, focusing on<--size:9.055215835571289-->
<--size:8.965999603271484-->running training with good performance on the available system.<--size:8.965999603271484-->
<--size:8.93006420135498-->Finally, NaaS proposes a joint codesign of optimized neural net-<--size:8.93006420135498-->
<--size:9.055215835571289-->work ( NN ) and accelerators to achieve the best performance per<--size:9.055215835571289-->
<--size:8.875886917114258-->unit power [ 57 ]. Calculon focuses on optimizing large-scale system<--size:8.875886917114258-->
<--size:8.965999603271484-->design and system-level optimizations rather than NN layers.<--size:8.965999603271484-->
<--size:10.909000396728516-->4 PERFORMANCE TRADE-OFFS<--size:10.909000396728516-->
<--size:8.875886917114258-->We used Calculon to explore the landscape of system configurations<--size:8.875886917114258-->
<--size:8.875886917114258-->defined by our analytical modeling (Section 2). The two studies that<--size:8.875886917114258-->
<--size:8.997325897216797-->follow study implementation trade-offs given a fixed system (Sec-<--size:8.997325897216797-->
<--size:8.875886917114258-->tion 4.1) and how LLM structure, software, and hardware factors in-<--size:8.875886917114258-->
<--size:8.8894624710083-->teract when exploring the full space of configurations (Section 4.2).<--size:8.8894624710083-->
<--size:10.909000396728516-->4.1<--size:10.909000396728516-->
<--size:10.909000396728516-->Parallelization Analysis<--size:10.909000396728516-->
<--size:9.055215835571289-->We consider the training of Megatron-1T LLM [ 29 ] on a baseline<--size:9.055215835571289-->
<--size:9.055215835571289-->system configured with 4,096 NVIDIA A100 GPUs with a global<--size:9.055215835571289-->
<--size:8.880414009094238-->batch equal to 4,096 . We try all combinations of TP , PP , and DP to<--size:8.880414009094238-->
<--size:8.965999603271484-->determine the resource allocation with the best performance.<--size:8.965999603271484-->
<--size:9.001791954040527-->We denote a splitting by ( 𝑡, 𝑝,𝑑 ) where 𝑡 × 𝑝 × 𝑑 = 4 , 096 GPUs<--size:9.001791954040527-->
<--size:8.875886917114258-->and each of 𝑡 , 𝑝 , and 𝑑 factors measure the degree of TP , PP , and DP<--size:8.875886917114258-->
<--size:8.875886917114258-->exploitation, respectively. Smaller values of 𝑡 , 𝑝 , or 𝑑 imply “less” of<--size:8.875886917114258-->
<--size:8.875886917114258-->that type of parallelism, with 1 being the minimum value of each. In<--size:8.875886917114258-->
<--size:8.875886917114258-->this case study, we assume a software implementation that employs,<--size:8.875886917114258-->
<--size:9.055215835571289-->as a memory-saving strategy, both optimizer sharding and 1F1B<--size:9.055215835571289-->
<--size:9.055215835571289-->scheduling (Table 1). We then try all possible settings of ( 𝑡, 𝑝,𝑑 )<--size:9.055215835571289-->
<--size:9.055215835571289-->where we set the NVLink domain size to 𝑡 ≤ 32; we specifically<--size:9.055215835571289-->
<--size:9.055215835571289-->set the NVLink size to the number of GPUs in the TP domain<--size:9.055215835571289-->
<--size:9.055215835571289-->to shed light on the effects (implicit costs) of TP . The impact of<--size:9.055215835571289-->
<--size:8.898500442504883-->changing ( 𝑡, 𝑝,𝑑 ) on execution time and memory consumption are<--size:8.898500442504883-->
<--size:8.875886917114258-->summarized in Fig. 4, and from it, we can make several observations.<--size:8.875886917114258-->
<--size:9.001791954040527-->First, over-emphasizing any one mode of parallelism leads to a<--size:9.001791954040527-->
<--size:8.875886917114258-->stark performance drop, as every parallelization strategy has a dom-<--size:8.875886917114258-->
<--size:8.875886917114258-->inating cost component. The top row of Fig. 4 shows how increasing<--size:8.875886917114258-->
<--size:9.015177726745605-->any of 𝑡 , 𝑝 , or 𝑑 to their extreme values (e.g., when 𝑑 = 32, letting<--size:9.015177726745605-->
<--size:8.965999603271484-->𝑡 = 1 and 𝑝 = 128 or 𝑡 = 32 and 𝑝 = 4), execution time is relatively<--size:8.965999603271484-->
<--size:9.055215835571289-->high. For TP , the culprit is the visible communication costs (“TP<--size:9.055215835571289-->
<--size:8.875886917114258-->comm”) that increase as 𝑡 increases while compute utilization drops<--size:8.875886917114258-->
<--size:9.055215835571289-->due to thinning operands in the local matrix multiplications. For<--size:9.055215835571289-->
<--size:8.965999603271484-->PP , the cost of pipeline bubbles—in particular, idle time between<--size:8.965999603271484-->
<--size:9.055215835571289-->forward and backward passes— increases as 𝑝 increases. For DP ,<--size:9.055215835571289-->
<--size:8.965999603271484-->communication costs (“DP comm”) increase as 𝑑 increases.<--size:8.965999603271484-->
<--size:9.037443161010742-->Second, each parallelism strategy affects memory usage differ-<--size:9.037443161010742-->
<--size:9.055215835571289-->ently, as shown in the bottom row of Fig. 4. While TP cuts both<--size:9.055215835571289-->
<--size:8.875886917114258-->weight and activation memory costs, PP reduces only weights. Due<--size:8.875886917114258-->
<--size:8.875886917114258-->to how interleaved pipeline scheduling is implemented, we need to<--size:8.875886917114258-->
<--size:8.961515426635742-->keep an even larger activation space as we would have to with no<--size:8.961515426635742-->
<--size:8.965999603271484-->PP. DP cannot reduce activation or weight storage.<--size:8.965999603271484-->
<--size:6.973999977111816-->SC ’23, November 12–17, 2023, Denver, CO, USA<--size:6.973999977111816-->
<--size:6.973999977111816-->Isaev, et al.<--size:6.973999977111816-->
<--size:5.56820011138916-->t=1<--size:5.56820011138916-->
<--size:5.56820011138916-->p=128<--size:5.56820011138916-->
<--size:5.56820011138916-->t=2<--size:5.56820011138916-->
<--size:5.56820011138916-->p=64<--size:5.56820011138916-->
<--size:5.56820011138916-->t=4<--size:5.56820011138916-->
<--size:5.56820011138916-->p=32<--size:5.56820011138916-->
<--size:5.56820011138916-->t=8<--size:5.56820011138916-->
<--size:5.56820011138916-->p=16<--size:5.56820011138916-->
<--size:5.56820011138916-->t=16<--size:5.56820011138916-->
<--size:5.56820011138916-->p=8<--size:5.56820011138916-->
<--size:5.56820011138916-->t=32<--size:5.56820011138916-->
<--size:5.56820011138916-->p=4<--size:5.56820011138916-->
<--size:5.56820011138916-->0<--size:5.56820011138916-->
<--size:5.56820011138916-->20<--size:5.56820011138916-->
<--size:5.56820011138916-->40<--size:5.56820011138916-->
<--size:5.56820011138916-->60<--size:5.56820011138916-->
<--size:5.56820011138916-->80<--size:5.56820011138916-->
<--size:5.56820011138916-->100<--size:5.56820011138916-->
<--size:6.074399948120117-->Time, s<--size:6.074399948120117-->
<--size:6.074399948120117-->TP vs PP (DP=32) batch time<--size:6.074399948120117-->
<--size:5.56820011138916-->p=1<--size:5.56820011138916-->
<--size:5.56820011138916-->d=512<--size:5.56820011138916-->
<--size:5.56820011138916-->p=2<--size:5.56820011138916-->
<--size:5.56820011138916-->d=256<--size:5.56820011138916-->
<--size:5.56820011138916-->p=4<--size:5.56820011138916-->
<--size:5.56820011138916-->d=128<--size:5.56820011138916-->
<--size:5.56820011138916-->p=8<--size:5.56820011138916-->
<--size:5.56820011138916-->d=64<--size:5.56820011138916-->
<--size:5.56820011138916-->p=16<--size:5.56820011138916-->
<--size:5.56820011138916-->d=32<--size:5.56820011138916-->
<--size:5.56820011138916-->p=32<--size:5.56820011138916-->
<--size:5.56820011138916-->d=16<--size:5.56820011138916-->
<--size:5.56820011138916-->p=64<--size:5.56820011138916-->
<--size:5.56820011138916-->d=8<--size:5.56820011138916-->
<--size:5.56820011138916-->p=128<--size:5.56820011138916-->
<--size:5.56820011138916-->d=4<--size:5.56820011138916-->
<--size:5.56820011138916-->0<--size:5.56820011138916-->
<--size:5.56820011138916-->20<--size:5.56820011138916-->
<--size:5.56820011138916-->40<--size:5.56820011138916-->
<--size:5.56820011138916-->60<--size:5.56820011138916-->
<--size:5.56820011138916-->80<--size:5.56820011138916-->
<--size:5.56820011138916-->100<--size:5.56820011138916-->
<--size:6.074399948120117-->Time, s<--size:6.074399948120117-->
<--size:6.074399948120117-->PP vs DP (TP=8) batch time<--size:6.074399948120117-->
<--size:6.074399948120117-->FW pass<--size:6.074399948120117-->
<--size:6.074399948120117-->BW pass<--size:6.074399948120117-->
<--size:6.074399948120117-->Optim step<--size:6.074399948120117-->
<--size:6.074399948120117-->PP bubble<--size:6.074399948120117-->
<--size:6.074399948120117-->FW recompute<--size:6.074399948120117-->
<--size:6.074399948120117-->TP comm<--size:6.074399948120117-->
<--size:6.074399948120117-->PP comm<--size:6.074399948120117-->
<--size:6.074399948120117-->DP comm<--size:6.074399948120117-->
<--size:5.56820011138916-->t=1<--size:5.56820011138916-->
<--size:5.56820011138916-->d=128<--size:5.56820011138916-->
<--size:5.56820011138916-->t=2<--size:5.56820011138916-->
<--size:5.56820011138916-->d=64<--size:5.56820011138916-->
<--size:5.56820011138916-->t=4<--size:5.56820011138916-->
<--size:5.56820011138916-->d=32<--size:5.56820011138916-->
<--size:5.56820011138916-->t=8<--size:5.56820011138916-->
<--size:5.56820011138916-->d=16<--size:5.56820011138916-->
<--size:5.56820011138916-->t=16<--size:5.56820011138916-->
<--size:5.56820011138916-->d=8<--size:5.56820011138916-->
<--size:5.56820011138916-->t=32<--size:5.56820011138916-->
<--size:5.56820011138916-->d=4<--size:5.56820011138916-->
<--size:5.56820011138916-->0<--size:5.56820011138916-->
<--size:5.56820011138916-->20<--size:5.56820011138916-->
<--size:5.56820011138916-->40<--size:5.56820011138916-->
<--size:5.56820011138916-->60<--size:5.56820011138916-->
<--size:5.56820011138916-->80<--size:5.56820011138916-->
<--size:5.56820011138916-->100<--size:5.56820011138916-->
<--size:6.074399948120117-->Time, s<--size:6.074399948120117-->
<--size:6.074399948120117-->TP vs DP (PP=32) batch time<--size:6.074399948120117-->
<--size:5.56820011138916-->t=1<--size:5.56820011138916-->
<--size:5.56820011138916-->p=128<--size:5.56820011138916-->
<--size:5.56820011138916-->t=2<--size:5.56820011138916-->
<--size:5.56820011138916-->p=64<--size:5.56820011138916-->
<--size:5.56820011138916-->t=4<--size:5.56820011138916-->
<--size:5.56820011138916-->p=32<--size:5.56820011138916-->
<--size:5.56820011138916-->t=8<--size:5.56820011138916-->
<--size:5.56820011138916-->p=16<--size:5.56820011138916-->
<--size:5.56820011138916-->t=16<--size:5.56820011138916-->
<--size:5.56820011138916-->p=8<--size:5.56820011138916-->
<--size:5.56820011138916-->t=32<--size:5.56820011138916-->
<--size:5.56820011138916-->p=4<--size:5.56820011138916-->
<--size:5.56820011138916-->0<--size:5.56820011138916-->
<--size:5.56820011138916-->50<--size:5.56820011138916-->
<--size:5.56820011138916-->100<--size:5.56820011138916-->
<--size:5.56820011138916-->150<--size:5.56820011138916-->
<--size:5.56820011138916-->200<--size:5.56820011138916-->
<--size:5.56820011138916-->250<--size:5.56820011138916-->
<--size:5.56820011138916-->300<--size:5.56820011138916-->
<--size:6.074399948120117-->Size, GB<--size:6.074399948120117-->
<--size:6.074399948120117-->TP vs PP (DP=32) memory consumption<--size:6.074399948120117-->
<--size:5.56820011138916-->p=1<--size:5.56820011138916-->
<--size:5.56820011138916-->d=512<--size:5.56820011138916-->
<--size:5.56820011138916-->p=2<--size:5.56820011138916-->
<--size:5.56820011138916-->d=256<--size:5.56820011138916-->
<--size:5.56820011138916-->p=4<--size:5.56820011138916-->
<--size:5.56820011138916-->d=128<--size:5.56820011138916-->
<--size:5.56820011138916-->p=8<--size:5.56820011138916-->
<--size:5.56820011138916-->d=64<--size:5.56820011138916-->
<--size:5.56820011138916-->p=16<--size:5.56820011138916-->
<--size:5.56820011138916-->d=32<--size:5.56820011138916-->
<--size:5.56820011138916-->p=32<--size:5.56820011138916-->
<--size:5.56820011138916-->d=16<--size:5.56820011138916-->
<--size:5.56820011138916-->p=64<--size:5.56820011138916-->
<--size:5.56820011138916-->d=8<--size:5.56820011138916-->
<--size:5.56820011138916-->p=128<--size:5.56820011138916-->
<--size:5.56820011138916-->d=4<--size:5.56820011138916-->
<--size:5.56820011138916-->0<--size:5.56820011138916-->
<--size:5.56820011138916-->50<--size:5.56820011138916-->
<--size:5.56820011138916-->100<--size:5.56820011138916-->
<--size:5.56820011138916-->150<--size:5.56820011138916-->
<--size:5.56820011138916-->200<--size:5.56820011138916-->
<--size:5.56820011138916-->250<--size:5.56820011138916-->
<--size:5.56820011138916-->300<--size:5.56820011138916-->
<--size:6.074399948120117-->Size, GB<--size:6.074399948120117-->
<--size:6.074399948120117-->PP vs DP (TP=8) memory consumption<--size:6.074399948120117-->
<--size:6.074399948120117-->Weight<--size:6.074399948120117-->
<--size:6.074399948120117-->Activation<--size:6.074399948120117-->
<--size:6.074399948120117-->Weight<--size:6.074399948120117-->
<--size:6.074399948120117-->gradients<--size:6.074399948120117-->
<--size:6.074399948120117-->Activation<--size:6.074399948120117-->
<--size:6.074399948120117-->gradients<--size:6.074399948120117-->
<--size:6.074399948120117-->Optimizer space<--size:6.074399948120117-->
<--size:5.56820011138916-->t=1<--size:5.56820011138916-->
<--size:5.56820011138916-->d=128<--size:5.56820011138916-->
<--size:5.56820011138916-->t=2<--size:5.56820011138916-->
<--size:5.56820011138916-->d=64<--size:5.56820011138916-->
<--size:5.56820011138916-->t=4<--size:5.56820011138916-->
<--size:5.56820011138916-->d=32<--size:5.56820011138916-->
<--size:5.56820011138916-->t=8<--size:5.56820011138916-->
<--size:5.56820011138916-->d=16<--size:5.56820011138916-->
<--size:5.56820011138916-->t=16<--size:5.56820011138916-->
<--size:5.56820011138916-->d=8<--size:5.56820011138916-->
<--size:5.56820011138916-->t=32<--size:5.56820011138916-->
<--size:5.56820011138916-->d=4<--size:5.56820011138916-->
<--size:5.56820011138916-->0<--size:5.56820011138916-->
<--size:5.56820011138916-->50<--size:5.56820011138916-->
<--size:5.56820011138916-->100<--size:5.56820011138916-->
<--size:5.56820011138916-->150<--size:5.56820011138916-->
<--size:5.56820011138916-->200<--size:5.56820011138916-->
<--size:5.56820011138916-->250<--size:5.56820011138916-->
<--size:5.56820011138916-->300<--size:5.56820011138916-->
<--size:6.074399948120117-->Size, GB<--size:6.074399948120117-->
<--size:6.074399948120117-->TP vs DP (PP=32) memory consumption<--size:6.074399948120117-->
<--size:9.111599922180176-->Megatron-1T single batch training on 4096 A100 GPUs with various parallelism strategies<--size:9.111599922180176-->
<--size:8.875886917114258-->Figure 4: Parallelization strategies analysis, 𝑡 indicates TP factor, 𝑝 - PP , and 𝑑 - DP . We can see that high TP and DP suffer from<--size:8.875886917114258-->
<--size:9.006256103515625-->communication overheads, while high PP suffers from scheduling bubble. On the memory side, TP helps reduce weight and<--size:9.006256103515625-->
<--size:9.001791954040527-->activation pressure, while PP helps only with weight space reduction. High DP helps balance optimizer space by sharding it,<--size:9.001791954040527-->
<--size:8.965999603271484-->achieving the same efficiency as PP and TP.<--size:8.965999603271484-->
<--size:8.875886917114258-->Third, the dependence of overall performance on ( 𝑡, 𝑝,𝑑 ) appears<--size:8.875886917114258-->
<--size:8.875886917114258-->to behave like a convex function, yielding an optimal parallelization-<--size:8.875886917114258-->
<--size:8.875886917114258-->split in execution time (where valleys occur in execution time) with<--size:8.875886917114258-->
<--size:9.055215835571289-->some minimum constraint on memory usage (i.e., since overall<--size:9.055215835571289-->
<--size:8.875886917114258-->memory usage decreases nearly monotonically as 𝑡 and 𝑝 increase).<--size:8.875886917114258-->
<--size:8.898500442504883-->In short, a good parallelism split reduces visible communication<--size:8.898500442504883-->
<--size:9.055215835571289-->time, improves compute utilization by reducing PP bubbles and<--size:9.055215835571289-->
<--size:8.875886917114258-->increasing local matrix multiply sizes, and implies a minimum total<--size:8.875886917114258-->
<--size:8.965999603271484-->required memory. Calculon illuminates these quantitatively.<--size:8.965999603271484-->
<--size:10.909000396728516-->4.2<--size:10.909000396728516-->
<--size:10.909000396728516-->Optimizations Analysis<--size:10.909000396728516-->
<--size:8.884939193725586-->While an optimal parallelization strategy exists per Section 4.1, the<--size:8.884939193725586-->
<--size:8.979438781738281-->best configuration depends on the structure of the LLM , the avail-<--size:8.979438781738281-->
<--size:9.024090766906738-->able system resources, and what other software implementations<--size:9.024090766906738-->
<--size:8.965999603271484-->are selected. We illustrate these variations in Fig. 5.<--size:8.965999603271484-->
<--size:8.875886917114258-->First, Fig. 5(a) shows the variation in execution time and memory<--size:8.875886917114258-->
<--size:8.957029342651367-->usage for training a baseline implementation of Megatron-1T [ 29 ]<--size:8.957029342651367-->
<--size:9.055215835571289-->assuming a per- GPU HBM memory capacity of 80 GiB . Dashes<--size:9.055215835571289-->
<--size:9.055215835571289-->indicate infeasible configurations due to memory capacity limits.<--size:9.055215835571289-->
<--size:8.884939193725586-->Among the feasible configurations, the minimum execution time is<--size:8.884939193725586-->
<--size:8.965999603271484-->attained for ( 𝑡, 𝑝 ) = ( 8 , 32 ) with 79 GiB, just under capacity.<--size:8.965999603271484-->
<--size:8.98391342163086-->Next, suppose we enable additional software or hardware tech-<--size:8.98391342163086-->
<--size:9.055215835571289-->niques. For instance, Fig. 5(b) considers the addition of sequence<--size:9.055215835571289-->
<--size:9.055215835571289-->parallelism (Table 1) and its associated performance-enhancing<--size:9.055215835571289-->
<--size:8.875886917114258-->techniques [ 20 ]. The optimal configuration shifts slightly to ( 𝑡, 𝑝 ) =<--size:8.875886917114258-->
<--size:8.965999603271484-->( 16 , 64 ) while also lowering the memory requirement to 72 GiB . En-<--size:8.965999603271484-->
<--size:8.875886917114258-->abling all compatible techniques from Table 1, as shown in Fig. 5(c),<--size:8.875886917114258-->
<--size:9.055215835571289-->yields a variety of configurations that could be chosen to mini-<--size:9.055215835571289-->
<--size:9.055215835571289-->mize either time ( ( 𝑡, 𝑝 ) = ( 16 , 4 ) ) or memory capacity ( 40 GiB at<--size:9.055215835571289-->
<--size:8.965999603271484-->( 𝑡, 𝑝 ) = ( 8 , 32 ) ), as desired. Additional optimizations, including op-<--size:8.965999603271484-->
<--size:9.001791954040527-->timizer sharding and TP and DP communication overlapped with<--size:9.001791954040527-->
<--size:9.055215835571289-->computation, significantly increase the number of possible map-<--size:9.055215835571289-->
<--size:9.055215835571289-->pings and move the optimal parallelization point towards higher<--size:9.055215835571289-->
<--size:8.965999603271484-->TP and DP with lower PP . And doubling memory capacity yields<--size:8.965999603271484-->
<--size:8.965999603271484-->Fig. 5(d), shifts such points even more in that direction.<--size:8.965999603271484-->
<--size:10.909000396728516-->5 OPTIMAL STRATEGY SEARCH<--size:10.909000396728516-->
<--size:8.875886917114258-->Beyond the examples in Section 4, we observed numerous instances<--size:8.875886917114258-->
<--size:8.965999603271484-->of complex interactions in configuration parameters.<--size:8.965999603271484-->
<--size:10.909000396728516-->5.1<--size:10.909000396728516-->
<--size:10.909000396728516-->Optimal Execution<--size:10.909000396728516-->
<--size:8.875886917114258-->There were many instances where selecting a memory-saving tech-<--size:8.875886917114258-->
<--size:8.875886917114258-->nique frees enough memory to allow a different software technique<--size:8.875886917114258-->
<--size:8.93006420135498-->needing memory to use the “newly available” capacity to decrease<--size:8.93006420135498-->
<--size:9.055215835571289-->running time. Discovering these effects motivates the design of<--size:9.055215835571289-->
<--size:8.965999603271484-->Calculon to allow exploring of the full combinatorial space.<--size:8.965999603271484-->
<--size:6.973999977111816-->Calculon: a Methodology and Tool for High-Level Codesign of Systems and Large Language Models<--size:6.973999977111816-->
<--size:6.973999977111816-->SC ’23, November 12–17, 2023, Denver, CO, USA<--size:6.973999977111816-->
<--size:5.267959117889404-->p=1<--size:5.267959117889404-->
<--size:5.267959117889404-->p=2<--size:5.267959117889404-->
<--size:5.267959117889404-->p=4<--size:5.267959117889404-->
<--size:5.267959117889404-->p=8<--size:5.267959117889404-->
<--size:5.267959117889404-->p=16<--size:5.267959117889404-->
<--size:5.267959117889404-->p=32<--size:5.267959117889404-->
<--size:5.267959117889404-->p=64<--size:5.267959117889404-->
<--size:5.268010139465332-->t=1<--size:5.268010139465332-->
<--size:5.268010139465332-->t=2<--size:5.268010139465332-->
<--size:5.268010139465332-->t=4<--size:5.268010139465332-->
<--size:5.268010139465332-->t=8<--size:5.268010139465332-->
<--size:5.268010139465332-->t=16<--size:5.268010139465332-->
<--size:5.268010139465332-->t=32<--size:5.268010139465332-->
<--size:4.310190200805664-->70.3<--size:4.310190200805664-->
<--size:4.310190200805664-->78G<--size:4.310190200805664-->
<--size:4.310190200805664-->62.5<--size:4.310190200805664-->
<--size:4.310190200805664-->79G<--size:4.310190200805664-->
<--size:4.310190200805664-->64.8<--size:4.310190200805664-->
<--size:4.310190200805664-->52G<--size:4.310190200805664-->
<--size:4.310190200805664-->70.8<--size:4.310190200805664-->
<--size:4.310190200805664-->80G<--size:4.310190200805664-->
<--size:4.310190200805664-->72.2<--size:4.310190200805664-->
<--size:4.310190200805664-->51G<--size:4.310190200805664-->
<--size:4.310190200805664-->75.0<--size:4.310190200805664-->
<--size:4.310190200805664-->30G<--size:4.310190200805664-->
<--size:4.310190200805664-->89.7<--size:4.310190200805664-->
<--size:4.310190200805664-->79G<--size:4.310190200805664-->
<--size:4.310190200805664-->89.4<--size:4.310190200805664-->
<--size:4.310190200805664-->70G<--size:4.310190200805664-->
<--size:4.310190200805664-->92.2<--size:4.310190200805664-->
<--size:4.310190200805664-->56G<--size:4.310190200805664-->
<--size:4.310190200805664-->95.6<--size:4.310190200805664-->
<--size:4.310190200805664-->22G<--size:4.310190200805664-->
<--size:5.746920108795166-->(a) Time and mem, 80 GiB<--size:5.746920108795166-->
<--size:5.746920108795166-->original optimizations<--size:5.746920108795166-->
<--size:5.267959117889404-->p=1<--size:5.267959117889404-->
<--size:5.267959117889404-->p=2<--size:5.267959117889404-->
<--size:5.267959117889404-->p=4<--size:5.267959117889404-->
<--size:5.267959117889404-->p=8<--size:5.267959117889404-->
<--size:5.267959117889404-->p=16<--size:5.267959117889404-->
<--size:5.267959117889404-->p=32<--size:5.267959117889404-->
<--size:5.267959117889404-->p=64<--size:5.267959117889404-->
<--size:5.268010139465332-->t=1<--size:5.268010139465332-->
<--size:5.268010139465332-->t=2<--size:5.268010139465332-->
<--size:5.268010139465332-->t=4<--size:5.268010139465332-->
<--size:5.268010139465332-->t=8<--size:5.268010139465332-->
<--size:5.268010139465332-->t=16<--size:5.268010139465332-->
<--size:5.268010139465332-->t=32<--size:5.268010139465332-->
<--size:4.310190200805664-->69.8<--size:4.310190200805664-->
<--size:4.310190200805664-->78G<--size:4.310190200805664-->
<--size:4.310190200805664-->60.3<--size:4.310190200805664-->
<--size:4.310190200805664-->78G<--size:4.310190200805664-->
<--size:4.310190200805664-->48.4<--size:4.310190200805664-->
<--size:4.310190200805664-->72G<--size:4.310190200805664-->
<--size:4.310190200805664-->53.7<--size:4.310190200805664-->
<--size:4.310190200805664-->75G<--size:4.310190200805664-->
<--size:4.310190200805664-->53.4<--size:4.310190200805664-->
<--size:4.310190200805664-->78G<--size:4.310190200805664-->
<--size:4.310190200805664-->53.5<--size:4.310190200805664-->
<--size:4.310190200805664-->79G<--size:4.310190200805664-->
<--size:4.310190200805664-->67.8<--size:4.310190200805664-->
<--size:4.310190200805664-->75G<--size:4.310190200805664-->
<--size:4.310190200805664-->64.7<--size:4.310190200805664-->
<--size:4.310190200805664-->79G<--size:4.310190200805664-->
<--size:4.310190200805664-->64.9<--size:4.310190200805664-->
<--size:4.310190200805664-->70G<--size:4.310190200805664-->
<--size:4.310190200805664-->65.5<--size:4.310190200805664-->
<--size:4.310190200805664-->74G<--size:4.310190200805664-->
<--size:5.746920108795166-->(b) Time and mem, 80 GiB<--size:5.746920108795166-->
<--size:5.746920108795166-->sequence parallelism optimations<--size:5.746920108795166-->
<--size:5.267959117889404-->p=1<--size:5.267959117889404-->
<--size:5.267959117889404-->p=2<--size:5.267959117889404-->
<--size:5.267959117889404-->p=4<--size:5.267959117889404-->
<--size:5.267959117889404-->p=8<--size:5.267959117889404-->
<--size:5.267959117889404-->p=16<--size:5.267959117889404-->
<--size:5.267959117889404-->p=32<--size:5.267959117889404-->
<--size:5.267959117889404-->p=64<--size:5.267959117889404-->
<--size:5.268010139465332-->t=1<--size:5.268010139465332-->
<--size:5.268010139465332-->t=2<--size:5.268010139465332-->
<--size:5.268010139465332-->t=4<--size:5.268010139465332-->
<--size:5.268010139465332-->t=8<--size:5.268010139465332-->
<--size:5.268010139465332-->t=16<--size:5.268010139465332-->
<--size:5.268010139465332-->t=32<--size:5.268010139465332-->
<--size:4.310190200805664-->114.8<--size:4.310190200805664-->
<--size:4.310190200805664-->80G<--size:4.310190200805664-->
<--size:4.310190200805664-->69.9<--size:4.310190200805664-->
<--size:4.310190200805664-->67G<--size:4.310190200805664-->
<--size:4.310190200805664-->74.0<--size:4.310190200805664-->
<--size:4.310190200805664-->56G<--size:4.310190200805664-->
<--size:4.310190200805664-->52.0<--size:4.310190200805664-->
<--size:4.310190200805664-->56G<--size:4.310190200805664-->
<--size:4.310190200805664-->41.1<--size:4.310190200805664-->
<--size:4.310190200805664-->78G<--size:4.310190200805664-->
<--size:4.310190200805664-->50.1<--size:4.310190200805664-->
<--size:4.310190200805664-->60G<--size:4.310190200805664-->
<--size:4.310190200805664-->53.3<--size:4.310190200805664-->
<--size:4.310190200805664-->80G<--size:4.310190200805664-->
<--size:4.310190200805664-->38.8<--size:4.310190200805664-->
<--size:4.310190200805664-->59G<--size:4.310190200805664-->
<--size:4.310190200805664-->38.6<--size:4.310190200805664-->
<--size:4.310190200805664-->69G<--size:4.310190200805664-->
<--size:4.310190200805664-->40.0<--size:4.310190200805664-->
<--size:4.310190200805664-->40G<--size:4.310190200805664-->
<--size:4.310190200805664-->41.5<--size:4.310190200805664-->
<--size:4.310190200805664-->62G<--size:4.310190200805664-->
<--size:4.310190200805664-->43.1<--size:4.310190200805664-->
<--size:4.310190200805664-->75G<--size:4.310190200805664-->
<--size:4.310190200805664-->37.9<--size:4.310190200805664-->
<--size:4.310190200805664-->79G<--size:4.310190200805664-->
<--size:4.310190200805664-->38.4<--size:4.310190200805664-->
<--size:4.310190200805664-->64G<--size:4.310190200805664-->
<--size:4.310190200805664-->39.0<--size:4.310190200805664-->
<--size:4.310190200805664-->48G<--size:4.310190200805664-->
<--size:4.310190200805664-->39.9<--size:4.310190200805664-->
<--size:4.310190200805664-->74G<--size:4.310190200805664-->
<--size:4.310190200805664-->40.7<--size:4.310190200805664-->
<--size:4.310190200805664-->68G<--size:4.310190200805664-->
<--size:4.310190200805664-->43.8<--size:4.310190200805664-->
<--size:4.310190200805664-->79G<--size:4.310190200805664-->
<--size:4.310190200805664-->41.2<--size:4.310190200805664-->
<--size:4.310190200805664-->77G<--size:4.310190200805664-->
<--size:4.310190200805664-->41.0<--size:4.310190200805664-->
<--size:4.310190200805664-->74G<--size:4.310190200805664-->
<--size:4.310190200805664-->41.3<--size:4.310190200805664-->
<--size:4.310190200805664-->70G<--size:4.310190200805664-->
<--size:4.310190200805664-->41.8<--size:4.310190200805664-->
<--size:4.310190200805664-->67G<--size:4.310190200805664-->
<--size:4.310190200805664-->42.4<--size:4.310190200805664-->
<--size:4.310190200805664-->72G<--size:4.310190200805664-->
<--size:4.310190200805664-->43.5<--size:4.310190200805664-->
<--size:4.310190200805664-->61G<--size:4.310190200805664-->
<--size:5.746920108795166-->(c) Time and mem, 80 GiB<--size:5.746920108795166-->
<--size:5.746920108795166-->all optimizations<--size:5.746920108795166-->
<--size:5.267959117889404-->p=1<--size:5.267959117889404-->
<--size:5.267959117889404-->p=2<--size:5.267959117889404-->
<--size:5.267959117889404-->p=4<--size:5.267959117889404-->
<--size:5.267959117889404-->p=8<--size:5.267959117889404-->
<--size:5.267959117889404-->p=16<--size:5.267959117889404-->
<--size:5.267959117889404-->p=32<--size:5.267959117889404-->
<--size:5.267959117889404-->p=64<--size:5.267959117889404-->
<--size:5.268010139465332-->t=1<--size:5.268010139465332-->
<--size:5.268010139465332-->t=2<--size:5.268010139465332-->
<--size:5.268010139465332-->t=4<--size:5.268010139465332-->
<--size:5.268010139465332-->t=8<--size:5.268010139465332-->
<--size:5.268010139465332-->t=16<--size:5.268010139465332-->
<--size:5.268010139465332-->t=32<--size:5.268010139465332-->
<--size:4.310190200805664-->78.4<--size:4.310190200805664-->
<--size:4.310190200805664-->112G<--size:4.310190200805664-->
<--size:4.310190200805664-->90.5<--size:4.310190200805664-->
<--size:4.310190200805664-->85G<--size:4.310190200805664-->
<--size:4.310190200805664-->67.0<--size:4.310190200805664-->
<--size:4.310190200805664-->153G<--size:4.310190200805664-->
<--size:4.310190200805664-->68.0<--size:4.310190200805664-->
<--size:4.310190200805664-->95G<--size:4.310190200805664-->
<--size:4.310190200805664-->53.4<--size:4.310190200805664-->
<--size:4.310190200805664-->152G<--size:4.310190200805664-->
<--size:4.310190200805664-->56.5<--size:4.310190200805664-->
<--size:4.310190200805664-->157G<--size:4.310190200805664-->
<--size:4.310190200805664-->55.4<--size:4.310190200805664-->
<--size:4.310190200805664-->143G<--size:4.310190200805664-->
<--size:4.310190200805664-->38.8<--size:4.310190200805664-->
<--size:4.310190200805664-->114G<--size:4.310190200805664-->
<--size:4.310190200805664-->39.6<--size:4.310190200805664-->
<--size:4.310190200805664-->87G<--size:4.310190200805664-->
<--size:4.310190200805664-->41.1<--size:4.310190200805664-->
<--size:4.310190200805664-->78G<--size:4.310190200805664-->
<--size:4.310190200805664-->44.1<--size:4.310190200805664-->
<--size:4.310190200805664-->125G<--size:4.310190200805664-->
<--size:4.310190200805664-->45.7<--size:4.310190200805664-->
<--size:4.310190200805664-->146G<--size:4.310190200805664-->
<--size:4.310190200805664-->37.1<--size:4.310190200805664-->
<--size:4.310190200805664-->131G<--size:4.310190200805664-->
<--size:4.310190200805664-->37.6<--size:4.310190200805664-->
<--size:4.310190200805664-->103G<--size:4.310190200805664-->
<--size:4.310190200805664-->38.6<--size:4.310190200805664-->
<--size:4.310190200805664-->69G<--size:4.310190200805664-->
<--size:4.310190200805664-->39.3<--size:4.310190200805664-->
<--size:4.310190200805664-->148G<--size:4.310190200805664-->
<--size:4.310190200805664-->40.9<--size:4.310190200805664-->
<--size:4.310190200805664-->137G<--size:4.310190200805664-->
<--size:4.310190200805664-->42.8<--size:4.310190200805664-->
<--size:4.310190200805664-->154G<--size:4.310190200805664-->
<--size:4.310190200805664-->37.3<--size:4.310190200805664-->
<--size:4.310190200805664-->128G<--size:4.310190200805664-->
<--size:4.310190200805664-->37.3<--size:4.310190200805664-->
<--size:4.310190200805664-->146G<--size:4.310190200805664-->
<--size:4.310190200805664-->37.7<--size:4.310190200805664-->
<--size:4.310190200805664-->132G<--size:4.310190200805664-->
<--size:4.310190200805664-->38.3<--size:4.310190200805664-->
<--size:4.310190200805664-->131G<--size:4.310190200805664-->
<--size:4.310190200805664-->39.2<--size:4.310190200805664-->
<--size:4.310190200805664-->118G<--size:4.310190200805664-->
<--size:4.310190200805664-->40.7<--size:4.310190200805664-->
<--size:4.310190200805664-->68G<--size:4.310190200805664-->
<--size:4.310190200805664-->40.4<--size:4.310190200805664-->
<--size:4.310190200805664-->127G<--size:4.310190200805664-->
<--size:4.310190200805664-->40.3<--size:4.310190200805664-->
<--size:4.310190200805664-->143G<--size:4.310190200805664-->
<--size:4.310190200805664-->40.7<--size:4.310190200805664-->
<--size:4.310190200805664-->130G<--size:4.310190200805664-->
<--size:4.310190200805664-->41.2<--size:4.310190200805664-->
<--size:4.310190200805664-->138G<--size:4.310190200805664-->
<--size:4.310190200805664-->41.7<--size:4.310190200805664-->
<--size:4.310190200805664-->109G<--size:4.310190200805664-->
<--size:4.310190200805664-->42.4<--size:4.310190200805664-->
<--size:4.310190200805664-->81G<--size:4.310190200805664-->
<--size:4.310190200805664-->43.5<--size:4.310190200805664-->
<--size:4.310190200805664-->61G<--size:4.310190200805664-->
<--size:5.746920108795166-->(d) Time and mem, 160 GiB<--size:5.746920108795166-->
<--size:5.746920108795166-->all optimizations<--size:5.746920108795166-->
<--size:8.620380401611328-->Megatron-1T single batch of training on<--size:8.620380401611328-->
<--size:8.620380401611328-->4096 A100 GPUs with various optimizations<--size:8.620380401611328-->
<--size:8.875886917114258-->Figure 5: Batch time training with various optimizations and<--size:8.875886917114258-->
<--size:8.912040710449219-->constraints (dashes indicate infeasible configurations due to<--size:8.912040710449219-->
<--size:9.055215835571289-->a lack of memory). The top number shows the best batch<--size:9.055215835571289-->
<--size:9.055215835571289-->time, and the bottom number shows the required memory<--size:9.055215835571289-->
<--size:8.875886917114258-->to run, 𝑡 and 𝑝 measure the degree of TP and PP . (a) time for<--size:8.875886917114258-->
<--size:8.952540397644043-->a system with 32 A100 with 80 GiB HBM in a single NVLink<--size:8.952540397644043-->
<--size:9.055215835571289-->domain and original optimizations set [ 29 ]; (b) same with<--size:9.055215835571289-->
<--size:9.055215835571289-->partial recompute and sequence parallelism [ 20 ]; (c) same<--size:9.055215835571289-->
<--size:8.997325897216797-->with all optimizations from Table 1; (d) same with memory<--size:8.997325897216797-->
<--size:8.965999603271484-->capacity increased to 160 GiB per GPU.<--size:8.965999603271484-->
<--size:8.970481872558594-->To get a coarse sense of this space, we considered all execution<--size:8.970481872558594-->
<--size:8.970481872558594-->configurations for training the GPT-3 175B-parameter model on a<--size:8.970481872558594-->
<--size:8.965999603271484-->4,096 - GPU system. There were 10,957,376 calculations possible. Out<--size:8.965999603271484-->
<--size:8.875886917114258-->of all possible configurations, only 1,974,902 were feasible (~18%) as<--size:8.875886917114258-->
<--size:8.93006420135498-->the rest would not have sufficient resources to run. The histogram<--size:8.93006420135498-->
<--size:8.875886917114258-->in Fig. 6(a) depicts the distribution of estimated performance of the<--size:8.875886917114258-->
<--size:8.898500442504883-->feasible runs measured as sample-processing rate, that is, the num-<--size:8.898500442504883-->
<--size:9.028543472290039-->ber of data samples processed per second of LLM training. While<--size:9.028543472290039-->
<--size:8.94355583190918-->the histogram has 10 bins, the 2 rightmost bins that correspond to<--size:8.94355583190918-->
<--size:9.055215835571289-->configurations within 20 % of the best configuration are indistin-<--size:9.055215835571289-->
<--size:8.898500442504883-->guishable with the naked eye. We observed only 30 configurations,<--size:8.898500442504883-->
<--size:8.957029342651367-->or less than 0.002% of the full space, achieved performance within<--size:8.957029342651367-->
<--size:8.965999603271484-->10 % of the best configuration. Figure 6(b) considers just the 100<--size:8.965999603271484-->
<--size:9.032994270324707-->best performers as an empirical cumulative distribution function.<--size:9.032994270324707-->
<--size:9.055215835571289-->Only about ten attain performance within 5 % of the best. Thus,<--size:9.055215835571289-->
<--size:8.965999603271484-->good configurations may be like needles in a haystack.<--size:8.965999603271484-->
<--size:8.952540397644043-->Anecdotally, several of these best-configurations did not match<--size:8.952540397644043-->
<--size:9.055215835571289-->our expectations or commonly reported heuristics. For example,<--size:9.055215835571289-->
<--size:9.055215835571289-->some of the best partitioning strategies included setting TP to 4<--size:9.055215835571289-->
<--size:8.94355583190918-->and PP to 2, moving both under the capacity of an 8- GPU NVLink<--size:8.94355583190918-->
<--size:4.908200263977051-->0 200<--size:4.908200263977051-->
<--size:4.908200263977051-->400<--size:4.908200263977051-->
<--size:4.908200263977051-->600<--size:4.908200263977051-->
<--size:4.908200263977051-->800<--size:4.908200263977051-->
<--size:4.908200263977051-->1000<--size:4.908200263977051-->
<--size:5.354400157928467-->Sample rate<--size:5.354400157928467-->
<--size:4.908200263977051-->0<--size:4.908200263977051-->
<--size:4.908200263977051-->300k<--size:4.908200263977051-->
<--size:4.908200263977051-->600k<--size:4.908200263977051-->
<--size:5.354400157928467-->Occurances<--size:5.354400157928467-->
<--size:5.354400157928467-->(a) Sample rate distribution<--size:5.354400157928467-->
<--size:4.908200263977051-->930<--size:4.908200263977051-->
<--size:4.908200263977051-->970<--size:4.908200263977051-->
<--size:4.908200263977051-->1010<--size:4.908200263977051-->
<--size:4.908200263977051-->1050<--size:4.908200263977051-->
<--size:4.908200263977051-->1090<--size:4.908200263977051-->
<--size:5.354400157928467-->Sample rate<--size:5.354400157928467-->
<--size:4.908200263977051-->0<--size:4.908200263977051-->
<--size:4.908200263977051-->0.2<--size:4.908200263977051-->
<--size:4.908200263977051-->0.4<--size:4.908200263977051-->
<--size:4.908200263977051-->0.6<--size:4.908200263977051-->
<--size:4.908200263977051-->0.8<--size:4.908200263977051-->
<--size:4.908200263977051-->1.0<--size:4.908200263977051-->
<--size:5.354400157928467-->CDF<--size:5.354400157928467-->
<--size:5.354400157928467-->(b) Top-100 sample rate CDF<--size:5.354400157928467-->
<--size:8.031599998474121-->1,974,902 execution strategies<--size:8.031599998474121-->
<--size:8.031599998474121-->for GPT3 175B on 4096 GPUs<--size:8.031599998474121-->
<--size:9.055215835571289-->Figure 6: All possible execution strategies for GPT3 175B<--size:9.055215835571289-->
<--size:8.997325897216797-->model training on 4,096 GPUs. (a) Histogram of the sample<--size:8.997325897216797-->
<--size:8.965999603271484-->rate. (b) CDF plot of top-100 configurations.<--size:8.965999603271484-->
<--size:8.875886917114258-->domain, whereas conventional wisdom assumes TP should be used<--size:8.875886917114258-->
<--size:8.965999603271484-->to saturate NVLink with PP needing only a slower network.<--size:8.965999603271484-->
<--size:8.934563636779785-->For these reasons we implemented an optimal execution search<--size:8.934563636779785-->
<--size:9.055215835571289-->engine in Calculon. Unlike the procedure described in Section 2,<--size:9.055215835571289-->
<--size:9.055215835571289-->which yields a result based on a single execution configuration,<--size:9.055215835571289-->
<--size:8.970481872558594-->this search engine exhaustively tries all possible execution config-<--size:8.970481872558594-->
<--size:9.055215835571289-->urations for a specific system, system size, and LLM and returns<--size:9.055215835571289-->
<--size:9.055215835571289-->the best performer and its statistics. Because Calculon calculates<--size:9.055215835571289-->
<--size:8.880414009094238-->performance so quickly, a standard multi-core desktop computer is<--size:8.880414009094238-->
<--size:8.965999603271484-->able to search the entire configuration space in minutes.<--size:8.965999603271484-->
<--size:10.909000396728516-->5.2<--size:10.909000396728516-->
<--size:10.909000396728516-->Optimal System Size<--size:10.909000396728516-->
<--size:9.01071834564209-->The most important “variable” affecting performance is the struc-<--size:9.01071834564209-->
<--size:8.875886917114258-->ture of the LLM model itself, which can impose many constraints on<--size:8.875886917114258-->
<--size:8.875886917114258-->what partitioning strategies will be most effective, memory capacity<--size:8.875886917114258-->
<--size:8.875886917114258-->requirements, and the ideal system size (number of processors). For<--size:8.875886917114258-->
<--size:9.055215835571289-->example, for best system utilization, PP should split transformer<--size:9.055215835571289-->
<--size:9.041889190673828-->blocks evenly, while TP should split attention heads and the neu-<--size:9.041889190673828-->
<--size:8.875886917114258-->rons in MLP block evenly, possibly cutting them so the local matrix<--size:8.875886917114258-->
<--size:8.965999603271484-->multiplication size is divisible by a large power of 2, typically 128.<--size:8.965999603271484-->
<--size:8.875886917114258-->One way this interaction between LLM shape and system param-<--size:8.875886917114258-->
<--size:9.055215835571289-->eters manifests is in the phenomenon of “efficiency cliffs,” which<--size:9.055215835571289-->
<--size:8.875886917114258-->are sudden drops in performance among system configurations that<--size:8.875886917114258-->
<--size:9.019635200500488-->are close in size. We can observe these cliffs in Fig. 7. It considers<--size:9.019635200500488-->
<--size:9.041889190673828-->three different LLMs: GPT3 175B [ 3 ], Turing-NLG 530B [ 45 ], and<--size:9.041889190673828-->
<--size:8.875886917114258-->Megatron-1T [ 29 ] models. Given a system with a certain number of<--size:8.875886917114258-->
<--size:8.875886917114258-->GPUs (x-axis, considering only multiples of 8 GPUs), we search the<--size:8.875886917114258-->
<--size:8.912040710449219-->full configuration space for the best performer that utilizes exactly<--size:8.912040710449219-->
<--size:8.875886917114258-->that many GPUs and plot its performance as a point (y-axis, higher<--size:8.875886917114258-->
<--size:9.055215835571289-->is better). Each data point is an exhaustive search performed as<--size:9.055215835571289-->
<--size:8.875886917114258-->described in Section 5.1. While the overall trend (envelope) steadily<--size:8.875886917114258-->
<--size:8.898500442504883-->increases with system size, observe that the performance variation<--size:8.898500442504883-->
<--size:8.965999603271484-->in “top performers” also grows dramatically. The Turing-NLG has<--size:8.965999603271484-->
<--size:9.001791954040527-->many LLM size-parameters that are not powers of two, making it<--size:9.001791954040527-->
<--size:8.903016090393066-->particularly tricky to map. This phenomenon occurs for all models<--size:8.903016090393066-->
<--size:9.032994270324707-->and implies that even though one might buy a system with some<--size:9.032994270324707-->
<--size:8.921056747436523-->number of GPUs, mapping a specific model might utilize a smaller<--size:8.921056747436523-->
<--size:6.973999977111816-->SC ’23, November 12–17, 2023, Denver, CO, USA<--size:6.973999977111816-->
<--size:6.973999977111816-->Isaev, et al.<--size:6.973999977111816-->
<--size:5.906612873077393-->0 1024<--size:5.906612873077393-->
<--size:5.906612873077393-->2048<--size:5.906612873077393-->
<--size:5.906612873077393-->3072<--size:5.906612873077393-->
<--size:5.906612873077393-->4096<--size:5.906612873077393-->
<--size:5.906612873077393-->5120<--size:5.906612873077393-->
<--size:5.906612873077393-->6144<--size:5.906612873077393-->
<--size:5.906612873077393-->7168<--size:5.906612873077393-->
<--size:5.906612873077393-->8192<--size:5.906612873077393-->
<--size:5.906670093536377-->0.0<--size:5.906670093536377-->
<--size:5.906670093536377-->0.2<--size:5.906670093536377-->
<--size:5.906670093536377-->0.4<--size:5.906670093536377-->
<--size:5.906670093536377-->0.6<--size:5.906670093536377-->
<--size:5.906670093536377-->0.8<--size:5.906670093536377-->
<--size:5.906670093536377-->1.0<--size:5.906670093536377-->
<--size:6.443640232086182-->Relative scaling<--size:6.443640232086182-->
<--size:6.443640232086182-->GPT3 175B<--size:6.443640232086182-->
<--size:5.906612873077393-->0 1024<--size:5.906612873077393-->
<--size:5.906612873077393-->2048<--size:5.906612873077393-->
<--size:5.906612873077393-->3072<--size:5.906612873077393-->
<--size:5.906612873077393-->4096<--size:5.906612873077393-->
<--size:5.906612873077393-->5120<--size:5.906612873077393-->
<--size:5.906612873077393-->6144<--size:5.906612873077393-->
<--size:5.906612873077393-->7168<--size:5.906612873077393-->
<--size:5.906612873077393-->8192<--size:5.906612873077393-->
<--size:6.443640232086182-->System size<--size:6.443640232086182-->
<--size:5.906670093536377-->0.0<--size:5.906670093536377-->
<--size:5.906670093536377-->0.2<--size:5.906670093536377-->
<--size:5.906670093536377-->0.4<--size:5.906670093536377-->
<--size:5.906670093536377-->0.6<--size:5.906670093536377-->
<--size:5.906670093536377-->0.8<--size:5.906670093536377-->
<--size:5.906670093536377-->1.0<--size:5.906670093536377-->
<--size:6.443640232086182-->Turing-NLG 530B<--size:6.443640232086182-->
<--size:6.443640232086182-->Perfect scaling<--size:6.443640232086182-->
<--size:6.443640232086182-->Best performance<--size:6.443640232086182-->
<--size:5.906612873077393-->0 1024<--size:5.906612873077393-->
<--size:5.906612873077393-->2048<--size:5.906612873077393-->
<--size:5.906612873077393-->3072<--size:5.906612873077393-->
<--size:5.906612873077393-->4096<--size:5.906612873077393-->
<--size:5.906612873077393-->5120<--size:5.906612873077393-->
<--size:5.906612873077393-->6144<--size:5.906612873077393-->
<--size:5.906612873077393-->7168<--size:5.906612873077393-->
<--size:5.906612873077393-->8192<--size:5.906612873077393-->
<--size:5.906670093536377-->0.0<--size:5.906670093536377-->
<--size:5.906670093536377-->0.2<--size:5.906670093536377-->
<--size:5.906670093536377-->0.4<--size:5.906670093536377-->
<--size:5.906670093536377-->0.6<--size:5.906670093536377-->
<--size:5.906670093536377-->0.8<--size:5.906670093536377-->
<--size:5.906670093536377-->1.0<--size:5.906670093536377-->
<--size:6.443640232086182-->Megatron-1T<--size:6.443640232086182-->
<--size:9.665460586547852-->LLM training scalability (no offloading)<--size:9.665460586547852-->
<--size:8.875886917114258-->Figure 7: Scaling LLM efficiency training on up to 8,192 GPUs.<--size:8.875886917114258-->
<--size:9.055215835571289-->“Efficiency cliffs”—sudden drops in performance—become<--size:9.055215835571289-->
<--size:9.055215835571289-->worse as the system size increases due to the difficulty of<--size:9.055215835571289-->
<--size:8.965999603271484-->finding a good mapping of the LLM to the system.<--size:8.965999603271484-->
<--size:8.875886917114258-->number due to these cliffs. Moreover, in two of the LLMs fewer con-<--size:8.875886917114258-->
<--size:8.907529830932617-->figurations can run at all, as represented by the increasing number<--size:8.907529830932617-->
<--size:8.965999603271484-->of points with zero relative performance.<--size:8.965999603271484-->
<--size:9.055215835571289-->In short, Calculon can be used to find the optimal execution<--size:9.055215835571289-->
<--size:8.916550636291504-->strategy for a particular system by exhaustively searching the con-<--size:8.916550636291504-->
<--size:9.055215835571289-->figuration space for every possible system partition size. For the<--size:9.055215835571289-->
<--size:9.055215835571289-->3 LLMs presented in Fig. 7 this required 467,553,284 calculations.<--size:9.055215835571289-->
<--size:8.94355583190918-->Indeed, the case of a “single-use” system in the context of systems<--size:8.94355583190918-->
<--size:9.055215835571289-->for LLM training is not out of the question, and this finding that<--size:9.055215835571289-->
<--size:8.875886917114258-->arbitrary increases in system size might not deliver the best perfor-<--size:8.875886917114258-->
<--size:8.979438781738281-->mance or efficiency, counters some prior claims [ 56 ]. Right-sizing<--size:8.979438781738281-->
<--size:8.992857933044434-->the system in light of such phenomena could mean the difference<--size:8.992857933044434-->
<--size:9.055215835571289-->between deciding to use or acquire a relatively smaller system<--size:9.055215835571289-->
<--size:9.028543472290039-->knowing that, due to efficiency cliffs, there might not be a usable<--size:9.028543472290039-->
<--size:8.965999603271484-->configuration without increasing the system scale by a lot.<--size:8.965999603271484-->
<--size:10.909000396728516-->6 OFFLOADING ANALYSIS<--size:10.909000396728516-->
<--size:8.875886917114258-->An interesting type of analysis enabled by Calculon is tensor offload-<--size:8.875886917114258-->
<--size:9.032994270324707-->ing. Recall that some software techniques aim to reduce memory<--size:9.032994270324707-->
<--size:9.006256103515625-->requirements while others trade increased memory requirements<--size:9.006256103515625-->
<--size:8.916550636291504-->for better performance. In hardware, there are at least two options<--size:8.916550636291504-->
<--size:9.055215835571289-->to alleviate capacity issues. One is to directly increase the size of<--size:9.055215835571289-->
<--size:8.875886917114258-->the processor’s memory system, typically HBM for top performing<--size:8.875886917114258-->
<--size:8.979438781738281-->processors and accelerators. Unfortunately, scaling HBM capacity<--size:8.979438781738281-->
<--size:9.055215835571289-->is limited by strict and expensive technological constraints. An<--size:9.055215835571289-->
<--size:9.055215835571289-->alternative is to employ an external high-capacity memory, such<--size:9.055215835571289-->
<--size:9.055215835571289-->as Central Processing Unit ( CPU ) memory or compute express<--size:9.055215835571289-->
<--size:9.032994270324707-->link ( CXL )-attached memory. Software strategies can exploit this<--size:9.032994270324707-->
<--size:8.974961280822754-->approach via zero-offload [ 42 ] and zero-infinity [ 38 ] for tensor of-<--size:8.974961280822754-->
<--size:8.875886917114258-->floading to CPU memory. These methods alleviate memory capacity<--size:8.875886917114258-->
<--size:9.055215835571289-->issues but may incur other costs, such as increased computation<--size:9.055215835571289-->
<--size:8.921056747436523-->(e.g., under activation checkpointing) or increased communication<--size:8.921056747436523-->
<--size:8.965999603271484-->costs due partly to limited bandwidth to external memory.<--size:8.965999603271484-->
<--size:6.1622443199157715-->K-2 K-1<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K-2 K-1<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K+1<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K-2 K-1<--size:6.1622443199157715-->
<--size:6.1622443199157715-->Compute<--size:6.1622443199157715-->
<--size:6.1622443199157715-->Prefetch<--size:6.1622443199157715-->
<--size:6.1622443199157715-->Offload<--size:6.1622443199157715-->
<--size:6.1622443199157715-->Offloading memory<--size:6.1622443199157715-->
<--size:6.1622443199157715-->GPU memory<--size:6.1622443199157715-->
<--size:6.1622443199157715-->Time<--size:6.1622443199157715-->
<--size:6.1622443199157715-->ti<--size:6.1622443199157715-->
<--size:6.1622443199157715-->ti-1<--size:6.1622443199157715-->
<--size:6.1622443199157715-->ti-2<--size:6.1622443199157715-->
<--size:6.1622443199157715-->ti-3<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K K+1<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K+1<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K+2<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K+2<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K+2<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K K-1<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K+1<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K-2 K-1<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K K+1 K+2<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K-3<--size:6.1622443199157715-->
<--size:6.1622443199157715-->K+3<--size:6.1622443199157715-->
<--size:7.1892852783203125-->Compute pipeline<--size:7.1892852783203125-->
<--bold--><--size:7.1892852783203125-->Memory exchange at time t<--size:7.1892852783203125--><--bold-->
<--size:6.1622443199157715-->ti+1 ti+2 ti+3<--size:6.1622443199157715-->
<--size:8.965999603271484-->Figure 8: Tensor offloading scheme modeled in Calculon<--size:8.965999603271484-->
<--size:8.974961280822754-->We can use Calculon to analyze what the minimum bandwidth<--size:8.974961280822754-->
<--size:8.948049545288086-->requirements for the offloaded memory system should be to make<--size:8.948049545288086-->
<--size:8.916550636291504-->the offloading effective. In this analysis, we assume that offloading<--size:8.916550636291504-->
<--size:8.94355583190918-->overlaps only with compute and network operations and not with<--size:8.94355583190918-->
<--size:8.992857933044434-->memory operations to HBM memory. This assumption avoids po-<--size:8.992857933044434-->
<--size:9.055215835571289-->tential compute throttling that depends on communication with<--size:9.055215835571289-->
<--size:8.965999603271484-->HBM memory. Also, we assume that offloading could be performed<--size:8.965999603271484-->
<--size:9.055215835571289-->with CPU or a direct memory access ( DMA )-like engine similar<--size:9.055215835571289-->
<--size:9.055215835571289-->to Tensor Memory Accelerator ( TMA ) in the NVIDIA H100 [ 31 ],<--size:9.055215835571289-->
<--size:8.952540397644043-->and thereby requiring no computational resources on the training<--size:8.952540397644043-->
<--size:8.93006420135498-->accelerator. We analyze the required offloading capacity and band-<--size:8.93006420135498-->
<--size:8.875886917114258-->width, assuming we only store a single transformer block currently<--size:8.875886917114258-->
<--size:9.055215835571289-->required for computation in HBM , and allocate space worth of a<--size:9.055215835571289-->
<--size:9.055215835571289-->single block’s tensors that are being prefetched and offloaded, as<--size:9.055215835571289-->
<--size:8.98391342163086-->shown in Fig. 8. We fully overlap computation for a current block<--size:8.98391342163086-->
<--size:8.875886917114258-->with the offloading of the results of the previous block computations<--size:8.875886917114258-->
<--size:8.965999603271484-->and prefetching operands for the next one.<--size:8.965999603271484-->
<--size:8.965999603271484-->The bandwidth required for such seamless tensor offloading is<--size:8.965999603271484-->
<--size:8.965999603271484-->Bandwidth offload ≥ Size tensor<--size:8.965999603271484-->
<--size:8.965999603271484-->𝑇 compute<--size:8.965999603271484-->
<--size:8.965999603271484-->. (1)<--size:8.965999603271484-->
<--size:8.979438781738281-->Depending on the size of the offloading tensors, the highest band-<--size:8.979438781738281-->
<--size:9.055215835571289-->width may be required when prefetching weights during the for-<--size:9.055215835571289-->
<--size:8.875886917114258-->ward pass; offloading activations during the forward pass; or prefetch-<--size:8.875886917114258-->
<--size:8.965999603271484-->ing activations, weights, and optimizer during the backward pass.<--size:8.965999603271484-->
<--size:9.055215835571289-->While weights and optimizer space size depend only on the<--size:9.055215835571289-->
<--size:9.055215835571289-->model parameters, compute time and activation size depend on<--size:9.055215835571289-->
<--size:9.055215835571289-->model parameters and micro-batch size. Micro-batch size heav-<--size:9.055215835571289-->
<--size:9.055215835571289-->ily depends on the amount of available memory and, as such, on<--size:9.055215835571289-->
<--size:8.948049545288086-->hardware and other software implementation choices, as we want<--size:8.948049545288086-->
<--size:9.055215835571289-->to increase micro-batch size as much as possible as long there is<--size:9.055215835571289-->
<--size:8.8894624710083-->enough memory. The best choice of offloading memory bandwidth<--size:8.8894624710083-->
<--size:8.965999603271484-->and capacity depends on the combination of LLM and system.<--size:8.965999603271484-->
<--size:9.055215835571289-->In order to determine optimal offloading memory parameters,<--size:9.055215835571289-->
<--size:8.92556095123291-->we set up Calculon with an offloading memory of infinite capacity<--size:8.92556095123291-->
<--size:9.055215835571289-->and infinite bandwidth, and reported utilized capacity and band-<--size:9.055215835571289-->
<--size:9.055215835571289-->width. The results for a 1 trillion-parameter LLM are presented<--size:9.055215835571289-->
<--size:8.875886917114258-->in Fig. 9(a) for performance and HBM consumption, and in Fig. 9(b)<--size:8.875886917114258-->
<--size:8.875886917114258-->for offloading memory bandwidth and capacity utilization. Utilized<--size:8.875886917114258-->
<--size:9.055215835571289-->bandwidth almost reaches 600 GB/s , while required memory ca-<--size:9.055215835571289-->
<--size:9.055215835571289-->pacity is as high as 4 TiB . However, due to the greedy nature of<--size:9.055215835571289-->
<--size:9.037443161010742-->the search, Calculon reports required bandwidth for the absolute<--size:9.037443161010742-->
<--size:8.957029342651367-->best performing configuration. As seen in Fig. 6, there might exist<--size:8.957029342651367-->
<--size:9.037443161010742-->other configurations that have close enough performance but de-<--size:9.037443161010742-->
<--size:8.965999603271484-->manding fewer resources. With the desire to choose an offloading<--size:8.965999603271484-->
<--size:8.94355583190918-->memory that is realistic and practical, inspecting Fig. 9(a,b) shows<--size:8.94355583190918-->
<--size:8.875886917114258-->that there exist many configurations with good performance where<--size:8.875886917114258-->
<--size:6.973999977111816-->Calculon: a Methodology and Tool for High-Level Codesign of Systems and Large Language Models<--size:6.973999977111816-->
<--size:6.973999977111816-->SC ’23, November 12–17, 2023, Denver, CO, USA<--size:6.973999977111816-->
<--size:5.237819671630859-->p=1<--size:5.237819671630859-->
<--size:5.237819671630859-->p=2<--size:5.237819671630859-->
<--size:5.237819671630859-->p=4<--size:5.237819671630859-->
<--size:5.237819671630859-->p=8<--size:5.237819671630859-->
<--size:5.237819671630859-->p=16<--size:5.237819671630859-->
<--size:5.237819671630859-->p=32<--size:5.237819671630859-->
<--size:5.237870216369629-->t=1<--size:5.237870216369629-->
<--size:5.237870216369629-->t=2<--size:5.237870216369629-->
<--size:5.237870216369629-->t=4<--size:5.237870216369629-->
<--size:5.237870216369629-->t=8<--size:5.237870216369629-->
<--size:5.237870216369629-->t=16<--size:5.237870216369629-->
<--size:5.237870216369629-->t=32<--size:5.237870216369629-->
<--size:5.7140398025512695-->Infinite<--size:5.7140398025512695-->
<--size:4.285530090332031-->42<--size:4.285530090332031-->
<--size:4.285530090332031-->72G<--size:4.285530090332031-->
<--size:4.285530090332031-->38<--size:4.285530090332031-->
<--size:4.285530090332031-->72G<--size:4.285530090332031-->
<--size:4.285530090332031-->97<--size:4.285530090332031-->
<--size:4.285530090332031-->72G<--size:4.285530090332031-->
<--size:4.285530090332031-->145<--size:4.285530090332031-->
<--size:4.285530090332031-->72G<--size:4.285530090332031-->
<--size:4.285530090332031-->155<--size:4.285530090332031-->
<--size:4.285530090332031-->73G<--size:4.285530090332031-->
<--size:4.285530090332031-->144<--size:4.285530090332031-->
<--size:4.285530090332031-->73G<--size:4.285530090332031-->
<--size:4.285530090332031-->42<--size:4.285530090332031-->
<--size:4.285530090332031-->36G<--size:4.285530090332031-->
<--size:4.285530090332031-->93<--size:4.285530090332031-->
<--size:4.285530090332031-->50G<--size:4.285530090332031-->
<--size:4.285530090332031-->159<--size:4.285530090332031-->
<--size:4.285530090332031-->43G<--size:4.285530090332031-->
<--size:4.285530090332031-->175<--size:4.285530090332031-->
<--size:4.285530090332031-->43G<--size:4.285530090332031-->
<--size:4.285530090332031-->169<--size:4.285530090332031-->
<--size:4.285530090332031-->44G<--size:4.285530090332031-->
<--size:4.285530090332031-->162<--size:4.285530090332031-->
<--size:4.285530090332031-->38G<--size:4.285530090332031-->
<--size:4.285530090332031-->177<--size:4.285530090332031-->
<--size:4.285530090332031-->72G<--size:4.285530090332031-->
<--size:4.285530090332031-->238<--size:4.285530090332031-->
<--size:4.285530090332031-->72G<--size:4.285530090332031-->
<--size:4.285530090332031-->241<--size:4.285530090332031-->
<--size:4.285530090332031-->25G<--size:4.285530090332031-->
<--size:4.285530090332031-->235<--size:4.285530090332031-->
<--size:4.285530090332031-->25G<--size:4.285530090332031-->
<--size:4.285530090332031-->223<--size:4.285530090332031-->
<--size:4.285530090332031-->72G<--size:4.285530090332031-->
<--size:4.285530090332031-->211<--size:4.285530090332031-->
<--size:4.285530090332031-->66G<--size:4.285530090332031-->
<--size:4.285530090332031-->248<--size:4.285530090332031-->
<--size:4.285530090332031-->41G<--size:4.285530090332031-->
<--size:4.285530090332031-->250<--size:4.285530090332031-->
<--size:4.285530090332031-->18G<--size:4.285530090332031-->
<--size:4.285530090332031-->245<--size:4.285530090332031-->
<--size:4.285530090332031-->18G<--size:4.285530090332031-->
<--size:4.285530090332031-->239<--size:4.285530090332031-->
<--size:4.285530090332031-->13G<--size:4.285530090332031-->
<--size:4.285530090332031-->232<--size:4.285530090332031-->
<--size:4.285530090332031-->36G<--size:4.285530090332031-->
<--size:4.285530090332031-->222<--size:4.285530090332031-->
<--size:4.285530090332031-->35G<--size:4.285530090332031-->
<--size:4.285530090332031-->246<--size:4.285530090332031-->
<--size:4.285530090332031-->14G<--size:4.285530090332031-->
<--size:4.285530090332031-->239<--size:4.285530090332031-->
<--size:4.285530090332031-->14G<--size:4.285530090332031-->
<--size:4.285530090332031-->234<--size:4.285530090332031-->
<--size:4.285530090332031-->9G<--size:4.285530090332031-->
<--size:4.285530090332031-->230<--size:4.285530090332031-->
<--size:4.285530090332031-->9G<--size:4.285530090332031-->
<--size:4.285530090332031-->224<--size:4.285530090332031-->
<--size:4.285530090332031-->19G<--size:4.285530090332031-->
<--size:4.285530090332031-->217<--size:4.285530090332031-->
<--size:4.285530090332031-->18G<--size:4.285530090332031-->
<--size:4.285530090332031-->213<--size:4.285530090332031-->
<--size:4.285530090332031-->12G<--size:4.285530090332031-->
<--size:4.285530090332031-->207<--size:4.285530090332031-->
<--size:4.285530090332031-->12G<--size:4.285530090332031-->
<--size:4.285530090332031-->204<--size:4.285530090332031-->
<--size:4.285530090332031-->7G<--size:4.285530090332031-->
<--size:4.285530090332031-->200<--size:4.285530090332031-->
<--size:4.285530090332031-->10G<--size:4.285530090332031-->
<--size:4.285530090332031-->197<--size:4.285530090332031-->
<--size:4.285530090332031-->10G<--size:4.285530090332031-->
<--size:4.285530090332031-->192<--size:4.285530090332031-->
<--size:4.285530090332031-->9G<--size:4.285530090332031-->
<--size:5.7140398025512695-->(a) Sample rate and HBM usage<--size:5.7140398025512695-->
<--size:5.237819671630859-->p=1<--size:5.237819671630859-->
<--size:5.237819671630859-->p=2<--size:5.237819671630859-->
<--size:5.237819671630859-->p=4<--size:5.237819671630859-->
<--size:5.237819671630859-->p=8<--size:5.237819671630859-->
<--size:5.237819671630859-->p=16<--size:5.237819671630859-->
<--size:5.237819671630859-->p=32<--size:5.237819671630859-->
<--size:5.237870216369629-->t=1<--size:5.237870216369629-->
<--size:5.237870216369629-->t=2<--size:5.237870216369629-->
<--size:5.237870216369629-->t=4<--size:5.237870216369629-->
<--size:5.237870216369629-->t=8<--size:5.237870216369629-->
<--size:5.237870216369629-->t=16<--size:5.237870216369629-->
<--size:5.237870216369629-->t=32<--size:5.237870216369629-->
<--size:4.285530090332031-->455G<--size:4.285530090332031-->
<--size:4.285530090332031-->2T<--size:4.285530090332031-->
<--size:4.285530090332031-->455G<--size:4.285530090332031-->
<--size:4.285530090332031-->1T<--size:4.285530090332031-->
<--size:4.285530090332031-->455G<--size:4.285530090332031-->
<--size:4.285530090332031-->904G<--size:4.285530090332031-->
<--size:4.285530090332031-->455G<--size:4.285530090332031-->
<--size:4.285530090332031-->683G<--size:4.285530090332031-->
<--size:4.285530090332031-->455G<--size:4.285530090332031-->
<--size:4.285530090332031-->592G<--size:4.285530090332031-->
<--size:4.285530090332031-->455G<--size:4.285530090332031-->
<--size:4.285530090332031-->586G<--size:4.285530090332031-->
<--size:4.285530090332031-->450G<--size:4.285530090332031-->
<--size:4.285530090332031-->1T<--size:4.285530090332031-->
<--size:4.285530090332031-->151G<--size:4.285530090332031-->
<--size:4.285530090332031-->900G<--size:4.285530090332031-->
<--size:4.285530090332031-->226G<--size:4.285530090332031-->
<--size:4.285530090332031-->669G<--size:4.285530090332031-->
<--size:4.285530090332031-->226G<--size:4.285530090332031-->
<--size:4.285530090332031-->565G<--size:4.285530090332031-->
<--size:4.285530090332031-->226G<--size:4.285530090332031-->
<--size:4.285530090332031-->533G<--size:4.285530090332031-->
<--size:4.285530090332031-->450G<--size:4.285530090332031-->
<--size:4.285530090332031-->382G<--size:4.285530090332031-->
<--size:4.285530090332031-->593G<--size:4.285530090332031-->
<--size:4.285530090332031-->4T<--size:4.285530090332031-->
<--size:4.285530090332031-->593G<--size:4.285530090332031-->
<--size:4.285530090332031-->2T<--size:4.285530090332031-->
<--size:4.285530090332031-->149G<--size:4.285530090332031-->
<--size:4.285530090332031-->552G<--size:4.285530090332031-->
<--size:4.285530090332031-->149G<--size:4.285530090332031-->
<--size:4.285530090332031-->507G<--size:4.285530090332031-->
<--size:4.285530090332031-->593G<--size:4.285530090332031-->
<--size:4.285530090332031-->706G<--size:4.285530090332031-->
<--size:4.285530090332031-->2T<--size:4.285530090332031-->
<--size:4.285530090332031-->292G<--size:4.285530090332031-->
<--size:4.285530090332031-->327G<--size:4.285530090332031-->
<--size:4.285530090332031-->2T<--size:4.285530090332031-->
<--size:4.285530090332031-->131G<--size:4.285530090332031-->
<--size:4.285530090332031-->549G<--size:4.285530090332031-->
<--size:4.285530090332031-->131G<--size:4.285530090332031-->
<--size:4.285530090332031-->494G<--size:4.285530090332031-->
<--size:4.285530090332031-->148G<--size:4.285530090332031-->
<--size:4.285530090332031-->255G<--size:4.285530090332031-->
<--size:4.285530090332031-->586G<--size:4.285530090332031-->
<--size:4.285530090332031-->353G<--size:4.285530090332031-->
<--size:4.285530090332031-->843G<--size:4.285530090332031-->
<--size:4.285530090332031-->233G<--size:4.285530090332031-->
<--size:4.285530090332031-->118G<--size:4.285530090332031-->
<--size:4.285530090332031-->542G<--size:4.285530090332031-->
<--size:4.285530090332031-->118G<--size:4.285530090332031-->
<--size:4.285530090332031-->490G<--size:4.285530090332031-->
<--size:4.285530090332031-->118G<--size:4.285530090332031-->
<--size:4.285530090332031-->253G<--size:4.285530090332031-->
<--size:4.285530090332031-->118G<--size:4.285530090332031-->
<--size:4.285530090332031-->240G<--size:4.285530090332031-->
<--size:4.285530090332031-->432G<--size:4.285530090332031-->
<--size:4.285530090332031-->216G<--size:4.285530090332031-->
<--size:4.285530090332031-->555G<--size:4.285530090332031-->
<--size:4.285530090332031-->160G<--size:4.285530090332031-->
<--size:4.285530090332031-->96G<--size:4.285530090332031-->
<--size:4.285530090332031-->484G<--size:4.285530090332031-->
<--size:4.285530090332031-->96G<--size:4.285530090332031-->
<--size:4.285530090332031-->461G<--size:4.285530090332031-->
<--size:4.285530090332031-->95G<--size:4.285530090332031-->
<--size:4.285530090332031-->239G<--size:4.285530090332031-->
<--size:4.285530090332031-->274G<--size:4.285530090332031-->
<--size:4.285530090332031-->176G<--size:4.285530090332031-->
<--size:4.285530090332031-->274G<--size:4.285530090332031-->
<--size:4.285530090332031-->147G<--size:4.285530090332031-->
<--size:4.285530090332031-->381G<--size:4.285530090332031-->
<--size:4.285530090332031-->102G<--size:4.285530090332031-->
<--size:5.7140398025512695-->(b) Offloading bandwidth and usage<--size:5.7140398025512695-->
<--size:5.237819671630859-->p=1<--size:5.237819671630859-->
<--size:5.237819671630859-->p=2<--size:5.237819671630859-->
<--size:5.237819671630859-->p=4<--size:5.237819671630859-->
<--size:5.237819671630859-->p=8<--size:5.237819671630859-->
<--size:5.237819671630859-->p=16<--size:5.237819671630859-->
<--size:5.237819671630859-->p=32<--size:5.237819671630859-->
<--size:5.237870216369629-->t=1<--size:5.237870216369629-->
<--size:5.237870216369629-->t=2<--size:5.237870216369629-->
<--size:5.237870216369629-->t=4<--size:5.237870216369629-->
<--size:5.237870216369629-->t=8<--size:5.237870216369629-->
<--size:5.237870216369629-->t=16<--size:5.237870216369629-->
<--size:5.237870216369629-->t=32<--size:5.237870216369629-->
<--size:5.7140398025512695-->512 GiB @ 100 GB/s<--size:5.7140398025512695-->
<--size:4.285530090332031-->50<--size:4.285530090332031-->
<--size:4.285530090332031-->77G<--size:4.285530090332031-->
<--size:4.285530090332031-->61<--size:4.285530090332031-->
<--size:4.285530090332031-->72G<--size:4.285530090332031-->
<--size:4.285530090332031-->60<--size:4.285530090332031-->
<--size:4.285530090332031-->79G<--size:4.285530090332031-->
<--size:4.285530090332031-->54<--size:4.285530090332031-->
<--size:4.285530090332031-->72G<--size:4.285530090332031-->
<--size:4.285530090332031-->61<--size:4.285530090332031-->
<--size:4.285530090332031-->62G<--size:4.285530090332031-->
<--size:4.285530090332031-->116<--size:4.285530090332031-->
<--size:4.285530090332031-->43G<--size:4.285530090332031-->
<--size:4.285530090332031-->114<--size:4.285530090332031-->
<--size:4.285530090332031-->43G<--size:4.285530090332031-->
<--size:4.285530090332031-->163<--size:4.285530090332031-->
<--size:4.285530090332031-->79G<--size:4.285530090332031-->
<--size:4.285530090332031-->159<--size:4.285530090332031-->
<--size:4.285530090332031-->51G<--size:4.285530090332031-->
<--size:4.285530090332031-->85<--size:4.285530090332031-->
<--size:4.285530090332031-->59G<--size:4.285530090332031-->
<--size:4.285530090332031-->158<--size:4.285530090332031-->
<--size:4.285530090332031-->23G<--size:4.285530090332031-->
<--size:4.285530090332031-->208<--size:4.285530090332031-->
<--size:4.285530090332031-->23G<--size:4.285530090332031-->
<--size:4.285530090332031-->230<--size:4.285530090332031-->
<--size:4.285530090332031-->74G<--size:4.285530090332031-->
<--size:4.285530090332031-->217<--size:4.285530090332031-->
<--size:4.285530090332031-->45G<--size:4.285530090332031-->
<--size:4.285530090332031-->205<--size:4.285530090332031-->
<--size:4.285530090332031-->26G<--size:4.285530090332031-->
<--size:4.285530090332031-->183<--size:4.285530090332031-->
<--size:4.285530090332031-->15G<--size:4.285530090332031-->
<--size:4.285530090332031-->243<--size:4.285530090332031-->
<--size:4.285530090332031-->15G<--size:4.285530090332031-->
<--size:4.285530090332031-->239<--size:4.285530090332031-->
<--size:4.285530090332031-->15G<--size:4.285530090332031-->
<--size:4.285530090332031-->232<--size:4.285530090332031-->
<--size:4.285530090332031-->37G<--size:4.285530090332031-->
<--size:4.285530090332031-->226<--size:4.285530090332031-->
<--size:4.285530090332031-->23G<--size:4.285530090332031-->
<--size:4.285530090332031-->216<--size:4.285530090332031-->
<--size:4.285530090332031-->63G<--size:4.285530090332031-->
<--size:4.285530090332031-->239<--size:4.285530090332031-->
<--size:4.285530090332031-->12G<--size:4.285530090332031-->
<--size:4.285530090332031-->232<--size:4.285530090332031-->
<--size:4.285530090332031-->12G<--size:4.285530090332031-->
<--size:4.285530090332031-->227<--size:4.285530090332031-->
<--size:4.285530090332031-->8G<--size:4.285530090332031-->
<--size:4.285530090332031-->223<--size:4.285530090332031-->
<--size:4.285530090332031-->8G<--size:4.285530090332031-->
<--size:4.285530090332031-->217<--size:4.285530090332031-->
<--size:4.285530090332031-->57G<--size:4.285530090332031-->
<--size:4.285530090332031-->211<--size:4.285530090332031-->
<--size:4.285530090332031-->32G<--size:4.285530090332031-->
<--size:4.285530090332031-->213<--size:4.285530090332031-->
<--size:4.285530090332031-->12G<--size:4.285530090332031-->
<--size:4.285530090332031-->207<--size:4.285530090332031-->
<--size:4.285530090332031-->12G<--size:4.285530090332031-->
<--size:4.285530090332031-->204<--size:4.285530090332031-->
<--size:4.285530090332031-->7G<--size:4.285530090332031-->
<--size:4.285530090332031-->200<--size:4.285530090332031-->
<--size:4.285530090332031-->55G<--size:4.285530090332031-->
<--size:4.285530090332031-->197<--size:4.285530090332031-->
<--size:4.285530090332031-->30G<--size:4.285530090332031-->
<--size:4.285530090332031-->192<--size:4.285530090332031-->
<--size:4.285530090332031-->17G<--size:4.285530090332031-->
<--size:5.7140398025512695-->(c) Sample rate and HBM usage<--size:5.7140398025512695-->
<--size:5.237819671630859-->p=1<--size:5.237819671630859-->
<--size:5.237819671630859-->p=2<--size:5.237819671630859-->
<--size:5.237819671630859-->p=4<--size:5.237819671630859-->
<--size:5.237819671630859-->p=8<--size:5.237819671630859-->
<--size:5.237819671630859-->p=16<--size:5.237819671630859-->
<--size:5.237819671630859-->p=32<--size:5.237819671630859-->
<--size:5.237870216369629-->t=1<--size:5.237870216369629-->
<--size:5.237870216369629-->t=2<--size:5.237870216369629-->
<--size:5.237870216369629-->t=4<--size:5.237870216369629-->
<--size:5.237870216369629-->t=8<--size:5.237870216369629-->
<--size:5.237870216369629-->t=16<--size:5.237870216369629-->
<--size:5.237870216369629-->t=32<--size:5.237870216369629-->
<--size:4.285530090332031-->100G<--size:4.285530090332031-->
<--size:4.285530090332031-->472G<--size:4.285530090332031-->
<--size:4.285530090332031-->100G<--size:4.285530090332031-->
<--size:4.285530090332031-->346G<--size:4.285530090332031-->
<--size:4.285530090332031-->100G<--size:4.285530090332031-->
<--size:4.285530090332031-->120G<--size:4.285530090332031-->
<--size:4.285530090332031-->100G<--size:4.285530090332031-->
<--size:4.285530090332031-->190G<--size:4.285530090332031-->
<--size:4.285530090332031-->100G<--size:4.285530090332031-->
<--size:4.285530090332031-->472G<--size:4.285530090332031-->
<--size:4.285530090332031-->100G<--size:4.285530090332031-->
<--size:4.285530090332031-->362G<--size:4.285530090332031-->
<--size:4.285530090332031-->100G<--size:4.285530090332031-->
<--size:4.285530090332031-->249G<--size:4.285530090332031-->
<--size:4.285530090332031-->39G<--size:4.285530090332031-->
<--size:4.285530090332031-->96G<--size:4.285530090332031-->
<--size:4.285530090332031-->39G<--size:4.285530090332031-->
<--size:4.285530090332031-->106G<--size:4.285530090332031-->
<--size:4.285530090332031-->100G<--size:4.285530090332031-->
<--size:4.285530090332031-->472G<--size:4.285530090332031-->
<--size:4.285530090332031-->100G<--size:4.285530090332031-->
<--size:4.285530090332031-->363G<--size:4.285530090332031-->
<--size:4.285530090332031-->100G<--size:4.285530090332031-->
<--size:4.285530090332031-->247G<--size:4.285530090332031-->
<--size:4.285530090332031-->39G<--size:4.285530090332031-->
<--size:4.285530090332031-->134G<--size:4.285530090332031-->
<--size:4.285530090332031-->39G<--size:4.285530090332031-->
<--size:4.285530090332031-->142G<--size:4.285530090332031-->
<--size:4.285530090332031-->59G<--size:4.285530090332031-->
<--size:4.285530090332031-->54G<--size:4.285530090332031-->
<--size:4.285530090332031-->73G<--size:4.285530090332031-->
<--size:4.285530090332031-->362G<--size:4.285530090332031-->
<--size:4.285530090332031-->73G<--size:4.285530090332031-->
<--size:4.285530090332031-->247G<--size:4.285530090332031-->
<--size:4.285530090332031-->73G<--size:4.285530090332031-->
<--size:4.285530090332031-->189G<--size:4.285530090332031-->
<--size:4.285530090332031-->38G<--size:4.285530090332031-->
<--size:4.285530090332031-->69G<--size:4.285530090332031-->
<--size:4.285530090332031-->38G<--size:4.285530090332031-->
<--size:4.285530090332031-->72G<--size:4.285530090332031-->
<--size:4.285530090332031-->37G<--size:4.285530090332031-->
<--size:4.285530090332031-->51G<--size:4.285530090332031-->
<--size:4.285530090332031-->35G<--size:4.285530090332031-->
<--size:4.285530090332031-->245G<--size:4.285530090332031-->
<--size:4.285530090332031-->35G<--size:4.285530090332031-->
<--size:4.285530090332031-->188G<--size:4.285530090332031-->
<--size:4.285530090332031-->66G<--size:4.285530090332031-->
<--size:4.285530090332031-->97G<--size:4.285530090332031-->
<--size:4.285530090332031-->66G<--size:4.285530090332031-->
<--size:4.285530090332031-->83G<--size:4.285530090332031-->
<--size:4.285530090332031-->98G<--size:4.285530090332031-->
<--size:4.285530090332031-->53G<--size:4.285530090332031-->
<--size:4.285530090332031-->34G<--size:4.285530090332031-->
<--size:4.285530090332031-->38G<--size:4.285530090332031-->
<--size:4.285530090332031-->96G<--size:4.285530090332031-->
<--size:4.285530090332031-->484G<--size:4.285530090332031-->
<--size:4.285530090332031-->96G<--size:4.285530090332031-->
<--size:4.285530090332031-->461G<--size:4.285530090332031-->
<--size:4.285530090332031-->95G<--size:4.285530090332031-->
<--size:4.285530090332031-->239G<--size:4.285530090332031-->
<--size:4.285530090332031-->95G<--size:4.285530090332031-->
<--size:4.285530090332031-->124G<--size:4.285530090332031-->
<--size:4.285530090332031-->95G<--size:4.285530090332031-->
<--size:4.285530090332031-->121G<--size:4.285530090332031-->
<--size:4.285530090332031-->95G<--size:4.285530090332031-->
<--size:4.285530090332031-->87G<--size:4.285530090332031-->
<--size:5.7140398025512695-->(d) Offloading bandwidth and usage<--size:5.7140398025512695-->
<--size:8.571060180664062-->Megatron-1T training on 4096 H100 80 GiB GPUs with a<--size:8.571060180664062-->
<--size:8.571060180664062-->secondary memory available for tensor offloading<--size:8.571060180664062-->
<--size:9.055215835571289-->Figure 9: Tensor offloading study. 𝑡 and 𝑝 measure the de-<--size:9.055215835571289-->
<--size:9.055215835571289-->gree of TP and PP . (a) shows sample rate (top number) and<--size:9.055215835571289-->
<--size:8.965999603271484-->HBM consumption (bottom number) with ideal offload mem-<--size:8.965999603271484-->
<--size:9.055215835571289-->ory with infinite size and bandwidth. (b) shows offloading<--size:9.055215835571289-->
<--size:8.92556095123291-->bandwidth (top number) and capacity (bottom number) con-<--size:8.92556095123291-->
<--size:9.055215835571289-->sumption. (c) and (d) show the same for offloading mem-<--size:9.055215835571289-->
<--size:8.875886917114258-->ory with 512 GiB capacity and 100 GB/s bandwidth. Note that<--size:8.875886917114258-->
<--size:8.961515426635742-->with resource abundance Calculon finds strategies that con-<--size:8.961515426635742-->
<--size:8.875886917114258-->sume significantly more resources. With reasonable resource<--size:8.875886917114258-->
<--size:8.898500442504883-->constraints, Calculon finds execution strategies with similar<--size:8.898500442504883-->
<--size:8.965999603271484-->performance utilizing much less resources.<--size:8.965999603271484-->
<--size:8.997325897216797-->the offloading memory requirements could fit in 512 GiB capacity<--size:8.997325897216797-->
<--size:8.965999603271484-->at 100 GB/s.<--size:8.965999603271484-->
<--size:8.965999603271484-->Fig. 9(c,d) shows the same analysis where we set the offloading<--size:8.965999603271484-->
<--size:9.006256103515625-->memory to 512 GiB capacity at 100 GB/s . Comparing Fig. 9(a) and<--size:9.006256103515625-->
<--size:9.055215835571289-->(c) shows the performance slowdown using offloading memory<--size:9.055215835571289-->
<--size:8.979438781738281-->with restricted configuration is well within 5% for many cases. At<--size:8.979438781738281-->
<--size:9.019635200500488-->the same time, utilized capacity and bandwidth is much lower, as<--size:9.019635200500488-->
<--size:9.055215835571289-->seen comparing Fig. 9(b) and (d). For the best performing split-<--size:9.055215835571289-->
<--size:8.898500442504883-->parallelism configuration with ( 𝑡, 𝑝 ) = ( 8 , 2 ) the performance drop<--size:8.898500442504883-->
<--size:8.98391342163086-->is less than 3% while utilizing 56% of bandwidth and 45% capacity<--size:8.98391342163086-->
<--size:8.965999603271484-->of the system with an infinite offload memory.<--size:8.965999603271484-->
<--size:8.875886917114258-->The existence of an offloading memory significantly affects HBM<--size:8.875886917114258-->
<--size:8.92556095123291-->utilization and the choice of optimizations for the best performing<--size:8.92556095123291-->
<--size:9.055215835571289-->configuration. With offloading memory, the majority of configu-<--size:9.055215835571289-->
<--size:9.055215835571289-->rations, including the most performant ones, do not utilize more<--size:9.055215835571289-->
<--size:8.934563636779785-->than 20 GB of fast HBM . The abundance of slower memory makes<--size:8.934563636779785-->
<--size:9.055215835571289-->using higher DP possible while making PP less appealing. While<--size:9.055215835571289-->
<--size:9.055215835571289-->both TP and DP communication overlap are available, Calculon<--size:9.055215835571289-->
<--size:8.8894624710083-->suggests configurations with higher DP and a value of TP no more<--size:8.8894624710083-->
<--size:9.055215835571289-->than 16. TP up to 16 can achieve best performance with a single<--size:9.055215835571289-->
<--size:9.055215835571289-->dimensional distribution, as described in [ 29 ], since distributing<--size:9.055215835571289-->
<--size:5.906612873077393-->0 1024<--size:5.906612873077393-->
<--size:5.906612873077393-->2048<--size:5.906612873077393-->
<--size:5.906612873077393-->3072<--size:5.906612873077393-->
<--size:5.906612873077393-->4096<--size:5.906612873077393-->
<--size:5.906612873077393-->5120<--size:5.906612873077393-->
<--size:5.906612873077393-->6144<--size:5.906612873077393-->
<--size:5.906612873077393-->7168<--size:5.906612873077393-->
<--size:5.906612873077393-->8192<--size:5.906612873077393-->
<--size:5.906670093536377-->0.0<--size:5.906670093536377-->
<--size:5.906670093536377-->0.2<--size:5.906670093536377-->
<--size:5.906670093536377-->0.4<--size:5.906670093536377-->
<--size:5.906670093536377-->0.6<--size:5.906670093536377-->
<--size:5.906670093536377-->0.8<--size:5.906670093536377-->
<--size:5.906670093536377-->1.0<--size:5.906670093536377-->
<--size:6.443640232086182-->Relative scaling<--size:6.443640232086182-->
<--size:6.443640232086182-->GPT3 175B<--size:6.443640232086182-->
<--size:5.906612873077393-->0 1024<--size:5.906612873077393-->
<--size:5.906612873077393-->2048<--size:5.906612873077393-->
<--size:5.906612873077393-->3072<--size:5.906612873077393-->
<--size:5.906612873077393-->4096<--size:5.906612873077393-->
<--size:5.906612873077393-->5120<--size:5.906612873077393-->
<--size:5.906612873077393-->6144<--size:5.906612873077393-->
<--size:5.906612873077393-->7168<--size:5.906612873077393-->
<--size:5.906612873077393-->8192<--size:5.906612873077393-->
<--size:6.443640232086182-->System size<--size:6.443640232086182-->
<--size:5.906670093536377-->0.0<--size:5.906670093536377-->
<--size:5.906670093536377-->0.2<--size:5.906670093536377-->
<--size:5.906670093536377-->0.4<--size:5.906670093536377-->
<--size:5.906670093536377-->0.6<--size:5.906670093536377-->
<--size:5.906670093536377-->0.8<--size:5.906670093536377-->
<--size:5.906670093536377-->1.0<--size:5.906670093536377-->
<--size:6.443640232086182-->Turing-NLG 530B<--size:6.443640232086182-->
<--size:6.443640232086182-->Perfect scaling<--size:6.443640232086182-->
<--size:6.443640232086182-->Best performance<--size:6.443640232086182-->
<--size:5.906612873077393-->0 1024<--size:5.906612873077393-->
<--size:5.906612873077393-->2048<--size:5.906612873077393-->
<--size:5.906612873077393-->3072<--size:5.906612873077393-->
<--size:5.906612873077393-->4096<--size:5.906612873077393-->
<--size:5.906612873077393-->5120<--size:5.906612873077393-->
<--size:5.906612873077393-->6144<--size:5.906612873077393-->
<--size:5.906612873077393-->7168<--size:5.906612873077393-->
<--size:5.906612873077393-->8192<--size:5.906612873077393-->
<--size:5.906670093536377-->0.0<--size:5.906670093536377-->
<--size:5.906670093536377-->0.2<--size:5.906670093536377-->
<--size:5.906670093536377-->0.4<--size:5.906670093536377-->
<--size:5.906670093536377-->0.6<--size:5.906670093536377-->
<--size:5.906670093536377-->0.8<--size:5.906670093536377-->
<--size:5.906670093536377-->1.0<--size:5.906670093536377-->
<--size:6.443640232086182-->Megatron-1T<--size:6.443640232086182-->
<--size:9.665460586547852-->LLM training scalability (100 GB/s offloading)<--size:9.665460586547852-->
<--size:9.055215835571289-->Figure 10: Scaling LLM efficiency training on up to 8,192<--size:9.055215835571289-->
<--size:9.055215835571289-->GPUs with offloading. Offloading helps keep higher effi-<--size:9.055215835571289-->
<--size:8.965999603271484-->ciency for LLMs with higher parameter counts.<--size:8.965999603271484-->
<--size:7.388360023498535-->GPT3 175B<--size:7.388360023498535-->
<--size:5.805140018463135-->0 10<--size:5.805140018463135-->
<--size:5.805140018463135-->20<--size:5.805140018463135-->
<--size:5.805140018463135-->40<--size:5.805140018463135-->
<--size:5.805140018463135-->60<--size:5.805140018463135-->
<--size:7.388360023498535-->Turing-NLG 530B<--size:7.388360023498535-->
<--size:5.805140018463135-->0 10<--size:5.805140018463135-->
<--size:5.805140018463135-->20<--size:5.805140018463135-->
<--size:5.805140018463135-->40<--size:5.805140018463135-->
<--size:5.805140018463135-->60<--size:5.805140018463135-->
<--size:6.332880020141602-->Relative speedup, %<--size:6.332880020141602-->
<--size:7.388360023498535-->Megatron-1T<--size:7.388360023498535-->
<--size:5.805083751678467-->0<--size:5.805083751678467-->
<--size:5.805083751678467-->256<--size:5.805083751678467-->
<--size:5.805083751678467-->512<--size:5.805083751678467-->
<--size:5.805083751678467-->768<--size:5.805083751678467-->
<--size:5.805083751678467-->1024<--size:5.805083751678467-->
<--size:5.805083751678467-->1280<--size:5.805083751678467-->
<--size:5.805083751678467-->1536<--size:5.805083751678467-->
<--size:5.805083751678467-->1792<--size:5.805083751678467-->
<--size:5.805083751678467-->2048<--size:5.805083751678467-->
<--size:5.805083751678467-->2304<--size:5.805083751678467-->
<--size:5.805083751678467-->2560<--size:5.805083751678467-->
<--size:5.805083751678467-->2816<--size:5.805083751678467-->
<--size:5.805083751678467-->3072<--size:5.805083751678467-->
<--size:5.805083751678467-->3328<--size:5.805083751678467-->
<--size:5.805083751678467-->3584<--size:5.805083751678467-->
<--size:5.805083751678467-->3840<--size:5.805083751678467-->
<--size:5.805083751678467-->4096<--size:5.805083751678467-->
<--size:5.805083751678467-->4352<--size:5.805083751678467-->
<--size:5.805083751678467-->4608<--size:5.805083751678467-->
<--size:5.805083751678467-->4864<--size:5.805083751678467-->
<--size:5.805083751678467-->5120<--size:5.805083751678467-->
<--size:5.805083751678467-->5376<--size:5.805083751678467-->
<--size:5.805083751678467-->5632<--size:5.805083751678467-->
<--size:5.805083751678467-->5888<--size:5.805083751678467-->
<--size:5.805083751678467-->6144<--size:5.805083751678467-->
<--size:5.805083751678467-->6400<--size:5.805083751678467-->
<--size:5.805083751678467-->6656<--size:5.805083751678467-->
<--size:5.805083751678467-->6912<--size:5.805083751678467-->
<--size:5.805083751678467-->7168<--size:5.805083751678467-->
<--size:5.805083751678467-->7424<--size:5.805083751678467-->
<--size:5.805083751678467-->7680<--size:5.805083751678467-->
<--size:5.805083751678467-->7936<--size:5.805083751678467-->
<--size:5.805083751678467-->8192<--size:5.805083751678467-->
<--size:6.332880020141602-->System size<--size:6.332880020141602-->
<--size:5.805140018463135-->0 10<--size:5.805140018463135-->
<--size:5.805140018463135-->20<--size:5.805140018463135-->
<--size:5.805140018463135-->40<--size:5.805140018463135-->
<--size:5.805140018463135-->60<--size:5.805140018463135-->
<--size:9.499320030212402-->Relative performance improvement with offloading<--size:9.499320030212402-->
<--size:9.499320030212402-->(512 GiB @ 100 GB/s)<--size:9.499320030212402-->
<--size:9.055215835571289-->Figure 11: Relative speedup of LLM training on up to 8,192<--size:9.055215835571289-->
<--size:8.875886917114258-->GPUs due to available offloading. While providing moderate<--size:8.875886917114258-->
<--size:8.875886917114258-->performance improvement for large system sizes, offloading<--size:8.875886917114258-->
<--size:8.921056747436523-->can be instrumental for fine-tuning LLMs on small systems.<--size:8.921056747436523-->
<--size:8.965999603271484-->GEMM across more dimensions works better only with larger TP<--size:8.965999603271484-->
<--size:8.94355583190918-->partition sizes [ 35 ]. The reason for the relatively higher DP is that<--size:8.94355583190918-->
<--size:9.055215835571289-->it is distributed across processors in the slower network, which<--size:9.055215835571289-->
<--size:8.965999603271484-->requires fewer processors to fully saturate network bandwidth. In<--size:8.965999603271484-->
<--size:8.98391342163086-->the case of NVIDIA GPUs and NVLink, we consider allocating up<--size:8.98391342163086-->
<--size:9.055215835571289-->to 15 % of cores for running NCCL kernels, whereas driving the<--size:9.055215835571289-->
<--size:8.875886917114258-->slower network only requires 2 % of cores. Using 15 % of cores slows<--size:8.875886917114258-->
<--size:6.973999977111816-->SC ’23, November 12–17, 2023, Denver, CO, USA<--size:6.973999977111816-->
<--size:6.973999977111816-->Isaev, et al.<--size:6.973999977111816-->
<--size:8.965999603271484-->Table 3: Price and performance results for various systems and LLMs under a fixed budget of $125M.<--size:8.965999603271484-->
<--size:8.965999603271484-->GPT-3 175B<--size:8.965999603271484-->
<--size:8.965999603271484-->Turing-NLG 530B<--size:8.965999603271484-->
<--size:8.965999603271484-->Megatron 1T<--size:8.965999603271484-->
<--size:8.965999603271484-->HBM3<--size:8.965999603271484-->
<--size:8.965999603271484-->DDR5<--size:8.965999603271484-->
<--size:8.965999603271484-->Price<--size:8.965999603271484-->
<--size:8.965999603271484-->Max GPUs<--size:8.965999603271484-->
<--size:8.965999603271484-->GPUs<--size:8.965999603271484-->
<--size:8.965999603271484-->Perf<--size:8.965999603271484-->
<--size:8.965999603271484-->Perf/$M<--size:8.965999603271484-->
<--size:8.965999603271484-->GPUs<--size:8.965999603271484-->
<--size:8.965999603271484-->Perf<--size:8.965999603271484-->
<--size:8.965999603271484-->Perf/$M<--size:8.965999603271484-->
<--size:8.965999603271484-->GPUs<--size:8.965999603271484-->
<--size:8.965999603271484-->Perf<--size:8.965999603271484-->
<--size:8.965999603271484-->Perf/$M<--size:8.965999603271484-->
<--size:8.965999603271484-->20G<--size:8.965999603271484-->
<--size:8.965999603271484-->0 $22.2k<--size:8.965999603271484-->
<--size:8.965999603271484-->5616<--size:8.965999603271484-->
<--size:8.965999603271484-->5472<--size:8.965999603271484-->
<--size:8.965999603271484-->1117<--size:8.965999603271484-->
<--size:8.965999603271484-->917<--size:8.965999603271484-->
<--size:8.965999603271484-->5600<--size:8.965999603271484-->
<--size:8.965999603271484-->349<--size:8.965999603271484-->
<--size:8.965999603271484-->280<--size:8.965999603271484-->
<--size:8.965999603271484-->5120<--size:8.965999603271484-->
<--size:8.965999603271484-->93<--size:8.965999603271484-->
<--size:8.965999603271484-->82<--size:8.965999603271484-->
<--size:8.965999603271484-->40G<--size:8.965999603271484-->
<--size:8.965999603271484-->0 $25k<--size:8.965999603271484-->
<--size:8.965999603271484-->5000<--size:8.965999603271484-->
<--size:8.965999603271484-->4992<--size:8.965999603271484-->
<--size:8.965999603271484-->1154<--size:8.965999603271484-->
<--size:8.965999603271484-->924<--size:8.965999603271484-->
<--size:8.965999603271484-->4984<--size:8.965999603271484-->
<--size:8.965999603271484-->447<--size:8.965999603271484-->
<--size:8.965999603271484-->359<--size:8.965999603271484-->
<--size:8.965999603271484-->4864<--size:8.965999603271484-->
<--size:8.965999603271484-->219<--size:8.965999603271484-->
<--size:8.965999603271484-->180<--size:8.965999603271484-->
<--size:8.965999603271484-->80G<--size:8.965999603271484-->
<--size:8.965999603271484-->0 $30k<--size:8.965999603271484-->
<--size:8.965999603271484-->4160<--size:8.965999603271484-->
<--size:8.965999603271484-->3744<--size:8.965999603271484-->
<--size:8.965999603271484-->990<--size:8.965999603271484-->
<--size:8.965999603271484-->881<--size:8.965999603271484-->
<--size:8.965999603271484-->4080<--size:8.965999603271484-->
<--size:8.965999603271484-->390<--size:8.965999603271484-->
<--size:8.965999603271484-->318<--size:8.965999603271484-->
<--size:8.965999603271484-->4096<--size:8.965999603271484-->
<--size:8.965999603271484-->224<--size:8.965999603271484-->
<--size:8.965999603271484-->182<--size:8.965999603271484-->
<--size:8.965999603271484-->120G<--size:8.965999603271484-->
<--size:8.965999603271484-->0 $40k<--size:8.965999603271484-->
<--size:8.965999603271484-->3120<--size:8.965999603271484-->
<--size:8.965999603271484-->3120<--size:8.965999603271484-->
<--size:8.965999603271484-->861<--size:8.965999603271484-->
<--size:8.965999603271484-->690<--size:8.965999603271484-->
<--size:8.965999603271484-->3120<--size:8.965999603271484-->
<--size:8.965999603271484-->282<--size:8.965999603271484-->
<--size:8.965999603271484-->226<--size:8.965999603271484-->
<--size:8.965999603271484-->3072<--size:8.965999603271484-->
<--size:8.965999603271484-->172<--size:8.965999603271484-->
<--size:8.965999603271484-->140<--size:8.965999603271484-->
<--size:8.965999603271484-->20G<--size:8.965999603271484-->
<--size:8.965999603271484-->256G<--size:8.965999603271484-->
<--size:8.965999603271484-->$24.8k<--size:8.965999603271484-->
<--size:8.965999603271484-->5048<--size:8.965999603271484-->
<--size:8.965999603271484-->4680<--size:8.965999603271484-->
<--size:8.965999603271484-->1209<--size:8.965999603271484-->
<--size:8.965999603271484-->1044<--size:8.965999603271484-->
<--size:8.965999603271484-->5040<--size:8.965999603271484-->
<--size:8.965999603271484-->518<--size:8.965999603271484-->
<--size:8.965999603271484-->415<--size:8.965999603271484-->
<--size:8.965999603271484-->4896<--size:8.965999603271484-->
<--size:8.965999603271484-->283<--size:8.965999603271484-->
<--size:8.965999603271484-->234<--size:8.965999603271484-->
<--size:8.965999603271484-->40G<--size:8.965999603271484-->
<--size:8.965999603271484-->256G<--size:8.965999603271484-->
<--size:8.965999603271484-->$27.5k<--size:8.965999603271484-->
<--size:8.965999603271484-->4544<--size:8.965999603271484-->
<--size:8.965999603271484-->4536<--size:8.965999603271484-->
<--size:8.965999603271484-->1172<--size:8.965999603271484-->
<--size:8.965999603271484-->939<--size:8.965999603271484-->
<--size:8.965999603271484-->4320<--size:8.965999603271484-->
<--size:8.965999603271484-->386<--size:8.965999603271484-->
<--size:8.965999603271484-->325<--size:8.965999603271484-->
<--size:8.965999603271484-->4544<--size:8.965999603271484-->
<--size:8.965999603271484-->261<--size:8.965999603271484-->
<--size:8.965999603271484-->209<--size:8.965999603271484-->
<--size:8.965999603271484-->80G<--size:8.965999603271484-->
<--size:8.965999603271484-->256G<--size:8.965999603271484-->
<--size:8.965999603271484-->$32.5k<--size:8.965999603271484-->
<--size:8.965999603271484-->3840<--size:8.965999603271484-->
<--size:8.965999603271484-->3744<--size:8.965999603271484-->
<--size:8.965999603271484-->990<--size:8.965999603271484-->
<--size:8.965999603271484-->813<--size:8.965999603271484-->
<--size:8.965999603271484-->3840<--size:8.965999603271484-->
<--size:8.965999603271484-->403<--size:8.965999603271484-->
<--size:8.965999603271484-->323<--size:8.965999603271484-->
<--size:8.965999603271484-->3840<--size:8.965999603271484-->
<--size:8.965999603271484-->228<--size:8.965999603271484-->
<--size:8.965999603271484-->182<--size:8.965999603271484-->
<--size:8.965999603271484-->120G<--size:8.965999603271484-->
<--size:8.965999603271484-->256G<--size:8.965999603271484-->
<--size:8.965999603271484-->$42.5k<--size:8.965999603271484-->
<--size:8.965999603271484-->2936<--size:8.965999603271484-->
<--size:8.965999603271484-->2928<--size:8.965999603271484-->
<--size:8.965999603271484-->786<--size:8.965999603271484-->
<--size:8.965999603271484-->632<--size:8.965999603271484-->
<--size:8.965999603271484-->2880<--size:8.965999603271484-->
<--size:8.965999603271484-->306<--size:8.965999603271484-->
<--size:8.965999603271484-->250<--size:8.965999603271484-->
<--size:8.965999603271484-->2720<--size:8.965999603271484-->
<--size:8.965999603271484-->160<--size:8.965999603271484-->
<--size:8.965999603271484-->139<--size:8.965999603271484-->
<--size:8.965999603271484-->20G<--size:8.965999603271484-->
<--size:8.965999603271484-->512G<--size:8.965999603271484-->
<--size:8.965999603271484-->$32.2k<--size:8.965999603271484-->
<--size:8.965999603271484-->3872<--size:8.965999603271484-->
<--size:8.965999603271484-->3744<--size:8.965999603271484-->
<--size:8.965999603271484-->990<--size:8.965999603271484-->
<--size:8.965999603271484-->820<--size:8.965999603271484-->
<--size:8.965999603271484-->3864<--size:8.965999603271484-->
<--size:8.965999603271484-->405<--size:8.965999603271484-->
<--size:8.965999603271484-->325<--size:8.965999603271484-->
<--size:8.965999603271484-->3872<--size:8.965999603271484-->
<--size:8.965999603271484-->230<--size:8.965999603271484-->
<--size:8.965999603271484-->184<--size:8.965999603271484-->
<--size:8.965999603271484-->40G<--size:8.965999603271484-->
<--size:8.965999603271484-->512G<--size:8.965999603271484-->
<--size:8.965999603271484-->$35k<--size:8.965999603271484-->
<--size:8.965999603271484-->3568<--size:8.965999603271484-->
<--size:8.965999603271484-->3568<--size:8.965999603271484-->
<--size:8.965999603271484-->943<--size:8.965999603271484-->
<--size:8.965999603271484-->755<--size:8.965999603271484-->
<--size:8.965999603271484-->3360<--size:8.965999603271484-->
<--size:8.965999603271484-->355<--size:8.965999603271484-->
<--size:8.965999603271484-->302<--size:8.965999603271484-->
<--size:8.965999603271484-->3504<--size:8.965999603271484-->
<--size:8.965999603271484-->209<--size:8.965999603271484-->
<--size:8.965999603271484-->170<--size:8.965999603271484-->
<--size:8.965999603271484-->80G<--size:8.965999603271484-->
<--size:8.965999603271484-->512G<--size:8.965999603271484-->
<--size:8.965999603271484-->$40k<--size:8.965999603271484-->
<--size:8.965999603271484-->3120<--size:8.965999603271484-->
<--size:8.965999603271484-->3120<--size:8.965999603271484-->
<--size:8.965999603271484-->837<--size:8.965999603271484-->
<--size:8.965999603271484-->671<--size:8.965999603271484-->
<--size:8.965999603271484-->2880<--size:8.965999603271484-->
<--size:8.965999603271484-->306<--size:8.965999603271484-->
<--size:8.965999603271484-->266<--size:8.965999603271484-->
<--size:8.965999603271484-->3072<--size:8.965999603271484-->
<--size:8.965999603271484-->184<--size:8.965999603271484-->
<--size:8.965999603271484-->150<--size:8.965999603271484-->
<--size:8.965999603271484-->120G<--size:8.965999603271484-->
<--size:8.965999603271484-->512G<--size:8.965999603271484-->
<--size:8.965999603271484-->$50k<--size:8.965999603271484-->
<--size:8.965999603271484-->2496<--size:8.965999603271484-->
<--size:8.965999603271484-->2328<--size:8.965999603271484-->
<--size:8.965999603271484-->642<--size:8.965999603271484-->
<--size:8.965999603271484-->552<--size:8.965999603271484-->
<--size:8.965999603271484-->2496<--size:8.965999603271484-->
<--size:8.965999603271484-->266<--size:8.965999603271484-->
<--size:8.965999603271484-->213<--size:8.965999603271484-->
<--size:8.965999603271484-->2456<--size:8.965999603271484-->
<--size:8.965999603271484-->151<--size:8.965999603271484-->
<--size:8.965999603271484-->123<--size:8.965999603271484-->
<--size:8.965999603271484-->20G<--size:8.965999603271484-->
<--size:8.965999603271484-->1T<--size:8.965999603271484-->
<--size:8.965999603271484-->$42.2k<--size:8.965999603271484-->
<--size:8.965999603271484-->2952<--size:8.965999603271484-->
<--size:8.965999603271484-->2944<--size:8.965999603271484-->
<--size:8.965999603271484-->790<--size:8.965999603271484-->
<--size:8.965999603271484-->635<--size:8.965999603271484-->
<--size:8.965999603271484-->2880<--size:8.965999603271484-->
<--size:8.965999603271484-->306<--size:8.965999603271484-->
<--size:8.965999603271484-->251<--size:8.965999603271484-->
<--size:8.965999603271484-->2944<--size:8.965999603271484-->
<--size:8.965999603271484-->177<--size:8.965999603271484-->
<--size:8.965999603271484-->142<--size:8.965999603271484-->
<--size:8.965999603271484-->40G<--size:8.965999603271484-->
<--size:8.965999603271484-->1T<--size:8.965999603271484-->
<--size:8.965999603271484-->$45k<--size:8.965999603271484-->
<--size:8.965999603271484-->2776<--size:8.965999603271484-->
<--size:8.965999603271484-->2672<--size:8.965999603271484-->
<--size:8.965999603271484-->724<--size:8.965999603271484-->
<--size:8.965999603271484-->602<--size:8.965999603271484-->
<--size:8.965999603271484-->2760<--size:8.965999603271484-->
<--size:8.965999603271484-->293<--size:8.965999603271484-->
<--size:8.965999603271484-->236<--size:8.965999603271484-->
<--size:8.965999603271484-->2720<--size:8.965999603271484-->
<--size:8.965999603271484-->164<--size:8.965999603271484-->
<--size:8.965999603271484-->134<--size:8.965999603271484-->
<--size:8.965999603271484-->80G<--size:8.965999603271484-->
<--size:8.965999603271484-->1T<--size:8.965999603271484-->
<--size:8.965999603271484-->$50k<--size:8.965999603271484-->
<--size:8.965999603271484-->2496<--size:8.965999603271484-->
<--size:8.965999603271484-->2328<--size:8.965999603271484-->
<--size:8.965999603271484-->642<--size:8.965999603271484-->
<--size:8.965999603271484-->552<--size:8.965999603271484-->
<--size:8.965999603271484-->2496<--size:8.965999603271484-->
<--size:8.965999603271484-->266<--size:8.965999603271484-->
<--size:8.965999603271484-->213<--size:8.965999603271484-->
<--size:8.965999603271484-->2456<--size:8.965999603271484-->
<--size:8.965999603271484-->151<--size:8.965999603271484-->
<--size:8.965999603271484-->123<--size:8.965999603271484-->
<--size:8.965999603271484-->120G<--size:8.965999603271484-->
<--size:8.965999603271484-->1T<--size:8.965999603271484-->
<--size:8.965999603271484-->$60k<--size:8.965999603271484-->
<--size:8.965999603271484-->2080<--size:8.965999603271484-->
<--size:8.965999603271484-->2080<--size:8.965999603271484-->
<--size:8.965999603271484-->579<--size:8.965999603271484-->
<--size:8.965999603271484-->464<--size:8.965999603271484-->
<--size:8.965999603271484-->2016<--size:8.965999603271484-->
<--size:8.965999603271484-->226<--size:8.965999603271484-->
<--size:8.965999603271484-->187<--size:8.965999603271484-->
<--size:8.965999603271484-->2080<--size:8.965999603271484-->
<--size:8.965999603271484-->130<--size:8.965999603271484-->
<--size:8.965999603271484-->104<--size:8.965999603271484-->
<--size:8.880414009094238-->down concurrent GEMM and exposes some amount of the commu-<--size:8.880414009094238-->
<--size:8.92556095123291-->nication time. (If the process of GPU design compartmentalization<--size:8.92556095123291-->
<--size:9.050776481628418-->proceeds further, with blocks similar to TMA being able to move<--size:9.050776481628418-->
<--size:8.965999603271484-->data across the network, this finding is likely to change.)<--size:8.965999603271484-->
<--size:8.961515426635742-->The improvements to system scalability with 512 GB of offload-<--size:8.961515426635742-->
<--size:9.055215835571289-->ing memory at 100 GB/s for three LLMs appear in Fig. 10. The<--size:9.055215835571289-->
<--size:9.032994270324707-->impact on smaller models, such as GPT3-175B, is modest. But for<--size:9.032994270324707-->
<--size:8.875886917114258-->larger models, like Megatron-1T, the improvement is much more sig-<--size:8.875886917114258-->
<--size:8.961515426635742-->nificant. Moreover, consider models like Turing-NLG 530B, which<--size:8.961515426635742-->
<--size:8.903016090393066-->suffer from efficiency cliffs due to the difficulty of mapping to arbi-<--size:8.903016090393066-->
<--size:8.898500442504883-->trary system sizes. These cliffs are mitigiated with offload capacity,<--size:8.898500442504883-->
<--size:8.979438781738281-->thereby reducing the plateaus of suboptimal system sizes. Offload<--size:8.979438781738281-->
<--size:9.024090766906738-->memory capacity provides more options to partition an LLM and<--size:9.024090766906738-->
<--size:9.055215835571289-->may be regarded as a cost-effective method for “future-proofing”<--size:9.055215835571289-->
<--size:8.921056747436523-->larger system acquisitions in support of models that might require<--size:8.921056747436523-->
<--size:8.965999603271484-->more non-power-of-two parallelism configurations.<--size:8.965999603271484-->
<--size:8.965999603271484-->Calculon estimates typical performance gains for LLM training<--size:8.965999603271484-->
<--size:8.875886917114258-->due to offloading to lie between 10 % and 20 % for Turing-NLG 530B<--size:8.875886917114258-->
<--size:9.055215835571289-->and Megatron-1T LLMs per Fig. 11. Though these values seem<--size:9.055215835571289-->
<--size:9.055215835571289-->modest, the decision to use offloading or not should come after<--size:9.055215835571289-->
<--size:9.041889190673828-->analyzing total cost of ownership ( TCO ), as even small efficiency<--size:9.041889190673828-->
<--size:8.965999603271484-->gains can accumulate during long system use time.<--size:8.965999603271484-->
<--size:9.055215835571289-->Where offloading shines is providing better LLM training ef-<--size:9.055215835571289-->
<--size:9.055215835571289-->ficiency at smaller scales. It allows the training of Megatron-1T<--size:9.055215835571289-->
<--size:9.019635200500488-->with high efficiency on less than 256 GPUs, which is not possible<--size:9.019635200500488-->
<--size:9.055215835571289-->without offloading (indicated by “infinite speedup” in the figure).<--size:9.055215835571289-->
<--size:9.055215835571289-->While small systems have a low aggregate performance to train<--size:9.055215835571289-->
<--size:8.916550636291504-->foundational models, LLM fine-tuning, which is believed to be key<--size:8.916550636291504-->
<--size:8.875886917114258-->in successful LLM adaptation, requires several orders of magnitude<--size:8.875886917114258-->
<--size:9.055215835571289-->less compute. However, we still need to have enough aggregate<--size:9.055215835571289-->
<--size:9.055215835571289-->memory capacity to fit LLM . Typical system design would force<--size:9.055215835571289-->
<--size:8.965999603271484-->LLM users to allocate more GPUs, increase model parallelism (e.g.,<--size:8.965999603271484-->
<--size:8.965999603271484-->TP and/or PP ), and suffer some efficiency loss due to strong-scaling<--size:8.965999603271484-->
<--size:8.875886917114258-->issues. Offloading enables LLM training at smaller GPU counts, per-<--size:8.875886917114258-->
<--size:8.875886917114258-->mitting higher DP and providing better performance and efficiency.<--size:8.875886917114258-->
<--size:10.909000396728516-->7 OPTIMAL SYSTEM SEARCH<--size:10.909000396728516-->
<--size:9.055215835571289-->The key benefit of Calculon is its ability to search across a wide<--size:9.055215835571289-->
<--size:8.921056747436523-->range of software configurations quickly. When determining what<--size:8.921056747436523-->
<--size:9.055215835571289-->hardware to deploy for a data center, designers are often faced<--size:9.055215835571289-->
<--size:8.948049545288086-->with the challenge of choosing an optimal system under a specific<--size:8.948049545288086-->
<--size:9.015177726745605-->budget. In this section, we use Calculon to showcase its ability to<--size:9.015177726745605-->
<--size:9.055215835571289-->quickly choose from a selection of systems in order to optimize<--size:9.055215835571289-->
<--size:8.965999603271484-->performance per price.<--size:8.965999603271484-->
<--size:9.006256103515625-->For this study, we analyze the training of three different LLMs:<--size:9.006256103515625-->
<--size:8.875886917114258-->GPT3 175B [ 3 ], Turing-NLG 530B [ 45 ], and Megatron-1T [ 29 ] mod-<--size:8.875886917114258-->
<--size:8.875886917114258-->els on different systems. We base our system design around NVIDIA’s<--size:8.875886917114258-->
<--size:9.055215835571289-->H100 [ 31 ] GPU interconnected in clusters of 8 with NVLink and<--size:9.055215835571289-->
<--size:8.952540397644043-->between clusters with NDR InfiniBand. We use theoretical system<--size:8.952540397644043-->
<--size:9.037443161010742-->components and pricing as follows. An H100 without any HBM3<--size:9.037443161010742-->
<--size:9.055215835571289-->memory is $20k, which includes all the required infrastructure.<--size:9.055215835571289-->
<--size:9.055215835571289-->HBM3 memory costs $ 2,250 , $ 5,000 , $ 10,000 , and $ 20,000 for 20<--size:9.055215835571289-->
<--size:9.041889190673828-->GiB, 40 GiB, 80 GiB, and 120 GiB, respectively. All HBM versions<--size:9.041889190673828-->
<--size:8.961515426635742-->run at 3TB/s. To add a secondary DDR5 memory to an H100 costs<--size:8.961515426635742-->
<--size:8.934563636779785-->$2.5k, $10k, and $20k for 256 GiB, 512 GiB, and 1 TiB, respectively.<--size:8.934563636779785-->
<--size:8.965999603271484-->All secondary memory options run at 100 GB/s per direction.<--size:8.965999603271484-->
<--size:9.055215835571289-->For a price-aware performance analysis we cap the maximum<--size:9.055215835571289-->
<--size:9.015177726745605-->system cost to $125M. In this case, due to cost difference for each<--size:9.015177726745605-->
<--size:8.875886917114258-->design, the number of H100s deployed is different based on the total<--size:8.875886917114258-->
<--size:9.055215835571289-->system cost constraint. We present all system options across the<--size:9.055215835571289-->
<--size:8.875886917114258-->full permutation of HBM3 and DDR5 options resulting in 16 system<--size:8.875886917114258-->
<--size:8.875886917114258-->designs. Table 3 shows these options. The “Price” column shows the<--size:8.875886917114258-->
<--size:9.055215835571289-->price of each H100 with its options and the “Max GPUs” column<--size:9.055215835571289-->
<--size:8.965999603271484-->shows the maximum GPUs that can be afforded with $125M.<--size:8.965999603271484-->
<--size:8.875886917114258-->For each system and LLM analyzed, we sweep across the system<--size:8.875886917114258-->
<--size:8.875886917114258-->size space exhaustively finding the absolute best execution strategy.<--size:8.875886917114258-->
<--size:9.055215835571289-->We report the used number of GPUs in the “GPUs” column for<--size:9.055215835571289-->
<--size:6.973999977111816-->Calculon: a Methodology and Tool for High-Level Codesign of Systems and Large Language Models<--size:6.973999977111816-->
<--size:6.973999977111816-->SC ’23, November 12–17, 2023, Denver, CO, USA<--size:6.973999977111816-->
<--size:9.055215835571289-->each LLM , as well as the performance (i.e. sample rate) and the<--size:9.055215835571289-->
<--size:9.055215835571289-->performance divided by system cost. Neither the least nor most<--size:9.055215835571289-->
<--size:9.055215835571289-->expensive system design competes very well. The system design<--size:9.055215835571289-->
<--size:8.875886917114258-->with the highest performance (highlighted) is the top performer for<--size:8.875886917114258-->
<--size:8.92556095123291-->all three LLMs. With only 256 GiB of DDR5 memory it was able to<--size:8.92556095123291-->
<--size:8.875886917114258-->offload enough data to keep the active memory usage under 20 GiB<--size:8.875886917114258-->
<--size:9.055215835571289-->in HBM3. This cost saving trade-off allows it to have the second<--size:9.055215835571289-->
<--size:8.965999603271484-->highest system size and maintain a high compute efficiency.<--size:8.965999603271484-->
<--size:10.909000396728516-->8 CONCLUSION<--size:10.909000396728516-->
<--size:9.055215835571289-->In conclusion, the value of Calculon lies in its ability to analyze<--size:9.055215835571289-->
<--size:9.055215835571289-->a large codesign space of hardware and software configurations,<--size:9.055215835571289-->
<--size:8.907529830932617-->thereby making it possible to discover new and sometimes surpris-<--size:8.907529830932617-->
<--size:8.916550636291504-->ing configurations that might outperform the best-known state-of-<--size:8.916550636291504-->
<--size:8.965999603271484-->the-art.<--size:8.965999603271484-->
<--size:9.055215835571289-->To help illustrate this point, we summarize some of the best<--size:9.055215835571289-->
<--size:9.055215835571289-->strategies discovered by Calculon in Table 4, with the resulting<--size:9.055215835571289-->
<--size:8.907529830932617-->performance improvements over the state-of-the-art Fig. 12. There<--size:8.907529830932617-->
<--size:8.884939193725586-->is a 30 % performance improvement compared to previous State-of-<--size:8.884939193725586-->
<--size:8.875886917114258-->the-Art from optimization strategies alone, and potential 30 % more<--size:8.875886917114258-->
<--size:9.055215835571289-->performance per cost improvement from a better system design<--size:9.055215835571289-->
<--size:8.965999603271484-->choice based on our cost model.<--size:8.965999603271484-->
<--size:5.401607990264893-->Baseline<--size:5.401607990264893-->
<--size:5.401607990264893-->recompute<--size:5.401607990264893-->
<--size:5.401607990264893-->SOTA<--size:5.401607990264893-->
<--size:5.401607990264893-->seq par<--size:5.401607990264893-->
<--size:5.401607990264893-->Calculon<--size:5.401607990264893-->
<--size:5.401607990264893-->SW algos<--size:5.401607990264893-->
<--size:5.401607990264893-->Calculon<--size:5.401607990264893-->
<--size:5.401607990264893-->SW algos+<--size:5.401607990264893-->
<--size:5.401607990264893-->offload<--size:5.401607990264893-->
<--size:4.910599708557129-->0<--size:4.910599708557129-->
<--size:4.910599708557129-->5<--size:4.910599708557129-->
<--size:4.910599708557129-->10<--size:4.910599708557129-->
<--size:4.910599708557129-->15<--size:4.910599708557129-->
<--size:4.910599708557129-->20<--size:4.910599708557129-->
<--size:4.910599708557129-->25<--size:4.910599708557129-->
<--size:5.892719745635986-->Time, s<--size:5.892719745635986-->
<--size:5.892719745635986-->Batch time<--size:5.892719745635986-->
<--size:4.910599708557129-->FW pass<--size:4.910599708557129-->
<--size:4.910599708557129-->BW pass<--size:4.910599708557129-->
<--size:4.910599708557129-->Optim step<--size:4.910599708557129-->
<--size:4.910599708557129-->PP bubble<--size:4.910599708557129-->
<--size:4.910599708557129-->FW recompute<--size:4.910599708557129-->
<--size:4.910599708557129-->TP comm<--size:4.910599708557129-->
<--size:4.910599708557129-->PP comm<--size:4.910599708557129-->
<--size:4.910599708557129-->DP comm<--size:4.910599708557129-->
<--size:5.401607990264893-->Baseline<--size:5.401607990264893-->
<--size:5.401607990264893-->recompute<--size:5.401607990264893-->
<--size:5.401607990264893-->SOTA<--size:5.401607990264893-->
<--size:5.401607990264893-->seq par<--size:5.401607990264893-->
<--size:5.401607990264893-->Calculon<--size:5.401607990264893-->
<--size:5.401607990264893-->SW algos<--size:5.401607990264893-->
<--size:5.401607990264893-->Calculon<--size:5.401607990264893-->
<--size:5.401607990264893-->SW algos+<--size:5.401607990264893-->
<--size:5.401607990264893-->offload<--size:5.401607990264893-->
<--size:4.910599708557129-->0<--size:4.910599708557129-->
<--size:4.910599708557129-->10<--size:4.910599708557129-->
<--size:4.910599708557129-->20<--size:4.910599708557129-->
<--size:4.910599708557129-->30<--size:4.910599708557129-->
<--size:4.910599708557129-->40<--size:4.910599708557129-->
<--size:4.910599708557129-->50<--size:4.910599708557129-->
<--size:4.910599708557129-->60<--size:4.910599708557129-->
<--size:4.910599708557129-->70<--size:4.910599708557129-->
<--size:4.910599708557129-->80<--size:4.910599708557129-->
<--size:5.892719745635986-->Size, GB<--size:5.892719745635986-->
<--size:5.892719745635986-->HBM consumption<--size:5.892719745635986-->
<--size:4.910599708557129-->Weight<--size:4.910599708557129-->
<--size:4.910599708557129-->Activation<--size:4.910599708557129-->
<--size:4.910599708557129-->Weight gradients<--size:4.910599708557129-->
<--size:4.910599708557129-->Activation gradients<--size:4.910599708557129-->
<--size:4.910599708557129-->Optimizer space<--size:4.910599708557129-->
<--size:8.839079856872559-->Calculon results compared to State-of-the-Art<--size:8.839079856872559-->
<--size:9.055215835571289-->Figure 12: Performance comparison of optimal execution<--size:9.055215835571289-->
<--size:8.921056747436523-->strategies found by Calculon with available State-of-the-Art<--size:8.921056747436523-->
<--size:8.965999603271484-->optimized Megatron-1T implementations.<--size:8.965999603271484-->
<--size:9.055215835571289-->The specific combination of optimizations discovered by Cal-<--size:9.055215835571289-->
<--size:9.055215835571289-->culon, to the best of our knowledge, is not implemented in any<--size:9.055215835571289-->
<--size:9.055215835571289-->framework. The combination of activation fusion and optimizer<--size:9.055215835571289-->
<--size:9.055215835571289-->sharding, significantly reduces memory requirements. But then,<--size:9.055215835571289-->
<--size:9.001791954040527-->counter to conventional wisdom, Calculon decided to exploit this<--size:9.001791954040527-->
<--size:8.965999603271484-->reduction by reducing PP and increasing DP (which benefits from<--size:8.965999603271484-->
<--size:8.934563636779785-->more memory). Normally, doing so would incur great communica-<--size:8.934563636779785-->
<--size:9.032994270324707-->tion cost. But by combining a large microbatch size with high PP<--size:9.032994270324707-->
<--size:8.992857933044434-->interleaving and DP communication overlap, all the cost could be<--size:8.992857933044434-->
<--size:8.965999603271484-->hidden behind increased per-microbatch compute time.<--size:8.965999603271484-->
<--size:8.965999603271484-->Table 4: Comparison of parallelization strategies.<--size:8.965999603271484-->
<--size:8.965999603271484-->( 𝑡, 𝑝,𝑑 )<--size:8.965999603271484-->
<--size:8.965999603271484-->( 𝑡, 𝑝,𝑑 )<--size:8.965999603271484-->
<--size:8.965999603271484-->( 𝑡, 𝑝,𝑑 )<--size:8.965999603271484-->
<--bold--><--size:8.965999603271484-->m PP<--size:8.965999603271484-->
<--size:8.965999603271484-->int<--size:8.965999603271484-->
<--size:8.965999603271484-->added<--size:8.965999603271484-->
<--size:8.965999603271484-->optimizations<--size:8.965999603271484-->
<--size:8.965999603271484-->MFU<--size:8.965999603271484-->
<--size:8.965999603271484-->recompute<--size:8.965999603271484-->
<--size:8.965999603271484-->( 8 , 64 , 8 )<--size:8.965999603271484-->
<--size:8.965999603271484-->1 2<--size:8.965999603271484-->
<--size:8.965999603271484-->Full recompute<--size:8.965999603271484-->
<--size:8.965999603271484-->p2p RS+AG<--size:8.965999603271484-->
<--size:8.965999603271484-->36.67%<--size:8.965999603271484-->
<--size:8.965999603271484-->seq par<--size:8.965999603271484-->
<--size:8.965999603271484-->( 8 , 64 , 8 )<--size:8.965999603271484-->
<--size:8.965999603271484-->1 2<--size:8.965999603271484-->
<--size:8.965999603271484-->Attn recompute<--size:8.965999603271484-->
<--size:8.965999603271484-->RS+AG<--size:8.965999603271484-->
<--size:8.965999603271484-->RS redo for SP<--size:8.965999603271484-->
<--size:8.965999603271484-->49.61%<--size:8.965999603271484-->
<--size:8.965999603271484-->Calculon<--size:8.965999603271484-->
<--size:8.965999603271484-->SW optim<--size:8.965999603271484-->
<--size:8.965999603271484-->( 8 , 16 , 32 )<--size:8.965999603271484-->
<--size:8.965999603271484-->2 8<--size:8.965999603271484-->
<--size:8.965999603271484-->TP and DP overlap<--size:8.965999603271484-->
<--size:8.965999603271484-->optimizer sharding<--size:8.965999603271484-->
<--size:8.965999603271484-->fused activation<--size:8.965999603271484-->
<--size:8.965999603271484-->–RS redo for SP<--size:8.965999603271484-->
<--size:8.965999603271484-->70.96%<--size:8.965999603271484-->
<--size:8.965999603271484-->Calculon<--size:8.965999603271484-->
<--size:8.965999603271484-->SW optim<--size:8.965999603271484-->
<--size:8.965999603271484-->+ offload<--size:8.965999603271484-->
<--size:8.965999603271484-->( 8 , 1 , 512 )<--size:8.965999603271484-->
<--size:8.965999603271484-->6 1<--size:8.965999603271484-->
<--size:8.965999603271484-->offload memory<--size:8.965999603271484-->
<--size:8.965999603271484-->weight + activation<--size:8.965999603271484-->
<--size:8.965999603271484-->+ optimizer offload<--size:8.965999603271484-->
<--size:8.965999603271484-->76.71%<--size:8.965999603271484-->
<--size:8.875886917114258-->A second counterintuitive finding is that a high HBM capacity is<--size:8.875886917114258-->
<--size:8.875886917114258-->not necessary for efficient LLM training. In fact, we would prefer to<--size:8.875886917114258-->
<--size:8.8894624710083-->have a large amount of slower and cheaper memory and rearrange<--size:8.8894624710083-->
<--size:8.875886917114258-->execution to better fit the application’s implicit data reuse patterns.<--size:8.875886917114258-->
<--size:8.875886917114258-->Overall, our findings support a hypothesis that a complete search<--size:8.875886917114258-->
<--size:9.006256103515625-->over the joint space of hardware and software configurations can<--size:9.006256103515625-->
<--size:9.055215835571289-->identify optimal configurations that may be hard or impossible<--size:9.055215835571289-->
<--size:9.028543472290039-->to discover using manual heuristics. Indeed, any heuristic-driven<--size:9.028543472290039-->
<--size:8.875886917114258-->process risks biasing exploration of the search space toward certain<--size:8.875886917114258-->
<--size:9.041889190673828-->known types of configurations, thereby missing others. We hope<--size:9.041889190673828-->
<--size:8.875886917114258-->the quick analysis facilitated by Calculon makes broad explorations<--size:8.875886917114258-->
<--size:8.965999603271484-->of unconventional designs more feasible.<--size:8.965999603271484-->
<--size:8.880414009094238-->( Note: Per Section 1, the full model description has been omitted<--size:8.880414009094238-->
<--size:9.055215835571289-->due to space constraints, but all details are available at the open-<--size:9.055215835571289-->
<--size:8.965999603271484-->source Calculon repository.)<--size:8.965999603271484-->
<--size:10.909000396728516-->REFERENCES<--size:10.909000396728516-->
<--size:6.973999977111816-->[1] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li,<--size:6.973999977111816-->
<--size:7.012252330780029-->Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley,<--size:7.012252330780029-->
<--size:7.043394565582275-->and Yuxiong He. 2022. DeepSpeed-Inference: Enabling Efficient Inference of<--size:7.043394565582275-->
<--size:6.98097038269043-->Transformer Models at Unprecedented Scale. In Proceedings of the International<--size:6.98097038269043-->
<--size:7.043394565582275-->Conference on High Performance Computing, Networking, Storage and Analysis<--size:7.043394565582275-->
<--size:6.973999977111816-->(Dallas, Texas) (SC ’22) . IEEE Press, Article 46, 15 pages.<--size:6.973999977111816-->
<--size:6.973999977111816-->[2] Léon Bottou. 1991. Stochastic Gradient Learning in Neural Networks. In Proceed-<--size:6.973999977111816-->
<--size:6.903907775878906-->ings of Neuro-Nîmes 91 . EC2, Nimes, France. http://leon.bottou.org/papers/bottou-<--size:6.903907775878906-->
<--size:6.973999977111816-->91c<--size:6.973999977111816-->
<--size:6.973999977111816-->[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,<--size:6.973999977111816-->
<--size:6.9948906898498535-->Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda<--size:6.9948906898498535-->
<--size:6.903907775878906-->Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,<--size:6.903907775878906-->
<--size:7.043394565582275-->Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,<--size:7.043394565582275-->
<--size:6.903907775878906-->Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin<--size:6.903907775878906-->
<--size:7.043394565582275-->Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya<--size:7.043394565582275-->
<--size:6.903907775878906-->Sutskever, and Dario Amodei. 2020. Language Models Are Few-Shot Learners. In<--size:6.903907775878906-->
<--size:6.935536861419678-->Proceedings of the 34th International Conference on Neural Information Processing<--size:6.935536861419678-->
<--size:7.0261101722717285-->Systems (Vancouver, BC, Canada) (NIPS’20) . Curran Associates Inc., Red Hook,<--size:7.0261101722717285-->
<--size:6.973999977111816-->NY, USA, Article 159, 25 pages.<--size:6.973999977111816-->
<--size:6.973999977111816-->[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira<--size:6.973999977111816-->
<--size:6.903907775878906-->Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,<--size:6.903907775878906-->
<--size:7.043394565582275-->Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish<--size:7.043394565582275-->
<--size:7.029570579528809-->Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,<--size:7.029570579528809-->
<--size:6.98097038269043-->Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe<--size:6.98097038269043-->
<--size:6.910948753356934-->Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis,<--size:6.910948753356934-->
<--size:7.005312442779541-->Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex<--size:7.005312442779541-->
<--size:7.043394565582275-->Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,<--size:7.043394565582275-->
<--size:6.973999977111816-->SC ’23, November 12–17, 2023, Denver, CO, USA<--size:6.973999977111816-->
<--size:6.973999977111816-->Isaev, et al.<--size:6.973999977111816-->
<--size:6.98097038269043-->William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam,<--size:6.98097038269043-->
<--size:6.9705119132995605-->Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,<--size:6.9705119132995605-->
<--size:7.043394565582275-->Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam<--size:7.043394565582275-->
<--size:7.043394565582275-->McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large<--size:7.043394565582275-->
<--size:6.935536861419678-->Language Models Trained on Code. https://doi.org/10.48550/ARXIV.2107.03374<--size:6.935536861419678-->
<--size:6.973999977111816-->[5] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training Deep<--size:6.973999977111816-->
<--size:6.9705119132995605-->Nets with Sublinear Memory Cost. https://doi.org/10.48550/ARXIV.1604.06174<--size:6.9705119132995605-->
<--size:6.973999977111816-->[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav<--size:6.973999977111816-->
<--size:6.914466857910156-->Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas-<--size:6.914466857910156-->
<--size:6.903907775878906-->tian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,<--size:6.903907775878906-->
<--size:6.963531017303467-->Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,<--size:6.963531017303467-->
<--size:6.903907775878906-->Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,<--size:6.903907775878906-->
<--size:6.903907775878906-->Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay<--size:6.903907775878906-->
<--size:6.903907775878906-->Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin<--size:6.903907775878906-->
<--size:7.019184589385986-->Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek<--size:7.019184589385986-->
<--size:7.043394565582275-->Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani<--size:7.043394565582275-->
<--size:7.043394565582275-->Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana<--size:7.043394565582275-->
<--size:7.043394565582275-->Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr<--size:7.043394565582275-->
<--size:6.903907775878906-->Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz,<--size:6.903907775878906-->
<--size:7.043394565582275-->Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,<--size:7.043394565582275-->
<--size:6.903907775878906-->Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling<--size:6.903907775878906-->
<--size:6.973999977111816-->with Pathways. arXiv:2204.02311 [cs.CL]<--size:6.973999977111816-->
<--size:6.973999977111816-->[7] Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V.<--size:6.973999977111816-->
<--size:7.019184589385986-->Le, Mark Z. Mao, Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang,<--size:7.019184589385986-->
<--size:6.921497344970703-->and Andrew Y. Ng. 2012. Large Scale Distributed Deep Networks. In Proceedings<--size:6.921497344970703-->
<--size:7.043394565582275-->of the 25th International Conference on Neural Information Processing Systems -<--size:7.043394565582275-->
<--size:6.928520679473877-->Volume 1 (Lake Tahoe, Nevada) (NIPS’12) . Curran Associates Inc., Red Hook, NY,<--size:6.928520679473877-->
<--size:6.973999977111816-->USA, 1223–1231.<--size:6.973999977111816-->
<--size:6.973999977111816-->[8] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan<--size:6.973999977111816-->
<--size:6.98097038269043-->Wu, Guoping Long, Jun Yang, Lixue Xia, Lansong Diao, Xiaoyong Liu, and Wei<--size:6.98097038269043-->
<--size:7.043394565582275-->Lin. 2021. DAPPLE: A Pipelined Data Parallel Approach for Training Large<--size:7.043394565582275-->
<--size:7.0261101722717285-->Models. In Proceedings of the 26th ACM SIGPLAN Symposium on Principles and<--size:7.0261101722717285-->
<--size:7.008783340454102-->Practice of Parallel Programming (Virtual Event, Republic of Korea) (PPoPP ’21) .<--size:7.008783340454102-->
<--size:7.043394565582275-->Association for Computing Machinery, New York, NY, USA, 431–445.<--size:7.043394565582275-->
<--size:7.043394565582275-->https:<--size:7.043394565582275-->
<--size:6.973999977111816-->//doi.org/10.1145/3437801.3441593<--size:6.973999977111816-->
<--size:6.973999977111816-->[9] Google, LLC. 2022. XLA: Optimizing Compiler for TensorFlow. https://www.<--size:6.973999977111816-->
<--size:6.973999977111816-->tensorflow.org/xla<--size:6.973999977111816-->
<--size:6.973999977111816-->[10] Andreas Griewank and Andrea Walther. 2000. Algorithm 799: Revolve: An<--size:6.973999977111816-->
<--size:7.043394565582275-->Implementation of Checkpointing for the Reverse or Adjoint Mode of Com-<--size:7.043394565582275-->
<--size:7.043394565582275-->putational Differentiation. ACM Trans. Math. Softw. 26, 1 (mar 2000), 19–45.<--size:7.043394565582275-->
<--size:6.973999977111816-->https://doi.org/10.1145/347837.347846<--size:6.973999977111816-->
<--size:6.973999977111816-->[11] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,<--size:6.973999977111816-->
<--size:6.903907775878906-->Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes<--size:6.903907775878906-->
<--size:6.917983055114746-->Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den<--size:6.917983055114746-->
<--size:6.903907775878906-->Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich<--size:6.903907775878906-->
<--size:7.043394565582275-->Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training Compute-<--size:7.043394565582275-->
<--size:6.973999977111816-->Optimal Large Language Models. arXiv:2203.15556 [cs.CL]<--size:6.973999977111816-->
<--size:6.973999977111816-->[12] Chien-Chin Huang, Gu Jin, and Jinyang Li. 2020. SwapAdvisor: Pushing Deep<--size:6.973999977111816-->
<--size:7.043394565582275-->Learning Beyond the GPU Memory Limit via Smart Swapping. In Proceedings<--size:7.043394565582275-->
<--size:7.043394565582275-->of the Twenty-Fifth International Conference on Architectural Support for Pro-<--size:7.043394565582275-->
<--size:7.015718936920166-->gramming Languages and Operating Systems (Lausanne, Switzerland) (ASPLOS<--size:7.015718936920166-->
<--size:7.043394565582275-->’20) . Association for Computing Machinery, New York, NY, USA, 1341–1355.<--size:7.043394565582275-->
<--size:6.973999977111816-->https://doi.org/10.1145/3373376.3378530<--size:6.973999977111816-->
<--size:6.973999977111816-->[13] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao<--size:6.973999977111816-->
<--size:7.043394565582275-->Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng<--size:7.043394565582275-->
<--size:7.043394565582275-->Chen. 2019. GPipe: Efficient Training of Giant Neural Networks Using Pipeline<--size:7.043394565582275-->
<--size:6.973999977111816-->Parallelism . Curran Associates Inc., Red Hook, NY, USA.<--size:6.973999977111816-->
<--size:6.973999977111816-->[14] Mikhail Isaev, Nic McDonald, Jeffrey Young, and Richard Vuduc. 2022. ParaGraph:<--size:6.973999977111816-->
<--size:6.949548244476318-->An application-simulator interface and toolkit for hardware-software co-design.<--size:6.949548244476318-->
<--size:6.9948906898498535-->In 51th International Conference on Parallel Processing (Bordeaux, France) (ICPP<--size:6.9948906898498535-->
<--size:7.043394565582275-->2022) . Association for Computing Machinery, New York, NY, USA, Article 61,<--size:7.043394565582275-->
<--size:6.973999977111816-->10 pages. https://doi.org/10.1145/3545008.3545069<--size:6.973999977111816-->
<--size:6.973999977111816-->[15] Arpan Jain, Ammar Ahmad Awan, Asmaa M. Aljuhani, Jahanzeb Maqbool<--size:6.973999977111816-->
<--size:7.043394565582275-->Hashmi, Quentin G. Anthony, Hari Subramoni, Dhableswar K. Panda, Raghu<--size:7.043394565582275-->
<--size:6.907429218292236-->Machiraju, and Anil Parwani. 2020. GEMS: GPU-enabled memory-Aware Model-<--size:6.907429218292236-->
<--size:7.043394565582275-->Parallelism system for Distributed DNN Training. In Proceedings of the Inter-<--size:7.043394565582275-->
<--size:7.043394565582275-->national Conference for High Performance Computing, Networking, Storage and<--size:7.043394565582275-->
<--size:6.973999977111816-->Analysis (Atlanta, Georgia) (SC ’20) . IEEE Press, Article 45, 15 pages.<--size:6.973999977111816-->
<--size:6.973999977111816-->[16] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Joseph<--size:6.973999977111816-->
<--size:6.928520679473877-->Gonzalez, Kurt Keutzer, and Ion Stoica. 2020. Checkmate: Breaking the Memory<--size:6.928520679473877-->
<--size:6.914466857910156-->Wall with Optimal Tensor Rematerialization. In Proceedings of Machine Learning<--size:6.914466857910156-->
<--size:6.973999977111816-->and Systems 2020 . 497–511.<--size:6.973999977111816-->
<--size:6.973999977111816-->[17] Norman P. Jouppi, Doe Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James<--size:6.973999977111816-->
<--size:6.998366355895996-->Laudon, Cliff Young, and David Patterson. 2020. A Domain-Specific Supercom-<--size:6.998366355895996-->
<--size:6.903907775878906-->puter for Training Deep Neural Networks. Commun. ACM 63, 7 (jun 2020), 67–78.<--size:6.903907775878906-->
<--size:6.973999977111816-->https://doi.org/10.1145/3360307<--size:6.973999977111816-->
<--size:6.973999977111816-->[18] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,<--size:6.973999977111816-->
<--size:6.903907775878906-->Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle,<--size:6.903907775878906-->
<--size:6.991413116455078-->Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt<--size:6.991413116455078-->
<--size:7.043394565582275-->Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati,<--size:7.043394565582275-->
<--size:7.043394565582275-->William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu,<--size:7.043394565582275-->
<--size:7.043394565582275-->Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander<--size:7.043394565582275-->
<--size:7.043394565582275-->Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve<--size:7.043394565582275-->
<--size:7.043394565582275-->Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle<--size:7.043394565582275-->
<--size:6.903907775878906-->Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran<--size:6.903907775878906-->
<--size:6.903907775878906-->Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie,<--size:6.903907775878906-->
<--size:6.963531017303467-->Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross,<--size:6.963531017303467-->
<--size:6.9425458908081055-->Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham,<--size:6.9425458908081055-->
<--size:7.043394565582275-->Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo<--size:7.043394565582275-->
<--size:6.96702241897583-->Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang,<--size:6.96702241897583-->
<--size:6.9530463218688965-->Eric Wilcox, and Doe Hyun Yoon. 2017. In-Datacenter Performance Analysis of<--size:6.9530463218688965-->
<--size:6.946047782897949-->a Tensor Processing Unit. SIGARCH Comput. Archit. News 45, 2 (jun 2017), 1–12.<--size:6.946047782897949-->
<--size:6.973999977111816-->https://doi.org/10.1145/3140659.3080246<--size:6.973999977111816-->
<--size:6.973999977111816-->[19] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-<--size:6.973999977111816-->
<--size:6.949548244476318-->mization. In 3rd International Conference on Learning Representations, ICLR 2015,<--size:6.949548244476318-->
<--size:6.9320292472839355-->San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , Yoshua Bengio<--size:6.9320292472839355-->
<--size:6.973999977111816-->and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6980<--size:6.973999977111816-->
<--size:6.973999977111816-->[20] Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael An-<--size:6.973999977111816-->
<--size:7.012252330780029-->dersch, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Reducing Activation<--size:7.012252330780029-->
<--size:6.9774861335754395-->Recomputation in Large Transformer Models. https://doi.org/10.48550/ARXIV.<--size:6.9774861335754395-->
<--size:6.973999977111816-->2205.05198<--size:6.973999977111816-->
<--size:6.973999977111816-->[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-<--size:6.973999977111816-->
<--size:7.043394565582275-->sification with Deep Convolutional Neural Networks. In Advances in Neural<--size:7.043394565582275-->
<--size:6.949548244476318-->Information Processing Systems , F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-<--size:6.949548244476318-->
<--size:7.043394565582275-->berger (Eds.), Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/<--size:7.043394565582275-->
<--size:6.973999977111816-->paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf<--size:6.973999977111816-->
<--size:6.973999977111816-->[22] Sameer Kumar, Victor Bitorff, Dehao Chen, Chiachen Chou, Blake A. Hecht-<--size:6.973999977111816-->
<--size:7.005312442779541-->man, HyoukJoong Lee, Naveen Kumar, Peter Mattson, Shibo Wang, Tao Wang,<--size:7.005312442779541-->
<--size:7.043394565582275-->Yuanzhong Xu, and Zongwei Zhou. 2019. Scale MLPerf-0.6 models on Google<--size:7.043394565582275-->
<--size:7.043394565582275-->TPU-v3 Pods. CoRR abs/1909.09756 (2019). arXiv:1909.09756 http://arxiv.org/<--size:7.043394565582275-->
<--size:6.973999977111816-->abs/1909.09756<--size:6.973999977111816-->
<--size:6.973999977111816-->[23] Sunwoo Lee, Dipendra Jha, Ankit Agrawal, Alok Choudhary, and Wei-keng Liao.<--size:6.973999977111816-->
<--size:7.015718936920166-->2017. Parallel Deep Convolutional Neural Network Training by Exploiting the<--size:7.015718936920166-->
<--size:6.903907775878906-->Overlapping of Computation and Communication. In 2017 IEEE 24th International<--size:6.903907775878906-->
<--size:6.95654296875-->Conference on High Performance Computing (HiPC) . 183–192. https://doi.org/10.<--size:6.95654296875-->
<--size:6.973999977111816-->1109/HiPC.2017.00030<--size:6.973999977111816-->
<--size:6.973999977111816-->[24] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat,<--size:6.973999977111816-->
<--size:6.903907775878906-->Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021. {GS}hard:<--size:6.903907775878906-->
<--size:7.0261101722717285-->Scaling Giant Models with Conditional Computation and Automatic Sharding.<--size:7.0261101722717285-->
<--size:6.935536861419678-->In International Conference on Learning Representations . https://openreview.net/<--size:6.935536861419678-->
<--size:6.973999977111816-->forum?id=qrwe7XHTmYb<--size:6.973999977111816-->
<--size:6.973999977111816-->[25] Wenyan Lu, Guihai Yan, Jiajun Li, Shijun Gong, Yinhe Han, and Xiaowei Li. 2017.<--size:6.973999977111816-->
<--size:6.903907775878906-->FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural<--size:6.903907775878906-->
<--size:6.946047782897949-->Networks. In 2017 IEEE International Symposium on High Performance Computer<--size:6.946047782897949-->
<--size:6.973999977111816-->Architecture (HPCA) . 553–564. https://doi.org/10.1109/HPCA.2017.29<--size:6.973999977111816-->
<--size:6.973999977111816-->[26] Nenad Markuš. 2018. Fusing batch normalization and convolution in runtime.<--size:6.973999977111816-->
<--size:6.973999977111816-->https://nenadmarkus.com/p/fusing-batchnorm-and-conv/<--size:6.973999977111816-->
<--size:6.973999977111816-->[27] Nic McDonald, Adriana Flores, Al Davis, Mikhail Isaev, John Kim, and Doug Gib-<--size:6.973999977111816-->
<--size:6.903907775878906-->son. 2018. SuperSim: Extensible Flit-Level Simulation of Large-Scale Interconnec-<--size:6.903907775878906-->
<--size:6.928520679473877-->tion Networks. In 2018 IEEE International Symposium on Performance Analysis of<--size:6.928520679473877-->
<--size:6.903907775878906-->Systems and Software (ISPASS) . 87–98. https://doi.org/10.1109/ISPASS.2018.00017<--size:6.903907775878906-->
<--size:6.973999977111816-->[28] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R.<--size:6.973999977111816-->
<--size:7.043394565582275-->Devanur, Gregory R. Ganger, Phillip B. Gibbons, and Matei Zaharia. 2019.<--size:7.043394565582275-->
<--size:7.005312442779541-->PipeDream: Generalized Pipeline Parallelism for DNN Training. In Proceedings<--size:7.005312442779541-->
<--size:6.903907775878906-->of the 27th ACM Symposium on Operating Systems Principles (Huntsville, Ontario,<--size:6.903907775878906-->
<--size:6.9425458908081055-->Canada) (SOSP ’19) . Association for Computing Machinery, New York, NY, USA,<--size:6.9425458908081055-->
<--size:6.973999977111816-->1–15. https://doi.org/10.1145/3341301.3359646<--size:6.973999977111816-->
<--size:6.973999977111816-->[29] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley,<--size:6.973999977111816-->
<--size:7.043394565582275-->Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,<--size:7.043394565582275-->
<--size:7.039941310882568-->Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. 2021.<--size:7.039941310882568-->
<--size:6.903907775878906-->Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-<--size:6.903907775878906-->
<--size:7.043394565582275-->LM. In Proceedings of the International Conference for High Performance Com-<--size:7.043394565582275-->
<--size:7.043394565582275-->puting, Networking, Storage and Analysis (St. Louis, Missouri) (SC ’21) . Asso-<--size:7.043394565582275-->
<--size:7.043394565582275-->ciation for Computing Machinery, New York, NY, USA, Article 58, 15 pages.<--size:7.043394565582275-->
<--size:6.973999977111816-->https://doi.org/10.1145/3458817.3476209<--size:6.973999977111816-->
<--size:6.973999977111816-->[30] NVIDIA. 2020. NVIDIA A100 Tensor Core GPU Architecture. https://resources.<--size:6.973999977111816-->
<--size:6.973999977111816-->nvidia.com/en-us-tensor-core<--size:6.973999977111816-->
<--size:6.973999977111816-->[31] NVIDIA.<--size:6.973999977111816-->
<--size:7.043394565582275-->2022.<--size:7.043394565582275-->
<--size:7.043394565582275-->NVIDIA<--size:7.043394565582275-->
<--size:7.043394565582275-->H100<--size:7.043394565582275-->
<--size:7.043394565582275-->Tensor<--size:7.043394565582275-->
<--size:7.043394565582275-->Core<--size:7.043394565582275-->
<--size:7.043394565582275-->GPU<--size:7.043394565582275-->
<--size:7.043394565582275-->Architecture.<--size:7.043394565582275-->
<--size:7.043394565582275-->https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-<--size:7.043394565582275-->
<--size:6.973999977111816-->ampere-architecture-whitepaper.pdf<--size:6.973999977111816-->
<--size:6.973999977111816-->[32] NVIDIA. 2022. NVLink and NVSwitch.<--size:6.973999977111816-->
<--size:7.043394565582275-->https://www.nvidia.com/en-us/data-<--size:7.043394565582275-->
<--size:6.973999977111816-->center/nvlink/<--size:6.973999977111816-->
<--size:6.973999977111816-->[33] NVIDIA. 2023. NVIDIA Deep Learning Performance. https://docs.nvidia.com/<--size:6.973999977111816-->
<--size:6.973999977111816-->deeplearning/performance/dl-performance-matrix-multiplication/index.html<--size:6.973999977111816-->
<--size:6.973999977111816-->[34] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,<--size:6.973999977111816-->
<--size:6.903907775878906-->Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon Emissions<--size:6.903907775878906-->
<--size:6.973999977111816-->Calculon: a Methodology and Tool for High-Level Codesign of Systems and Large Language Models<--size:6.973999977111816-->
<--size:6.973999977111816-->SC ’23, November 12–17, 2023, Denver, CO, USA<--size:6.973999977111816-->
<--size:6.903907775878906-->and Large Neural Network Training. https://doi.org/10.48550/ARXIV.2104.10350<--size:6.903907775878906-->
<--size:6.973999977111816-->[35] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Brad-<--size:6.973999977111816-->
<--size:7.019184589385986-->bury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff<--size:7.019184589385986-->
<--size:6.925009727478027-->Dean. 2022. Efficiently Scaling Transformer Inference. arXiv:2211.05102 [cs.LG]<--size:6.925009727478027-->
<--size:6.973999977111816-->[36] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya<--size:6.973999977111816-->
<--size:6.903907775878906-->Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).<--size:6.903907775878906-->
<--size:6.973999977111816-->[37] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. ZeRO:<--size:6.973999977111816-->
<--size:6.98097038269043-->Memory Optimizations toward Training Trillion Parameter Models. In Proceed-<--size:6.98097038269043-->
<--size:6.910948753356934-->ings of the International Conference for High Performance Computing, Networking,<--size:6.910948753356934-->
<--size:6.903907775878906-->Storage and Analysis (Atlanta, Georgia) (SC ’20) . IEEE Press, Article 20, 16 pages.<--size:6.903907775878906-->
<--size:6.973999977111816-->[38] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong<--size:6.973999977111816-->
<--size:7.043394565582275-->He. 2021. ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale<--size:7.043394565582275-->
<--size:7.043394565582275-->Deep Learning. In Proceedings of the International Conference for High Perfor-<--size:7.043394565582275-->
<--size:6.903907775878906-->mance Computing, Networking, Storage and Analysis (St. Louis, Missouri) (SC ’21) .<--size:6.903907775878906-->
<--size:6.925009727478027-->Association for Computing Machinery, New York, NY, USA, Article 59, 14 pages.<--size:6.925009727478027-->
<--size:6.973999977111816-->https://doi.org/10.1145/3458817.3476205<--size:6.973999977111816-->
<--size:6.973999977111816-->[39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.<--size:6.973999977111816-->
<--size:6.903907775878906-->2022. Hierarchical Text-Conditional Image Generation with CLIP Latents. https:<--size:6.903907775878906-->
<--size:6.973999977111816-->//doi.org/10.48550/ARXIV.2204.06125<--size:6.973999977111816-->
<--size:6.973999977111816-->[40] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Rad-<--size:6.973999977111816-->
<--size:6.9320292472839355-->ford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation.<--size:6.9320292472839355-->
<--size:6.925009727478027-->In Proceedings of the 38th International Conference on Machine Learning (Proceed-<--size:6.925009727478027-->
<--size:6.903907775878906-->ings of Machine Learning Research, Vol. 139) , Marina Meila and Tong Zhang (Eds.).<--size:6.903907775878906-->
<--size:6.973999977111816-->PMLR, 8821–8831. https://proceedings.mlr.press/v139/ramesh21a.html<--size:6.973999977111816-->
<--size:6.973999977111816-->[41] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deep-<--size:6.973999977111816-->
<--size:6.939042568206787-->Speed: System Optimizations Enable Training Deep Learning Models with Over<--size:6.939042568206787-->
<--size:7.043394565582275-->100 Billion Parameters. In Proceedings of the 26th ACM SIGKDD International<--size:7.043394565582275-->
<--size:7.0364861488342285-->Conference on Knowledge Discovery and Data Mining (Virtual Event, CA, USA)<--size:7.0364861488342285-->
<--size:6.903907775878906-->(KDD ’20) . Association for Computing Machinery, New York, NY, USA, 3505–3506.<--size:6.903907775878906-->
<--size:6.973999977111816-->https://doi.org/10.1145/3394486.3406703<--size:6.973999977111816-->
<--size:6.973999977111816-->[42] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase,<--size:6.973999977111816-->
<--size:6.963531017303467-->Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-Offload:<--size:6.963531017303467-->
<--size:7.008783340454102-->Democratizing Billion-Scale Model Training. In 2021 USENIX Annual Technical<--size:7.008783340454102-->
<--size:6.935536861419678-->Conference, USENIX ATC 2021, July 14-16, 2021 , Irina Calciu and Geoff Kuenning<--size:6.935536861419678-->
<--size:6.903907775878906-->(Eds.). USENIX Association, 551–564. https://www.usenix.org/conference/atc21/<--size:6.903907775878906-->
<--size:6.973999977111816-->presentation/ren-jie<--size:6.973999977111816-->
<--size:6.973999977111816-->[43] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Pen-<--size:6.973999977111816-->
<--size:7.043394565582275-->porn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff<--size:7.043394565582275-->
<--size:6.903907775878906-->Young, Ryan Sepassi, and Blake Hechtman. 2018. Mesh-TensorFlow: Deep Learn-<--size:6.903907775878906-->
<--size:6.973999977111816-->ing for Supercomputers. arXiv:1811.02084 [cs.LG]<--size:6.973999977111816-->
<--size:6.973999977111816-->[44] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,<--size:6.973999977111816-->
<--size:7.043394565582275-->and Bryan Catanzaro. 2019. Megatron-LM: Training Multi-Billion Parameter<--size:7.043394565582275-->
<--size:7.043394565582275-->Language Models Using Model Parallelism.<--size:7.043394565582275-->
<--size:7.043394565582275-->https://doi.org/10.48550/ARXIV.<--size:7.043394565582275-->
<--size:6.973999977111816-->1909.08053<--size:6.973999977111816-->
<--size:6.973999977111816-->[45] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam<--size:6.973999977111816-->
<--size:7.043394565582275-->Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,<--size:7.043394565582275-->
<--size:7.043394565582275-->Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie<--size:7.043394565582275-->
<--size:6.903907775878906-->Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh<--size:6.903907775878906-->
<--size:7.043394565582275-->Tiwary, and Bryan Catanzaro. 2022. Using DeepSpeed and Megatron to Train<--size:7.043394565582275-->
<--size:6.949548244476318-->Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. https:<--size:6.949548244476318-->
<--size:6.973999977111816-->//doi.org/10.48550/ARXIV.2201.11990<--size:6.973999977111816-->
<--size:6.973999977111816-->[46] Xiaoyang Sun, Wei Wang, Shenghao Qiu, Renyu Yang, Songfang Huang, Jie Xu,<--size:6.973999977111816-->
<--size:7.043394565582275-->and Zheng Wang. 2022. StrongHold: Fast and Affordable Billion-Scale Deep<--size:7.043394565582275-->
<--size:6.960038185119629-->Learning Model Training. In Proceedings of the International Conference on High<--size:6.960038185119629-->
<--size:6.903907775878906-->Performance Computing, Networking, Storage and Analysis (Dallas, Texas) (SC ’22) .<--size:6.903907775878906-->
<--size:6.973999977111816-->IEEE Press, Article 71, 17 pages.<--size:6.973999977111816-->
<--size:6.973999977111816-->[47] Jakub Tarnawski, Amar Phanishayee, Nikhil Devanur, Divya Mahajan, and<--size:6.973999977111816-->
<--size:7.043394565582275-->Fanny Nina Paravecino. 2020. Efficient Algorithms for Device Placement of<--size:7.043394565582275-->
<--size:7.043394565582275-->DNN Graph Operators. In Proceedings of the 34th International Conference on<--size:7.043394565582275-->
<--size:6.903907775878906-->Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS’20) . Curran<--size:6.903907775878906-->
<--size:6.973999977111816-->Associates Inc., Red Hook, NY, USA, Article 1296, 13 pages.<--size:6.973999977111816-->
<--size:6.973999977111816-->[48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne<--size:6.973999977111816-->
<--size:7.043394565582275-->Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,<--size:7.043394565582275-->
<--size:7.043394565582275-->Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guil-<--size:7.043394565582275-->
<--size:6.98097038269043-->laume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.<--size:6.98097038269043-->
<--size:6.973999977111816-->arXiv:2302.13971 [cs.CL]<--size:6.973999977111816-->
<--size:6.973999977111816-->[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,<--size:6.973999977111816-->
<--size:7.043394565582275-->Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All<--size:7.043394565582275-->
<--size:6.973999977111816-->You Need (NIPS’17) . Curran Associates Inc., Red Hook, NY, USA, 6000–6010.<--size:6.973999977111816-->
<--size:6.973999977111816-->[50] Oreste Villa, Daniel Lustig, Zi Yan, Evgeny Bolotin, Yaosheng Fu, Niladrish<--size:6.973999977111816-->
<--size:7.043394565582275-->Chatterjee, Nan Jiang, and David Nellans. 2021. Need for Speed: Experiences<--size:7.043394565582275-->
<--size:6.939042568206787-->Building a Trustworthy System-Level GPU Simulator. In 2021 IEEE International<--size:6.939042568206787-->
<--size:6.921497344970703-->Symposium on High-Performance Computer Architecture (HPCA) . 868–880. https:<--size:6.921497344970703-->
<--size:6.973999977111816-->//doi.org/10.1109/HPCA51647.2021.00077<--size:6.973999977111816-->
<--size:6.973999977111816-->[51] Guanhua Wang, Kehan Wang, Kenan Jiang, XIANGJUN LI, and Ion Stoica.<--size:6.973999977111816-->
<--size:7.043394565582275-->2021.<--size:7.043394565582275-->
<--size:7.043394565582275-->Wavelet: Efficient DNN Training with Tick-Tock Scheduling. In Pro-<--size:7.043394565582275-->
<--size:7.043394565582275-->ceedings of Machine Learning and Systems , A. Smola, A. Dimakis, and I. Sto-<--size:7.043394565582275-->
<--size:7.043394565582275-->ica (Eds.), Vol. 3. 696–710.<--size:7.043394565582275-->
<--size:7.043394565582275-->https://proceedings.mlsys.org/paper/2021/file/<--size:7.043394565582275-->
<--size:6.973999977111816-->c81e728d9d4c2f636f067f89cc14862c-Paper.pdf<--size:6.973999977111816-->
<--size:6.973999977111816-->[52] Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake Hecht-<--size:6.973999977111816-->
<--size:7.029570579528809-->man, Dehao Chen, Karthik Srinivasa Murthy, Marcello Maggioni, Qiao Zhang,<--size:7.029570579528809-->
<--size:6.949548244476318-->Sameer Kumar, Tongfei Guo, Yuanzhong Xu, and Zongwei Zhou. 2022. Overlap<--size:6.949548244476318-->
<--size:6.903907775878906-->Communication with Dependent Computation via Decomposition in Large Deep<--size:6.903907775878906-->
<--size:7.043394565582275-->Learning Models. In Proceedings of the 28th ACM International Conference on<--size:7.043394565582275-->
<--size:6.903907775878906-->Architectural Support for Programming Languages and Operating Systems, Volume<--size:6.903907775878906-->
<--size:6.903907775878906-->1 (Vancouver, BC, Canada) (ASPLOS 2023) . Association for Computing Machinery,<--size:6.903907775878906-->
<--size:6.973999977111816-->New York, NY, USA, 93–106. https://doi.org/10.1145/3567955.3567959<--size:6.973999977111816-->
<--size:6.973999977111816-->[53] Jeremiah J. Wilke, Joseph P. Kenny, Samuel Knight, and Sebastien Rumley. 2018.<--size:6.973999977111816-->
<--size:6.9948906898498535-->Compiler-Assisted Source-to-Source Skeletonization of Application Models for<--size:6.9948906898498535-->
<--size:6.903907775878906-->System Simulation. In High Performance Computing , Rio Yokota, Michèle Weiland,<--size:6.903907775878906-->
<--size:6.903907775878906-->David Keyes, and Carsten Trinitis (Eds.). Springer International Publishing, Cham,<--size:6.903907775878906-->
<--size:6.973999977111816-->123–143.<--size:6.973999977111816-->
<--size:6.973999977111816-->[54] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang,<--size:6.973999977111816-->
<--size:7.043394565582275-->Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni,<--size:7.043394565582275-->
<--size:6.903907775878906-->Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng<--size:6.903907775878906-->
<--size:6.984453201293945-->Chen. 2021. GSPMD: General and Scalable Parallelization for ML Computation<--size:6.984453201293945-->
<--size:6.973999977111816-->Graphs. https://doi.org/10.48550/ARXIV.2105.04663<--size:6.973999977111816-->
<--size:6.973999977111816-->[55] Xiru Zhang, Michael McKenna, Jill Mesirov, and David Waltz. 1989. An Efficient<--size:6.973999977111816-->
<--size:6.914466857910156-->Implementation of the Back-propagation Algorithm on the Connection Machine<--size:6.914466857910156-->
<--size:6.96702241897583-->CM-2. In Advances in Neural Information Processing Systems , D. Touretzky (Ed.),<--size:6.96702241897583-->
<--size:7.043394565582275-->Vol. 2. Morgan-Kaufmann.<--size:7.043394565582275-->
<--size:7.043394565582275-->https://proceedings.neurips.cc/paper_files/paper/<--size:7.043394565582275-->
<--size:6.973999977111816-->1989/file/e3796ae838835da0b6f6ea37bcf8bcb7-Paper.pdf<--size:6.973999977111816-->
<--size:6.973999977111816-->[56] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yan-<--size:6.973999977111816-->
<--size:6.9774861335754395-->ping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P. Xing, Joseph E.<--size:6.9774861335754395-->
<--size:6.928520679473877-->Gonzalez, and Ion Stoica. 2022. Alpa: Automating Inter- and Intra-Operator Par-<--size:6.928520679473877-->
<--size:6.914466857910156-->allelism for Distributed Deep Learning. In 16th USENIX Symposium on Operating<--size:6.914466857910156-->
<--size:7.043394565582275-->Systems Design and Implementation (OSDI 22) . USENIX Association, Carlsbad,<--size:7.043394565582275-->
<--size:7.008783340454102-->CA, 559–578. https://www.usenix.org/conference/osdi22/presentation/zheng-<--size:7.008783340454102-->
<--size:6.973999977111816-->lianmin<--size:6.973999977111816-->
<--size:6.973999977111816-->[57] Yanqi Zhou, Xuanyi Dong, Tianjian Meng, Mingxing Tan, Berkin Akin, Daiyi<--size:6.973999977111816-->
<--size:6.9425458908081055-->Peng, Amir Yazdanbakhsh, Da Huang, Ravi Narayanaswami, and James Laudon.<--size:6.9425458908081055-->
<--size:7.043394565582275-->2022.<--size:7.043394565582275-->
<--size:7.043394565582275-->Towards the Co-design of Neural Networks and Accelerators. In<--size:7.043394565582275-->
<--size:7.043394565582275-->Proceedings of Machine Learning and Systems , D. Marculescu, Y. Chi, and<--size:7.043394565582275-->
<--size:7.043394565582275-->C. Wu (Eds.), Vol. 4. 141–152.<--size:7.043394565582275-->
<--size:7.043394565582275-->https://proceedings.mlsys.org/paper/2022/file/<--size:7.043394565582275-->
<--size:6.973999977111816-->31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf<--size:6.973999977111816-->
<--size:17.21500015258789-->Appendix: Artifact Description/Artifact Evaluation<--size:17.21500015258789-->
<--size:10.909000396728516-->ARTIFACT DOI<--size:10.909000396728516-->
<--size:9.055215835571289-->https://doi.org/10.5281/zenodo.8223205<--size:9.055215835571289-->
<--size:9.055215835571289-->https://doi.org/10.5281/zenodo.8267785<--size:9.055215835571289-->
<--size:8.965999603271484-->https://doi.org/10.6084/m9.figshare.23996292.v1<--size:8.965999603271484-->
<--size:10.909000396728516-->ARTIFACT IDENTIFICATION<--size:10.909000396728516-->
<--size:9.055215835571289-->The paper presents Calculon, a parameterized analytical perfor-<--size:9.055215835571289-->
<--size:8.875886917114258-->mance model of transformer-based Large Language Models (LLMs)<--size:8.875886917114258-->
<--size:8.907529830932617-->training implemented in Python. Calculon itself is the main contri-<--size:8.907529830932617-->
<--size:8.916550636291504-->bution of the paper. Calculon generated all of the studies and their<--size:8.916550636291504-->
<--size:9.055215835571289-->results presented in the paper. We made Calculon open-sourced<--size:9.055215835571289-->
<--size:9.055215835571289-->and available at https://github.com/paragraph-sim/calculon. We<--size:9.055215835571289-->
<--size:9.055215835571289-->are releasing the scripts and data needed for the result repro-<--size:9.055215835571289-->
<--size:9.055215835571289-->ducibility as part of a special sc23 reproducibility repository.<--size:9.055215835571289-->
<--size:9.055215835571289-->https://github.com/calculon-ai/sc23 The model derives from the<--size:9.055215835571289-->
<--size:8.903016090393066-->extensive survey of performance optimizations proposed for LLMs<--size:8.903016090393066-->
<--size:9.046334266662598-->training. The core analytical model performs a single calculation<--size:9.046334266662598-->
<--size:9.055215835571289-->of time and resource usage for a single application, system, exe-<--size:9.055215835571289-->
<--size:9.055215835571289-->cution strategy set. This model is given three specifications: the<--size:9.055215835571289-->
<--size:9.055215835571289-->LLM, the system the LLM is running on, and the execution strat-<--size:9.055215835571289-->
<--size:9.055215835571289-->egy describing how the LLM is run on the system, including all<--size:9.055215835571289-->
<--size:9.055215835571289-->the optimizations applied to execution. On top of this model, we<--size:9.055215835571289-->
<--size:8.93006420135498-->implemented a complete space search algorithm that concurrently<--size:8.93006420135498-->
<--size:9.01071834564209-->iterates on millions of specification combinations and reports the<--size:9.01071834564209-->
<--size:9.055215835571289-->ones that perform the best. Each calculation takes about 1 ms of<--size:9.055215835571289-->
<--size:9.055215835571289-->a CPU thread time, allowing it to perform extensive studies over<--size:9.055215835571289-->
<--size:8.875886917114258-->billions of configurations utilizing several thousand CPU core hours.<--size:8.875886917114258-->
<--size:9.055215835571289-->The tool can facilitate quick exploration of future LLM training<--size:9.055215835571289-->
<--size:8.974961280822754-->systems and a better understanding of the performance trade-offs<--size:8.974961280822754-->
<--size:8.965999603271484-->that exist.<--size:8.965999603271484-->
<--size:10.909000396728516-->REPRODUCIBILITY OF EXPERIMENTS<--size:10.909000396728516-->
<--size:8.875886917114258-->This document describes the structure of the project and the neces-<--size:8.875886917114258-->
<--size:9.028543472290039-->sary steps required to reproduce experiments from the paper. All<--size:9.028543472290039-->
<--size:8.921056747436523-->the plots and tables in the paper are generated using Calculon and<--size:8.921056747436523-->
<--size:9.024090766906738-->Python scripts that we share as part of our github repo. To repro-<--size:9.024090766906738-->
<--size:8.912040710449219-->duce results in the paper, simply cloning the code from github and<--size:8.912040710449219-->
<--size:8.875886917114258-->running reproducibility scripts locally is enough. Calculon depends<--size:8.875886917114258-->
<--size:8.875886917114258-->only on standard Python libraries and requires Python 3.9 or newer.<--size:8.875886917114258-->
<--size:8.875886917114258-->To regenerate data for larger experiments, only minimal changes to<--size:8.875886917114258-->
<--size:8.948049545288086-->the script are required to run it on slurm or using any similar task<--size:8.948049545288086-->
<--size:9.055215835571289-->manager. Details on how to run Calculon manually are provided<--size:9.055215835571289-->
<--size:9.055215835571289-->in the README.md file in the repository. All the necessary data<--size:9.055215835571289-->
<--size:8.997325897216797-->and scripts are located in the calculon-sc23 repository. The folder<--size:8.997325897216797-->
<--size:9.055215835571289-->is organized with subfolders for every chapter of the paper that<--size:9.055215835571289-->
<--size:9.055215835571289-->contains the results of the experiments. Each chapter folder has<--size:9.055215835571289-->
<--size:8.875886917114258-->further subfolders for every plot and table. These subfolders contain<--size:8.875886917114258-->
<--size:8.974961280822754-->all the data generation scripts, data itself if it cannot be generated<--size:8.974961280822754-->
<--size:8.875886917114258-->on a laptop promptly, and plotting scripts. The result of running the<--size:8.875886917114258-->
<--size:8.974961280822754-->reproducibility scripts should be identical to the results presented<--size:8.974961280822754-->
<--size:8.875886917114258-->in the corresponding chapter of the paper in the corresponding plot<--size:8.875886917114258-->
<--size:9.001791954040527-->or table. There is also a run_all script that produces results for all<--size:9.001791954040527-->
<--size:8.893982887268066-->of the experiments. We will also provide a colab notebook that can<--size:8.893982887268066-->
<--size:8.997325897216797-->regenerate all the plots for the paper. While a single Calculon cal-<--size:8.997325897216797-->
<--size:8.921056747436523-->culation takes about 1 millisecond, most of the experiments utilize<--size:8.921056747436523-->
<--size:8.875886917114258-->complete search space to find the best configuration with respect to<--size:8.875886917114258-->
<--size:8.875886917114258-->the study search space. Some of the studies involve running billions<--size:8.875886917114258-->
<--size:9.055215835571289-->of calculations. That results in O(1000) CPU core hours. Figures<--size:9.055215835571289-->
<--size:9.050776481628418-->2, 3, 4, 7, and 10, and Table 2, can be reproduced on a single CPU<--size:9.050776481628418-->
<--size:8.875886917114258-->system in less than an hour by simply running the scripts provided<--size:8.875886917114258-->
<--size:8.997325897216797-->in the corresponding subfolder. Figures 5, 6, 8, 9, and Table 3 take<--size:8.997325897216797-->
<--size:9.055215835571289-->longer to generate data and require a distributed multi-CPU sys-<--size:9.055215835571289-->
<--size:9.055215835571289-->tem to collect the data in a reasonable time. As such, we provide<--size:9.055215835571289-->
<--size:9.055215835571289-->the data generated by Calculon in the form of json files located<--size:9.055215835571289-->
<--size:8.988387107849121-->in the corresponding subfolder. Instead of a single reproducibility<--size:8.988387107849121-->
<--size:9.055215835571289-->script, we provide a script that generates the data and the script<--size:9.055215835571289-->
<--size:8.965999603271484-->that produces a plot from it.<--size:8.965999603271484-->
<--size:10.909000396728516-->ARTIFACT DEPENDENCIES REQUIREMENTS<--size:10.909000396728516-->
<--size:8.916550636291504-->Calculon is developed in Python 3 and requires Python 3.9 or later<--size:8.916550636291504-->
<--size:8.92556095123291-->and only depends on pypi available packages. As such, it is able to<--size:8.92556095123291-->
<--size:8.875886917114258-->run in any environment that runs Python 3.9+. We successfully used<--size:8.875886917114258-->
<--size:8.875886917114258-->CentOS, Ubuntu, MacOS, and Google colab during our experiments.<--size:8.875886917114258-->
<--size:8.965999603271484-->No pre-existing datasets are needed to run Calculon.<--size:8.965999603271484-->
<--size:10.909000396728516-->ARTIFACT INSTALLATION DEPLOYMENT<--size:10.909000396728516-->
<--size:10.909000396728516-->PROCESS<--size:10.909000396728516-->
<--size:8.903016090393066-->General instructions on how to install and run Calculon can found<--size:8.903016090393066-->
<--size:8.965999603271484-->here: https://github.com/calculon-ai/calculon<--size:8.965999603271484-->
<--size:9.055215835571289-->For<--size:9.055215835571289-->
<--size:9.055215835571289-->the<--size:9.055215835571289-->
<--size:9.055215835571289-->SC<--size:9.055215835571289-->
<--size:9.055215835571289-->‘23<--size:9.055215835571289-->
<--size:9.055215835571289-->paper,<--size:9.055215835571289-->
<--size:9.055215835571289-->a script<--size:9.055215835571289-->
<--size:9.055215835571289-->is<--size:9.055215835571289-->
<--size:9.055215835571289-->provided<--size:9.055215835571289-->
<--size:9.055215835571289-->at<--size:9.055215835571289-->
<--size:9.055215835571289-->https://github.com/calculon-ai/calculon-sc23<--size:9.055215835571289-->
<--size:9.055215835571289-->that<--size:9.055215835571289-->
<--size:9.055215835571289-->will<--size:9.055215835571289-->
<--size:9.055215835571289-->run<--size:9.055215835571289-->
<--size:9.055215835571289-->Calculon and plot all results. It runs either locally on a single<--size:9.055215835571289-->
<--size:9.055215835571289-->machine or on a distributed cluster that has a global network<--size:9.055215835571289-->
<--size:9.055215835571289-->filesystem. The script performs all Calculon calculations by<--size:9.055215835571289-->
<--size:9.055215835571289-->issuing single machine jobs (no MPI, SHMEM, etc. needed). In<--size:9.055215835571289-->
<--size:9.055215835571289-->total O(10,000) jobs will be launched, many of which are only<--size:9.055215835571289-->
<--size:9.055215835571289-->a few minutes long. The script provides a mechanism for users<--size:9.055215835571289-->
<--size:9.055215835571289-->to customize the execution of jobs on their own cluster’s job<--size:9.055215835571289-->
<--size:8.965999603271484-->scheduler. A Chameleon Cloud interface is provided.<--size:8.965999603271484-->

mode:  8.965999603271484
Title: 1 INTRODUCTION
Title: 2 ANALYTICAL MODEL
Title: 3 LLMs presented in Fig. 7 this required 467,553,284 calculations.
Title: 4 PERFORMANCE TRADE-OFFS
Title: 5 OPTIMAL STRATEGY SEARCH
Title: 6 OFFLOADING ANALYSIS
Title: 7 OPTIMAL SYSTEM SEARCH
Title: 8 CONCLUSION
