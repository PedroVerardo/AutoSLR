<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Gao2021-ICSE</title>
<meta name="generator" content="Docling HTML Serializer">
<style>
    html {
        background-color: #f5f5f5;
        font-family: Arial, sans-serif;
        line-height: 1.6;
    }
    body {
        max-width: 800px;
        margin: 0 auto;
        padding: 2rem;
        background-color: white;
        box-shadow: 0 0 10px rgba(0,0,0,0.1);
    }
    h1, h2, h3, h4, h5, h6 {
        color: #333;
        margin-top: 1.5em;
        margin-bottom: 0.5em;
    }
    h1 {
        font-size: 2em;
        border-bottom: 1px solid #eee;
        padding-bottom: 0.3em;
    }
    table {
        border-collapse: collapse;
        margin: 1em 0;
        width: 100%;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;
    }
    th {
        background-color: #f2f2f2;
        font-weight: bold;
    }
    figure {
        margin: 1.5em 0;
        text-align: center;
    }
    figcaption {
        color: #666;
        font-style: italic;
        margin-top: 0.5em;
    }
    img {
        max-width: 100%;
        height: auto;
    }
    pre {
        background-color: #f6f8fa;
        border-radius: 3px;
        padding: 1em;
        overflow: auto;
    }
    code {
        font-family: monospace;
        background-color: #f6f8fa;
        padding: 0.2em 0.4em;
        border-radius: 3px;
    }
    pre code {
        background-color: transparent;
        padding: 0;
    }
    .formula {
        text-align: center;
        padding: 0.5em;
        margin: 1em 0;
        background-color: #f9f9f9;
    }
    .formula-not-decoded {
        text-align: center;
        padding: 0.5em;
        margin: 1em 0;
        background: repeating-linear-gradient(
            45deg,
            #f0f0f0,
            #f0f0f0 10px,
            #f9f9f9 10px,
            #f9f9f9 20px
        );
    }
    .page-break {
        page-break-after: always;
        border-top: 1px dashed #ccc;
        margin: 2em 0;
    }
    .key-value-region {
        background-color: #f9f9f9;
        padding: 1em;
        border-radius: 4px;
        margin: 1em 0;
    }
    .key-value-region dt {
        font-weight: bold;
    }
    .key-value-region dd {
        margin-left: 1em;
        margin-bottom: 0.5em;
    }
    .form-container {
        border: 1px solid #ddd;
        padding: 1em;
        border-radius: 4px;
        margin: 1em 0;
    }
    .form-item {
        margin-bottom: 0.5em;
    }
    .image-classification {
        font-size: 0.9em;
        color: #666;
        margin-top: 0.5em;
    }
</style>
</head>
<body>
<div class='page'>
<h2>Resource-Guided  Configuration  Space Reduction for Deep Learning Models</h2>
<p>Yanjie  G ao1,  Yonghao  Zhu1,  Hongyu Zhang2,  Haoxiang  Lin11,  Mao  Yang1 1Microsoft Research,  Beijing,  China 2The University  of Newcastle, NSW,  Australia</p>
<p>Email:  {yanjga,  v-yonghz,  haoxlin,  maoyang}@microsoft.com,  hongyu.zhang@newcastle.edu.au</p>
<p>Abstract -Deep learning models, like traditional  software  systems,  provide  a  large  number  of configuration  options.  A  deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory,  computing  power  of  devices,  or  execution  time.  The training with non-conforming configurations could lead to many failed  AutoML  trial jobs  or inappropriate  models,  which  cause significant  resource  waste  and  severely  slow  down  development productivity.</p>
<p>as the number of layers and layer type).  The numerous options provided  by  a  DL  model  result  in  an  exponential  number of possible  configurations,  making  manual  tuning  extremely tedious  and time-consuming.</p>
<p>In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model  learning  performance  because  it  excludes  trial  jobs  not satisfying  the  constraints  and  saves  resources  for  more  trials. We formulate the resource-guided configuration space reduction as  a  constraint  satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to  the  model  weight  size,  number  of  floating-point  operations, model  inference  time,  and  GPU  memory  consumption.  It  then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate  the  effectiveness  of  DnnSAT  in  accelerating  stateof-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to  3.95X  on  public  benchmarks.  We  believe  that  DnnSAT  can make AutoML more practical in a real-world environment with constrained resources.</p>
<p>Index  Terms -configurable  systems,  deep  learning,  AutoML, constraint solving</p>
<h2>I. In t r o d u c t i o n</h2>
<p>Many traditional  software  systems,  such  as  compilers  and web  servers,  are  highly  configurable.  They  provide  many configuration options, and different combinations of the options could  lead  to  different  system  quality  attributes.  In  recent years, deep learning (DL) models have become an integral part of many  modern  software-intensive  systems  such  as  speech recognition  systems,  chatbots,  and  games.  Like  traditional software  systems,  DL models  also provide many  configuration options for developers to tune. These configuration options can be  largely  classified  into  two  categories:  hyperparameter (such as the batch size and learning rate) and neural architecture (such</p>
<p>^Corresponding  author.</p>
<p>Recently,  automated machine learning  (AutoML)  techniques have  been  widely  adopted  to  automate  and  accelerate  DL model tuning.  AutoML tools  such  as  NNI  (Neural  Network Intelligence)  [1]  usually  apply Hyperparameter Optimization (HPO) [2] and Neural Architecture Search (NAS) [3] algorithms and launch hundreds or even thousands of AutoML trial jobs (or trials for short)  to  systematically  explore diverse configurations of hyperparameters and neural architectures.  It has  been  found that AutoML  significantly  boosts  the  productivity  of  DL development  [2]-[4].</p>
<p>However,  current  AutoML  approaches  do  not  take  into consideration the computational constraints imposed by various resources  such  as  available  memory,  computing  power  of devices,  or  execution  time,  because  the  resources  required by  a  DL  model  often  remain  unknown  to  developers  before job  execution.  Non-conforming  model  configurations  could lead  to  many  failed  AutoML  trials  or  inappropriate  models, which  not  only  waste  significant  shared  resources  (such  as GPU,  CPU,  storage,  and network I/O)  but also  severely  slow down  development productivity.  A  typical  resource  is  GPU memory, which is critical yet limited for training. If developers do  not  size  the  model  carefully  enough,  trials  will  trigger OOM (out-of-memory)  exceptions  and fail.  For instance,  one PyTorch ResNet-50 [5] trial with an overlarge batch size of 256 causes  an  OOM  when being  scheduled on  the NVIDIA  Tesla P100  GPU;  it requires  22  GB  of GPU memory, but P100  has only  16  GB  in  total  [6 ].  Even  worse,  since  there  can  be more than  ten  tunable  hyperparameters  for  the  ResNet-50  model, other hundreds  of trials  with the  same batch  size  could also experience  OOM and crash.  According  to  our recent empirical study  on  failed  DL jobs  in  Microsoft  [7],  8 .8 %  of the  4960 job  failures  were  caused by  GPU memory  exhaustion,  which accounts  for the  largest category  in all  deep  learning  specific failures.  Therefore, it is necessary for AutoML tools to enforce a  constraint  that  a  DL  model  cannot  consume  more  GPU memory than the capacity when exploring model configurations. Another useful  constraint is  that the  size  of a model's  weights cannot exceed a certain  upper bound.  Otherwise,  the resulting DL application may be too  large for efficient management and execution  due  to  insufficient  computing  power  of the  target</p>
<p>resource-restricted  devices.  If the  unsatisfiable  AutoML trials can  be  excluded  ahead  of  time,  resources  will  be  saved  to perform more trials,  thus  a larger configuration  space could be explored.</p>
<p>A  simple  workaround  is  to  run  and  profile  trials  for  a while to  estimate their resource consumption.  Such  a resourceconsuming  method is  unaffordable in the  scenario  of AutoML, where  there  exist  a  large  number  of possible  hyperparameter combinations and neural architectures.  Some research work [8 ][1 0 ]  incorporates  certain  resource  quantification  into  the  loss function  for  a  global  optimization  with  the  model  learning performance  ( e.g. , accuracy).  However,  their  purpose  is  to reduce the resource  consumption  of the  final  model  as  much as possible  while achieving  an  expected  model  learning performance, instead of excluding unsatisfiable trials in advance to  improve  the  search  efficiency.  Therefore,  such  work  could also cause failed trials/inappropriate models and resource waste. Besides,  they  are  limited  to  NAS  algorithms  only  and  cannot be  applied  to  HPO  algorithms.</p>
<p>In this paper, we  propose  DnnSAT,  a  resource-guided AutoML approach for deep  learning  models,  which  can help existing  AutoML  tools  efficiently  reduce  the  configuration space  ahead  of  time.  We  formulate  such  a  resource-guided space  reduction problem  as  a constraint  satisfaction problem (CSP)  [11].  DnnSAT includes  a unified  analytic  cost model  to construct common constraints with respect to the model weight size, number  of  floating-point  operations  (FLOPs),  model inference time,  and GPU memory consumption.  It then utilizes an  SMT  (satisfiability  modulo  theories)  solver  (e.g.,  Microsoft Z3  [12])  to  obtain  the  satisfiable  model  configurations  before trial  execution.  According  to  the  characteristics  of constraints ( e.g. , monotonicity),  we  also  apply  some  special  optimizations to  accelerate  the  solving.</p>
<p>We  have  implemented  DnnSAT  and  evaluated  it  extensively  on  public  AutoML  benchmarks  (HPOBench [13] and NAS-Bench-101  [14]) with various  search  methods  (Random Search  [15],  Regularized  Evolution  [16],  Hyperband  [2],  and Reinforcement Learning  [14])  and representative  DL models (VGG-16  [17]  and  LSTM  [18]-based  Seq2Seq  [19]).  The experimental results  show  that DnnSAT  achieves  an  average speedup  from  1.19X  to  3.95X  on  public  benchmarks  and noticeably  reduces  the  configuration  space.</p>
<p>In  summary,  this  paper  makes  the  following  contributions:</p>
<ul>
<li>1) We propose a resource-guided AutoML approach for deep learning  models  to  efficiently  reduce  the  configuration space  ahead  of time.</li>
<li>2)  We  build  a  unified  analytic  cost  model  to  construct common constraints  and utilize  an  SMT solver to  obtain the  satisfiable  configurations  of  hyperparameters  and neural  architectures.</li>
<li>3)  We implement a tool  named DnnSAT  and demonstrate its  practical  effectiveness.</li>
</ul>
<p>The  rest  of  the  paper  is  organized  as  follows.  Section  II introduces  the background.  Section  III  presents  our  analytic approach.  Section  IV  details  the  design  and  implementation of DnnSAT.  Section V shows  experimental results.  Section VI</p>
<ul>
<li>1 from tensorflow.keras import layers,  models 2 ... 3  model = models.Sequential() 4 model.add(layers.Conv2D(filters=64,   kernel_size= (3,   3), ^ activation= 'relu',   input_shape= (32,   32,   3))) 5   model.add(layers. AveragePooling2D(pool_size= (2,   2),  padding ^ = 'same')) 6 model.add(layers.Flatten()) 7 model.add(layers.Dense(units=64,   activation= 'relu')) 8 model.compile(optimizer= 'adam', ^ loss=tf.   keras.   losses. MeanSquaredErrorO)</li>
<li>9   model.fit(train_images,  train_labels,  batch_size,  epochs=10)</li>
</ul>
<h2>(a) Training  pro g ra m   u sin g   K eras  A P I.</h2>
<figure><figcaption><div class="caption">(b) C o m p u ta tio n graph  fo r  m odel  in fe r -( c ) Tensors w ith shapes on w hich Conv2D ence. operates.</div></figcaption></figure>
<p>Fig.  1: A sample TensorFlow model sequentially constructed by the framework built-in Conv2D (2D  convolution), AvgPool2D (2D  average  pooling), Flatten (collapsing  the  input  into  one  dimension  without  affecting  the batch  size),  and Dense (fully  connected  layer)  operators.</p>
<p>discusses  extensibility  and  possible  threats.  We  survey  related work in  Section VII  and conclude this  paper in  Section  VIII.</p>
<h2>II. Ba c k g r o u n d</h2>
<p>Deep learning (DL) is a subfield of machine learning to learn layered  data  representations  known  as  models.  A  DL  model is  formalized  as  a tensor-oriented  computation  graph  [2 0 ]  by frameworks  such  as  TensorFlow  (TF)  [21],  PyTorch  [22],  and MXNet  [23],  which is  a directed acyclic graph (DAG).  The inputs  and  outputs  of such  a  computation  graph  and  its  nodes are tensors (multi-dimensional arrays of numerical values). The shape  of  a  tensor  is  the  element  number  in  each  dimension plus the element data type. Each node represents the invocation of a mathematical  operation  called  an operator  (e.g., elementwise  matrix  addition).  An  edge  delivers  an  output tensor  and specifies  the  execution  dependency.  In this  paper,  we  use  the terms  'operator'  and  'node'  interchangeably  since  a node  is completely  determined  by  its  invoked  operator.</p>
<p>Fig.  1a  shows  a  simple  TensorFlow  training  program using the  Keras  [24]  API,  which  sets  up  a  sequential  model  with the  framework built-in Conv2D (2D  convolution  with  a  3 x 3 kernel size), AvgPool2D (2D average pooling with the 'same' padding  setting1), Flatten (collapsing  the  input  into  one dimension without affecting  the batch  size),  and Dense (fully connected layer with 64 units)  operators  (lines 4-7).  The above filter  size,  padding,  and  number  of units  are hyperparameters, which  are  parameters  to  control the  training  process.  Fig.  1b demonstrates  the  corresponding  computation  graph  for model</p>
<p>1See  https://keras.io/api/layers/pooling_layers/average_pooling2d/  for  an explanation  of the padding  argument.</p>
<figure><figcaption><div class="caption">Fig.  2: Settings  of  an  MNIST  training  program  supported  by  NNI  with Hyperparameter Optimization.</div></figcaption></figure>
<p>inference.  Fig.  1c  illustrates  the  input,  weight,  and  output tensors  with  shapes  on  which Conv2D operates.</p>
<p>To  choose  better  configurations  of  hyperparameters  and neural  architectures,  developers  often  adopt  a trial-and-error strategy:  submitting  hundreds  or even thousands  of trial jobs with  each  being  assigned a  different model  configuration.  This strategy  is  very  inefficient  because  of  the  overlarge  configuration  space.  Recently,  many  AutoML  tools,  such  as  NNI (Neural Network Intelligence)  [1],  Auto-Keras  [4],  and AutoSklearn [25], are proposed to automate the exploration of model configurations.  They  help  developers  find  a  hyperparameter combination  (Hyperparameter  Optimization  (HPO)  [13])  or design an elaborate neural network (Neural Architecture Search (NAS)  [14]),  which  can  both  minimize the  loss  and maximize the model learning performance  (e.g.,  accuracy).</p>
<p>Suppose  that  a  DL  model  has  m  hyperparameters  whose domains  are B 1,B 2, ·GLYPH&lt;129&gt;GLYPH&lt;129&gt; , B m . A  model  configuration  is  an instance  of such  a model  with  concrete hyperparameter values. All  model  configurations  constitute  the configuration  space. HPO applies  some search method (e.g., random  search,  evolutionary strategies, or Bayesian optimization) in the enumeration of the  configuration  space  to  find a  candidate with the  optimal hyperparameter vector (combination)  A   e  B i  x B 2 x ·  ·  ·   x Bm, which  best  meets  the  training  objective.  Fig.  2  shows  the settings  of  an  MNIST  training  program  supported  by  the AutoML tool NNI with HPO. The upper part (lines 1-8) defines the  possible  values  of  tunable  hyperparameters  (batch  size, learning  rate,  etc.);  the  bottom  part  (lines 1 0 -2 0 ) specifies the  search  method and runtime resource requirements  (GPU count,  main  memory  size, etc. ). The controller process  of NNI performs  a random  search  (line  14)  and may  fork  over a hundred trials  executing  the command  on  line  16.  Trials  send timely  feedback  such  as  the  mean  squared error or accuracy to  the  controller  for the  decision  of early  stopping.</p>
<p>Similarly,  NAS  automates  the architecture engineering  of a DL model. The configuration space consists of various automatically  generated,  syntactically  legal  neural  network  structures such  as  chained or multi-branch  networks.  NAS  also  applies</p>
<table><caption><div class="caption">TABLE I Co m m o n   h y p e r p a r a m e t e r s o f DL m o d e l s .</div></caption><tbody><tr><th>Hyperparameter</th><th>Domain</th><th>Hyperparameter</th><th>Domain</th></tr><tr><td>Batch Size</td><td>N</td><td>Kernel Size</td><td>N</td></tr><tr><td>Output Channels</td><td>N</td><td>Stride</td><td>N</td></tr><tr><td>Padding</td><td>N</td><td># of RNN Layers</td><td>N</td></tr><tr><td># of Units</td><td>N</td><td>Sequence Length</td><td>N</td></tr><tr><td>Operator Type</td><td>N</td><td>Edge</td><td>B</td></tr><tr><td>Learning Rate</td><td>R+</td><td>Dropout</td><td>[0, 1)</td></tr></tbody></table>
<p>different  search  methods  including  random  search,  gradientbased algorithms,  and reinforcement learning  (RL)  [26]  for the space  exploration.</p>
<h2>III. Pr o p o s e d Ap p r o a c h</h2>
<h2>A. Problem Formulation</h2>
<p>Formally, a DL model X  is represented as a directed acyclic graph  (DAG)  [20]:</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>Each  node u i is  an  operator (i.e., a  mathematical  operation such  as  convolution  and  pooling).  A  directed  edge  (ui ,  uj ) pointing  from  an  output of node u i to  an  input  of u j delivers the  output tensor and specifies the execution dependency. Each p k is  a hyperparameter (e.g., input tensor shape and batch size) whose domain is denoted as  B k .  Table I lists  some  commonly used hyperparameters  with their domains.</p>
<p>For  bk e  B k where  1  &lt; k &lt; m, we  use  X (b 1 , b2 , ·  ·   ·   , b m ) to  represent  one  model  configuration  of  X .  Then,  the  configuration  space  of model  X ,  denoted by  A x ,  is  defined as follows:</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>In  an  AutoML  experiment,  there  may  exist  a  series  of  DL models  X 1, X   2 , ···  , X   N (e.g., models  searched  by  NAS). For  such  an experiment,  its  configuration  space  A   is  defined as  the  union  set of all  models'   configuration  spaces:</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>We formulate  the  resource-guided  configuration  space  reduction  for  an  AutoML  experiment  with N models  as  the following  constraint  satisfaction  problem (V ,D ,C ) [11]:</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>V  is  a  set  of model  configuration  variables,  D  represents the respective variable domains, and  C  is  the constraint set. Each variable  V i 6 [1 N ] can take on the model configurations  of X i (i.e., the  domain  of V i is A Xi).  For a constraint  Cj ,  f j is  a non-negative restriction function which denotes the demand for a certain resource; lb j and ub j are the  lower and upper bounds of that resource,  respectively. If one is  more interested in the upper bound, we simply assume that lb j = 0.  Hyperparameters</p>
<p>are decision  variables of the  constraints,  which  are  quantities controlled by the decision-maker to define the  search  space for optimization.  As  mentioned before,  an  example  fj  calculates the  GPU  memory consumption for training  a DL  model,  and ub j is  the  memory  capacity  of the  allocated  GPU  device  (e.g., 16  GB  for NVIDIA Tesla P100).</p>
<h2>B. The  Proposed Analytic Approach</h2>
<p>DnnSAT adopts an analytic approach to construct the restriction  functions  on resources.  We  observe that the  algorithmic execution of a DL model can be represented as iterative forward and backward propagation on its computation graph.2 Therefore, computing the resource  consumption for one iteration is  then reduced  to  the  calculation  of the  resource  required  by  each operator on the computation graph in accordance with a certain graph  traversal  order.  Currently,  a  DL  model  is  required  to be  deterministic  without  control-flow  operators  (e.g.,  loops and conditional branches),  thus  we  assume  that the  execution flow  and resource  consumption  across  different  iterations  are identical.  We  define  an  auxiliary  function  g  on  the  operator set, which represents  the current resource  consumption when the  operator  under  visiting  has  just  finished  execution  in the  iteration.  Let  S  = (ui1, ui2, ··· , u i n) be  a  topological (linear)  order  extended  from  the  edge  order  of  the  model such  that u j i S u k i = ^ (u k  ,u i i j ) e  X .  S  is  called  an operator schedule, representing  the  actual runtime  execution order of operators. DnnSAT pre-generates S by referring to the framework implementations  [27]-[29].  Suppose that r  is  the operator resource cost function, IterC nt is  the  iteration  count, and R i n i t , R f i n i are  the  resource  consumption  of the  one-off initialization and clean-up performed by DL frameworks which are  assumed to  be  0  if not  specifically  mentioned.  We  define g  and the  restriction  function  f   as  follows:</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>So long as the above h and t are functions, g uniquely exists by the transfinite recursion theorem  schema [30] since S is  a well order,  and f also exists. The formalization of g indicates that it could compute the current resource consumption by referring to extra information of previous consumption and visited operators. Examples  of g and f can  be  found  in  Section  IV-B. DnnSAT includes  a  unified  analytic  cost  model  to  define  operators' resource  cost functions  and  common  constraints.  Table  II  lists the  constraints  that  we  have  implemented.</p>
<p>The  objective  of  DnnSAT  is  to  reduce  the  configuration space A by  eliminating  those  X i (b 1  ,b i i 2 ,   ··· ,b i m ) which violate  the  enforced  constraints  before  submitting  AutoML trials. A naive method is to compute the values of the restriction functions  for each  model  configuration in A and then check whether  such values  fall  within the  allowed bounds.  DnnSAT adopts  a  much  more  efficient  approach:  it  specifies  those</p>
<p>2This  also  applies  to  model inference  which has  a  simplified  representation with  single-pass  forward propagation  and no backward propagation.</p>
<p>TABLE II r a i n t s s e d b y t a t i o n a l u r c e s</p>
<p>Co m m o n   Co n s t Im p o Th e   Co m p u Re s o .</p>
<table><tbody><tr><th>Restriction Category</th><th>Resource Upper Bound</th><th>Scenario</th></tr><tr><td>Model Weight Size</td><td>Allowed Binary Size</td><td>Inference</td></tr><tr><td>Number of Floating-point Operations (FLOPs)</td><td>(Device FLOPS) x (SLA of Inference Latency)*</td><td>Inference</td></tr><tr><td>Model Inference Time</td><td>SLA of Inference Latency</td><td>Inference</td></tr><tr><td>GPU Memory Consumption</td><td>Device Memory Capacity</td><td>Training Inference</td></tr></tbody></table>
<p>*   'FLOPS'  denotes  floating-point operations  per  second,  and  'SLA' stands  for  service-level  agreement.</p>
<figure><figcaption><div class="caption">Fig.  3:  Workflow  of DnnSAT.</div></figcaption></figure>
<p>constraints  in  the  SMT-LIB  (Satisfiability  Modulo  Theories Library  [31])  syntax  and  uses  the  Microsoft  Z3  solver  [12] to  obtain  all  the  satisfiable  model  configurations.  Such  an approach  is  feasible  because  the  restriction  functions  of the common constraints can be composed by SMT solvers' built-in functions  ( e.g. , multiplication  and division)  and the  constraints are  simple  numerical  inequalities.  We  also  apply  some  special optimizations  to  accelerate  the  solving based  on the  constraint characteristics  (e.g.,  monotonicity),  which  are  described  in Section IV-E.</p>
<h2>IV. De s i g n a n d I m p l</h2>
<p>e m e n t</p>
<p>a</p>
<p>t</p>
<p>i o</p>
<p>n</p>
<h2>A. Workflow</h2>
<p>Fig.  3  shows  the  workflow  of DnnSAT.  It  accepts  a  source DL  model,  configuration settings, and  constraint settings as input. The  model  is  parsed  by  a  front-end  parser  and reconstructed to  the  corresponding  computation  graph.  Some DL  frameworks  such  as  PyTorch  employ  the define-by-run approach [2 2 ] such  that  a saved  model  records  only  an execution trace instead of the full computation graph.  DnnSAT currently  relies  on  users  to  supply  multiple  models  in  case the  graph  may  dynamically  change  (in  the  future,  it may  be possible  to  try  extracting  the  full  computation  graph  from  a DL program by  static  code  analysis).  Configuration  settings include  the  hyperparameters  to  be  tuned  and  their  domain definitions.  Constraint  settings  contain the  built-in  constraints that  need to  be  satisfied  and  their  allowed bounds,  as  well  as constraint-related runtime constants  of the DL framework ( e.g. , type  and version)  and target  device  (e.g.,  FLOPS).</p>
<p>DnnSAT has defined four common constraints (Section IV-B) and a  set of analytic  and framework-independent resource  cost functions  for  DL  operators  (Section  IV-C).  It  traverses  the computation graph in accordance  with  a predefined  operator schedule (i.e., operator  execution  order) to automatically generate  the  constraint  specifications  in  SMT-LIB  for  later solving  (Section  IV-D).  There  are  two  working  modes  for</p>
<p>integrating  DnnSAT with  existing  AutoML tools.  One  is  the interactive mode,  in  which  AutoML tools  work as  usual but each  configuration  will  be  sent  to  DnnSAT  for  solving  via an  API  call before  launching  a trial.  Such  a mode  is  simple, non-intrusive,  and requires  less  effort.  We  have run  DnnSAT with  HPOBench  [13]  and NAS-Bench-101  [14]  interactively to  evaluate the effectiveness  in  speeding  up  AutoML methods (Section V-A),  and  similar integration  work could be  done in AutoML tools  such as NNI and Auto-Keras.  The other is the pruning mode,  in  which  DnnSAT  eliminates the unsatisfiable model  configurations  in  advance  and  feedbacks  a  reduced configuration  space to AutoML tools during their initialization. To solve the constraints, the Microsoft Z3 solver is invoked with some optimizations  (Section  IV-E).  DnnSAT  is  extensible to support user-defined constraints  by permitting  users  to  specify their  own  constraint  specifications  in  SMT-LIB  with  those defined hyperparameters.</p>
<h2>B. The  Constraints</h2>
<p>DL  models are both compute-intensive and memoryintensive,  making  them  difficult  to  be  trained  or  deployed on  systems  and  platforms  with  limited  resources.  In  this paper, we consider four representative computational constraints with  respect  to  the  model,  namely  weight  size,  number  of floating-point  operations,  inference  time,  and  GPU  memory consumption. The meaning  of the notations  and symbols  can be found in  Section  III.</p>
<p>Model Weight Size. Weights (including biases) are the numerical  learnable  parameters  of  operators,  being saved in  the  model  file  and  taking  up  most  of  the  space.  The size  of weights  is  important,  especially on  resource-restricted devices  such  as  mobile  phones. An  overlarge  DL  model causes  inefficient  model/application  management,  expends unaffordable energy [32], or even cannot fit in the target devices' main  memory.  The  total  model  weight  size  is  calculated by accumulating  the weight  size of each operator.  Assuming  that W T is  the restriction  function  and WTmin,  W Tmax are  the lower and upper bounds in bytes,  the constraint is then defined as  follows:</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>Number of Floating-point Operations  (FLOPs). FLOPs is considered as a stronger predictor of energy usage and inference time  [33].  The  total  FLOPs  for  inference  is  calculated  by accumulating  the FLOPs  of each  operator in  accordance with the  operator  schedule.  Assuming  that F is the  restriction function and F m i n , F m a x are  the minimal and maximal values allowed,  the  constraint  is  then  defined as  follows:</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>Model Inference Time. This is a critical runtime performance indicator for DL applications. The total time is  calculated by accumulating the execution time of each operator in accordance with the operator  schedule.  Assuming  that T is  the restriction function  and Tmin,  Tmax are  the  lower and  upper bounds,  the constraint  is  then  defined as  follows:</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>GPU  Memory  Consumption. As  mentioned  before,  GPU OOM accounts for the largest DL failure category [7], therefore controlling the GPU  memory  consumption  is  critical  to reduce OOM exceptions and save shared resources.  However, the  calculation  is  rather  complicated  since  there  are  many hidden framework factors  observably  affecting the final GPU memory consumption  [6].  We adopt a  simplified yet common approach  for both  inference and data-parallel training,  which accumulates the GPU memory required by each operator under forward propagation  in  accordance with  the operator  schedule. Assuming that M is  the  restriction  function  and M m i n , M m a x are  the  minimal  and  maximal  GPU  memory  consumption allowed  in  bytes  (e.g.,  taking  the  memory  capacity  as  the maximal value),  the  constraint is then  defined as  follows:</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>If u is an operator  under  backward  propagation,  we  let r(u) = 0. Rinit represents  the  GPU  memory  consumed by  the  CUDA  context  and  initial  input  tensors  during  the framework initialization. The CUDA context contains managing information to control and use GPU devices, which is assumed to  be  a  constant  obtained by profiling.</p>
<h2>C. Resource  Cost Functions  of Operators</h2>
<p>Operators  are  mathematical  functions  on  various  types  of tensors.  DnnSAT  defines  analytic  and  framework-independent resource  cost  functions for  operators. Such  a  solution  is technically  feasible  because:  (1)  frequently  used  operators are  well-defined  with  clear  syntax  and  semantics;  (2)  DL frameworks implement them similarly  (e.g.,  calling  NVIDIA CUDA, cuDNN, or cuBLAS  APIs).  In this  section,  we take the  Conv2D  operator  in  Fig.  1b  as  an  example  to  illustrate the  four  resource  cost  functions  with  respect  to  the  studied constraints.</p>
<p>The following  symbols  are used to  denote the hyperparameters  and  tensor  shapes. S f is  the  size  of  input  data  type (e.g., 4  bytes for FLOAT32  data). N represents  batch size. Hk and Wk are kernel  (filter)  height  and width;  they  are usually equal. f l t i and si are  filter  size  and  stride  size  at  index  i. f l t represents  the  number  of filters.  Padding padi 'controls  the amount of implicit  zero-paddings  on  both  sides  for padding number of points for each dimension', and dilation di 'controls the  spacing  between  the kernel  points'  [34]. Hin,  Win, and</p>
<p>C i n are  input  height,  width,  and  channels,  respectively. H , o W , o and C o are  output  height,  width,  and  channels,  which have the following  relations  with other  symbols:</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>Then,  the  resource  cost functions  with respect to the  four constraints model  weight  size ( W T ), FLOPs  ( F ) , model inference  time ( T ), and GPU  memory  consumption (M ) are defined as  follows:</p>
<div class="formula-not-decoded">Formula not decoded</div>
<p>M i t , M w t , and M out denote the  GPU memory  occupied by the  input,  weight,  and  output  tensors,  respectively. B d and F i o p s are  memory  bandwidth  and floating-point operations per second (FLOPS) of the target device, which can be assumed to be constants.  The first item of T ( Conv2D )  represents  the  data access  time from  and to  GPU memory  [35].  We do  not count M i t in the  GPU memory consumption because an operator's input  tensors  reuse  the  GPU  memory  of  either  the  initial inputs or predecessors' outputs,  which  have  been  calculated in the  initialization  cost (R i n i t ) and predecessor operators'   cost functions.  More  details  about  the  estimation  of  FLOPs  and model  inference time can be found at  [35]  and  [36].</p>
<p>Currently,  DnnSAT  supports  70+  operators.  It is  also  extensible and can  support new  operators,  which  are  discussed in Section  VI.</p>
<h2>D. Constraint Specification</h2>
<p>In  this  section,  we  describe  how  to automatically  generate a constraint specification  in  SMT-LIB.  The  DL model  in Fig.  1 is used as an example, and the enforced constraint is the model weight size  must be  less  than  or equal to  10 MB. A  snippet of the constraint  setting  is  shown  as  follows:</p>
<pre><code>{"constraint':'weight_size',"max":   10485760,'min':0}</code></pre>
<p>Fig.  4  lists  the  illustrated  SMT-LIB  code.  First,  DnnSAT specifies the  constraint bounds read from the  constraint setting file  (lines  3-5).  It  then  declares  the  hyperparameters  of the size  of input data type ( S f ),  number of input channels  (Cin), kernel  size (Hk and W k),  filter  size  (flt),  and  unit  size (U ) of the Dense operator  (lines  7-12)  and writes  their domains (lines  14-16),  according  to  the  contents  of the  configuration setting  file.  As  the  batch  size  (N)  does  not  contribute  to  the model  weight  size,  we  simply  ignore  it.</p>
<p>Next, DnnSAT  traverses the computation graph from Conv2D to Dense one after another.  For each  operator type, we  prepare  SMT-LIB  templates  in  Python  via  Z3  APIs  for</p>
<pre><code>1 (s e t - l o g i c QF_UFNIA) ; N o n - l i n e a r i n t e g e r a r i t h m e t i c l o g i c 2 ; Constraint  bounds 3 (declare-const  W T_M in  I n t ) ; Lower  bound: 0  MB 4 (declare-const  W T_M ax  I n t ) ; Upper  bound: 10  MB 5 (a s s e r t (and (   = WT_Min  0) (   = WT_Max  10485760))) 6 ; Hyperparameters 7 (declare-const  Sf  I n t ) ; S i z e o f i n p u t data  ty p e 8 (declare-const  Cin  I n t ) ; Conv2D  in p u t c h a n n e l s 9 (declare-const  Hk  I n t ) ; Conv2D  k e r n e l h e i g h t 1 0 (declare-const  W k  I n t ) ; Conv2D  k e r n e l weight 1 1 (declare-const  f l t I n t ) ; Conv2D  number  o f   f i l t e r s 1 2 (declare-const  U   I n t ) ; Dense  u n i t s i z e 1 3 ; Hyperparameter  domains 1 4 (a s s e r t (and (and (   = Sf  4) ( = Cin  3)) (and (   = Hk  W k) (   = U ^ 64)))) 1 5 (a s s e r t (or (or (=  H k 3) (=  Hk 5)) (or (= Hk  7) (=  H k 11)))) 16 (a s s e r t (or (or (= f l t 64) (= f l t 128)) (= f l t 512))) 1 7 ; Compute  th e weight s i z e o f Conv2D 1 8 (declare-const  WT_Conv2D  I n t ) ; Conv2D  weight s i z e 1 9 (declare-const  Co  I n t ) ; Conv2D  output c h a n n e l s 2 0 (a s s e r t (= f l t Co)) 2 1 (a s s e r t (= (* Sf (+ (* (* (* Hk  W k) Cin) Co) Co)) WT_Conv2D)) 2 2 ; Compute  th e weight s i z e o f Dense 2 3 (declare-const  WT_Dense  I n t ) 2 4 (a s s e r t (= (* Sf (+ U 1)) WT_Dense)) 2 5 ; Compute  th e   model weight s i z e 2 6 (declare-const  W T I n t ) 2 7 (a s s e r t (= (+ WT_Conv2D  WT_Dense) WT)) 2 8 ; S p e c i f y t h e c o n s t r a i n t 2 9 (a s s e r t (and (<=  W T_M in  W T ) (<=  W T WT_Max))) 3 0 (check-sat) 3 1 (e x i t )</code></pre>
<p>the  resource  cost  functions.  When  an  operator  is  visited, DnnSAT locates the matched template and generates the Python wrapper of SMT-LIB code using those declared hyperparameter symbols. For example, lines 18-21 correspond to calculating the weight  size  of Conv2D . Since AvgPool2D uses the  'same' padding  setting  (line  5  in  Fig.  1a),  its  output  tensor  shape keeps  unchanged  with  that  of Conv2D . Both AvgPool2D and Flatten do  not have  weights,  so  we also  ignore them. Dense has  a  weight  tensor of a  64-element array plus  a bias of  1 data  element  (lines  23-24).  Therefore,  the  total  model weight  size  is  equal  to  the  sum  of the  weight  sizes  of both Conv2D and Dense (lines  26-27).  Finally,  DnnSAT  asserts that the result must fall  between the  lower and upper bounds (line  29).  Note  that  although  the  example  uses  only  integer variables, DnnSAT can easily replace the variable  statements to  support real-valued hyperparameters because  the underlying SMT solver Z3  supports  real  values  and real  functions.</p>
<h2>E. Constraint Solving</h2>
<p>The  resource-guided  configuration  space  reduction  is  formulated as  a constraint  satisfaction problem (CSP).  DnnSAT chooses Microsoft Z3 to solve the constraints because the SMT solver  is  very  efficient  to  handle  higher-order  and  nonlinear functions. However,  the  solving  may  slow down  significantly when dealing with an overlarge configuration  space or a very complicated  restriction  function.  We  summarize  below  our major  optimization  techniques  for  accelerating  the  solving speed:</p>
<ul>
<li>1) Parallel and distributed solving .   DnnSAT partitions the full  configuration  space  into  multiple  smaller  subspaces and  solves them  in  parallel. The  process  is shown schematically  in  Fig.  5.  For  the  parallel  solving,  each worker thread is  assigned  a  standalone Z3  context.  The distributed  solving  is  built  on  top  of Spark  [37],  which handles  configuration  space  partitioning,  distributed  task deployment  (via the mapPartitions API),  scheduling,  and  fault  tolerance.</li>
<li>2) Tiny subspaces . Proper partitioning of the configuration space  is  very  important  for  tackling  the  skew  problem  [3 8 ],  [3 9 ]  in  the  parallel  and  distributed  solving. DnnSAT adopts the idea of tiny tasks [40] and divides the original  space  into  numerous  tiny  subspaces (e.g., each containing  less  than  100  configurations).  Our  approach has  two  advantages:  (1 )  it  will  not result in  observable computation  skew  across  subspaces;  (2)  the  Z3  solver cannot return  all  the  satisfiable  model  configurations  at once  like  what  ALLSAT  [41]  does,  hence  DnnSAT  has to  iteratively  call  Z3  by  providing  the  conjunction  of the negation  of each  existing  solution to  derive  the next  one. A tiny  subspace does not make the conjunction long  and complicated,  thus  the  solving  efficiency  is  significantly improved.  DnnSAT  currently  implements  a  simple  work queue to  manage the  tiny  subspaces.  We  will  consider dynamic  partitioning  and  work  stealing  [4 2 ] for better load balancing  in  the  future.</li>
<li>3) Interval  filtering . The  restriction  function  of  a  constraint  may  be  monotonic  with  regard  to  (w.r.t. for short)  some  hyperparameters.  For  instance,  the  model weight  size  function  is  monotonically  increasing  w.r.t. kernel size, filter size, and  unit size mentioned  in Fig.  4.  Another  example  is  that  the  FLOPs  function is monotonically  increasing  w.r.t.  batch  size  but  not to  kernel  size.  This  is  because  the  output  height  and width  of the Conv2D operator decrease with kernel  size increasing,  which  may  reduce the  FLOPs  of subsequent operators. With  such  monotonicity  information, we can  apply  the  interval  filtering  technique  which  safely discards  specific  value  intervals  of the  hyperparameters. Suppose  that  the  restriction  function  is  monotonically increasing  w.r.t.  the  hyperparameter pi  whose  domain is [v m i n ,  v max ]. If  the  function  value  of  a  configuration (pi = vi ,p 2 = v p 2, ··· ,p m = v p m ) exceeds  the upper bound,  any  configurations  (p1  G   (v1 , v m a x ],p2  = v P 2, · ·   · , p m = v m P )  will not satisfy the constraint either; if it  is  smaller  than  the  lower  bound,  any  configurations (pi g [v m in ,vi ) , p 2 = v p 2, ··· ,p m =  v  m p ) will  also violate the  constraint.  Furthermore, if two  configurations (pi  G   {vi, v2 },p 2  = v p 2, · ·   · ,p m = v p m ) are  satisfied and  vi &lt;  v2 , any  configurations  (pi  G   (vi ,v 2),p 2  = v P 2, · ·   · , p m = v m P ) will  satisfy  the  constraint  as  well.</li>
</ul>
<h2>V. Ev a l u a t i o n</h2>
<p>To  evaluate  the  proposed  DnnSAT,  we  experiment  with different  representative  AutoML benchmarks  and DL models.</p>
<figure><figcaption><div class="caption">Fig.  5:  Parallel  and distributed  solving by partitioning the configuration space.</div></figcaption></figure>
<p>We aim to  answer the  following  Research  Questions  (RQs): RQ1: How  effective  is  DnnSAT  in  speeding  up  AutoML methods?</p>
<p>RQ2: How effective is  DnnSAT  in reducing  the  configuration space?</p>
<p>RQ3: How efficient is  DnnSAT  in  constraint  solving?</p>
<p>Our experiments are conducted on an Azure Standard ND12s virtual  machine  with  12  Intel  Xeon  E5-2690  vCPUs,  224  GB main  memory,  and  2  NVIDIA  Tesla  P40  (24  GB  GDDR5X memory)  GPUs,  running  Ubuntu  16.04.</p>
<p>A. RQ1:  How  effective  is  DnnSAT  in  speeding  up  AutoML methods?</p>
<p>In this  section, we consider the model weight size constraint and  evaluate  the  effectiveness  of  DnnSAT  on  the  following two  benchmarks:</p>
<ul>
<li>1) HPOBench is  for HPO methods  and consists  of 'a large grid  of  hyperparameter  configurations  of  feedforward neural  networks  for  regression'  [13].  The  model  has two tunable Dense operators  followed by a non-tunable Dense on  top.  There  are  nine  hyperparameters  (e.g., batch  size, unit  size,  and initial  learning  rate)  and  6 2 2 0 8 model  configurations  in total.  We use the HPO-BenchProtein dataset.  The  maximal model  weight  size  is  256 KB.</li>
<li>2) NAS-Bench-101 is  for  NAS  methods  and  'constructs  a compact,  yet  expressive,  search  space,  exploiting  graph isomorphisms  to  identify  423k  unique  convolutional architectures'  [14].  The  dataset  is  CIFAR10  [43].  The maximal  model  weight  size  is  about  47.7  MB.</li>
</ul>
<p>Both benchmarks  generated DL models  and collected  a rich set  of runtime  statistics  (final  training/validation/test  error  and accuracy,  total  training  time, etc.) from  the  trained  models. These  statistics  can  be  used  to  simulate  the  execution  of AutoML  trials  and  evaluate  different  configuration  search methods.  To  evaluate  the  learning  performance  of  a  model configuration  resulted  from  an  AutoML  trial,  following  the existing  work  [8 ],  [13],  [14],  [44],  we  use  the  mean  squared error  (MSE)  for  HPOBench  and regret  (i.e., 1 -accuracy) for  NAS-Bench-101  as  the test  measurement.  The  smaller the value,  the  better  the  model.  We  choose  the  model  weight</p>
<p>Fig.  6:  Test measurement curves  and speedups of the HPOBench experiments on  100 trials using  different search methods,  with  an 8  KB  upper bound of the model weight size.</p>
<figure><figcaption><div class="caption">Fig.  7:  Test measurement curves  and  speedups of the NAS-Bench-101  experiments on 500 trials using different search methods, with a 38  MB upper bound of the  model weight size.</div></figcaption></figure>
<figure><figcaption><div class="caption">Fig.  8:  Test measurement curves  and  speedups  of the HPOBench experiments on  100 trials  using Random Search,  with three upper bounds  (2 KB,  8  KB, and  16 KB)  of the model weight size.</div></figcaption></figure>
<p>size  constraint  because  it  is  one  of  the  most  representative constraints  for  DL  applications  and  for  performing  model inference on resource-restricted devices such as mobile phones.</p>
<p>On each benchmark,  we perform two  sets of experiments: baseline  experiments  and  DnnSAT-guided  experiments.  We choose  the  following  commonly  used  search  methods  in</p>
<p>AutoML as baselines:  Random  Search  (RS)  [15],  Regularized Evolution  (RE)  [16],  Hyperband  (HB;  HPOBench  only)  [2], and  Reinforcement Learning  (RL;  NAS-Bench-101  only)  [14]. The two sets of experiments perform the same number of trials (100  for  HPOBench  and  500  for  NAS-Bench-101).  We then measure the  speedup  achieved  by  DnnSAT  over each  baseline method on each benchmark. The speedup is calculated as  2 base  , T D n n S A T where Tbase is  the  estimated time of a baseline (i.e., RS,  RE, and HB) reaching the lowest test measurement (i.e., the best learning  performance)  and TD n n S A T   is  the  time  of a  DnnSATguided method reaching the same or lower test measurement. We repeat each  experiment  10  times  and compute the average speedup  value.  DnnSAT runs  in the interactive mode because of the  simplicity  of the  integration  effort.  To  solve  the  first model  configuration,  DnnSAT  needs  to  build  the  constraint from  scratch (i.e., warm-up),  which  spends  more  time  than solving  later individual  configurations  of the  same model.  For HPOBench, the solving time is 0.1s (warm-up: 2.5s) on average. For NAS-Bench-101, it is  0.6s  (warm-up:  14.0s)  on  average.</p>
<p>Fig.  6   demonstrates  the test measurement (MSE) curves  of HPOBench on  100  evaluated trials.  The  upper bound  of the</p>
<p>model weight size is  set to  a  small  value  of 8   KB  (which  is set  for  deploying  KB-sized DL models  to  resource-restricted IoT  devices  [45]).  The x-axis  is  the wall-clock time,  and the y-axis  denotes the test measurement value.  Overall,  DnnSAT achieves  a  speedup  of  2.04X  (RS),  1.19X  (RE),  and  3.95X (HB)  on  HPOBench,  for  obtaining  the  same  optimal  model learning performance that can  be found by  the baseline.  From the  experiment  results,  we  also  find  that  DnnSAT  helps  the curves  go  down  faster  and  reach  smaller  test  measurement values, which means that better model configurations are found. The  reason  is  that  DnnSAT  reduces  the  configuration  space so  that  HPOBench can perform a  much  more efficient search than  before.  We  also  notice  that  the  experiment  time  has  been observably  shortened when DnnSAT is enabled since a smaller model  size implies  fewer  FLOPs  and thus  less  training  time. In  Fig.  6 c,  time  reduction  is  particularly  significant  because of the mechanism of Hyperband:  the more resources  saved by DnnSAT, the more training budget allocated to Hyperband for high-efficiency  search.</p>
<p>Fig.  7  demonstrates  the test measurement  (regret)  curves  of NAS-Bench-101  on 500 evaluated trials, with the upper bound of the  model  weight  size  set  to  38  MB.  DnnSAT  achieves  a speedup  of  1.23X  (RS),  1.19X  (RE),  and  1.52X  (RL).</p>
<p>To understand the generality of our approach under different constraint bounds,  we  additionally  choose two  upper bounds (2  KB  and  16  KB)  and conduct HPOBench experiments using Random  Search.  The  results  of  Fig.  8   and  Fig.  6 a  indicate that  DnnSAT  is  generally  effective  and  achieves  a  speedup of  4.15X  (2  KB),  2.04X  (8   KB),  and  1.21X  (16  KB).  The results  also  show that the  stricter the  constraint,  the  greater the improvement achieved by DnnSAT.</p>
<h2>B. RQ2: How effective is DnnSAT in reducing the configuration space?</h2>
<p>In  this  section,  we  evaluate  the  reduction  effectiveness  of DnnSAT on real-world DL models. We choose two representative  models  as  our  experimental  subjects,  namely  VGG-16 (VGG  model  with  16  layers) [17] and  LSTM  [18]-based Seq2Seq  [19].  For  VGG-16,  we  select  batch  size  (interval [1,256]), kernel size (1, 3, and 5), and unit size (128, 512,  1024, 4096, and 10240) as the hyperparameters; hence there are 3840 model configurations  in  total.  For Seq2Seq,  we  consider batch size  (interval  [128, 512])  and hidden  size  (interval  [16,128]), in  which  the  configuration  space  consists  of  43505  model configurations.</p>
<p>We apply all the  four discussed constraints  separately  and collectively  to  both  DL  models.  Each  constraint  is  set  with several upper bounds.  The bound choices  are based on actual conditions  such  as  the  model  scale,  device  capability,  and application  SLA.  For  example,  we  choose  6 ,  8 ,  and  12  GB for  VGG-16 under the GPU memory consumption constraint because  they  correspond to  three  typical memory  capacities  of NVIDIA GPUs.  Since the two models differ a  lot,  we cannot use the  same bound value  for both.  Note that we use batched inference time, that is, the total inference time of a batch of data items.  Specific  upper bound values  can be found in Table III.</p>
<p>TABLE III Co n f i g u r a t i o n Sp a c e Re d u c t i o n o n Re a l -Wo r l d   DL Mo d e l s</p>
<p>.</p>
<table><tbody><tr><th>Model Name</th><th>M ax Weight Size (MB)</th><th>M ax GPU Memory (GB)</th><th>M ax BI Tim e (S)</th><th>Max FLOPs (G FLOPs)</th><th>All Constraints</th></tr><tr><td>VGG-16</td><td>1024 (80.0%)</td><td>12 (84.3% )</td><td>1000 (80.3%)</td><td>4096 (58.4% )</td><td>(53.4%)</td></tr><tr><td></td><td>512 (60.0%)</td><td>8 (56.2% )</td><td>800 (64.0%)</td><td>3584 (51.1% )</td><td>(31.8%)</td></tr><tr><td></td><td>128 (0.0%)</td><td>6 (42.1% )</td><td>10 (0.7%)</td><td>3072 (43.7% )</td><td>(0.0%)</td></tr><tr><td>Seq2Seq (LSTM)</td><td>128 (32.1%)</td><td>0.6 (34.7% )</td><td>50 (74.1%)</td><td>64 (35.2% )</td><td>(24.1%)</td></tr><tr><td></td><td>64 (18.0%)</td><td>0.4 (15.3% )</td><td>30 (67.2%)</td><td>4 (26.0% )</td><td>(1.6%)</td></tr><tr><td></td><td>32 (5.0%)</td><td>0.2 (0.0%)</td><td>(33.2%)</td><td>(4.2%)</td><td>(0.0%)</td></tr></tbody></table>
<p>Note:  ' B I  T im e "   s ta n d s   f o r  b a tc h e d   in f e r e n c e   tim e ,  w h ic h   c a lc u la te s   t h e   to ta l i n f e r e n c e   t i m e   o f   a  b a t c h   o f   d a ta   i te m s . T h e   tw o   v a lu e s   in a  c e ll  r e p r e s e n t   t h e u p p e r   b o u n d a n d   s p a c e   r a t i o ,   r e s p e c t i v e l y .</p>
<p>The space  ratio (SR) is used  to assess the  reduction effectiveness  of DnnSAT.  Suppose  that A and A DnnS A T   are the  original  and  reduced  configuration  spaces,  respectively. Then,  SR = x 100%.  A  smaller  SR means  a  stronger reduction effect. The promising experimental results in Table III demonstrate that DnnSAT is  effective  in  configuration  space reduction.  For  example,  the  SR  of VGG-16  under  the  GPU memory consumption  constraint ranges  from 42.1%  to  84.3%. Meanwhile, the  SR of Seq2Seq under the batched inference (BI)  time  constraint  ranges  from  33.2%  to  74.1%.  Tighter bounds  or  a  combination  of  multiple  constraints  will  lead to  a  more  significant  reduction.  The  results  can  also  give hints  to  developers  and  help  them  choose  optimal  settings of neural  architectures,  hyperparameters,  and  computational resources.  For instance,  the  SR  of VGG-16  equals  0%  when the  upper  bound  of  the  model  weight  size  is  set  to 128 MB,  which  indicates  that  such  a  model  may  not be  further dwindled by simply adjusting  the hyperparameters.  Therefore, developers  need  to  look  for  advanced  DL  techniques  ( e.g. , model compression [32])  to  embed it into  a resource-restricted application.  After  applying  four  constraints  collectively,  the SR value further decreases  and is below the  minimum of the one-constraint SR values.  The results demonstrate  the  stronger reduction effect when adopting multiple constraints collectively.</p>
<p>DnnSAT runs in both interactive and pruning modes with  12 threads, tiny subspaces containing  1 0  model configurations, and interval filtering being off. We show the runtime performance of DnnSAT under one constraint and four constraints  as  follows:</p>
<ul>
<li>1) Interactive  mode.  For  VGG-16,  the  solving  time  per configuration is 0.10s  (warm-up:  6.13s) on average under one  constraint,  and 0.13s  (warm-up:  6.18s)  under  four constraints. For Seq2Seq, the corresponding time is 0.05s (warm-up:  1.40s) on average and 0.12s (warm-up:  1.44s), respectively.</li>
<li>2) Pruning  mode. For  VGG-16,  DnnSAT  spends  226s on  average  under  one  constraint  and  283s  under  four constraints.  For Seq2 Seq, the corresponding time is  806s on  average and  1301s,  respectively.</li>
</ul>
<h2>C. RQ3:  How efficient is  DnnSAT in  constraint solving?</h2>
<p>In  this  section,  we  evaluate  the  solving  efficiency  of  the optimization  techniques proposed in  Section  IV-E.  We choose the  LSTM-based  Seq2Seq  model  with  batch  size  (interval [1,4800]) and hidden size (interval  [16, 2 0 ]) as the hyperparameters. The configuration  space then  consists  of 24000 model configurations  in total.  To  increase the  solving  difficulty,  we use a loose  FLOPs constraint under which  every configuration satisfies.</p>
<p>A series  of experiments  are conducted with  different thread numbers  (1,  4,  8 ,  and  12),  subspace  sizes  (10,  50,  100,  500, 1000, and equipartition), and interval filtering settings  (ON and OFF).  We do  not create  more threads  since the experimental machine has  only  12  vCPUs.  'Equipartition'  means that we divide the original  configuration  space equally by the number of threads  and do  not further  split  it into  tiny  subspaces.  For example,  in  the case of  1 2   threads,  each  thread  independently solves  a  subspace of 2 0 0 0   configurations.</p>
<p>Table IV  shows the end-to-end execution time  (in  seconds) and  speedup  relative  to  the  baseline  experiment  (1  thread, equipartition,  and  interval  filtering  being  off).  The  speedup ranges  from 7.6X  to  17892.1X,  confirming  the  strong  effectiveness of our optimizations. Simply increasing  the number of threads  achieves  an  ultra-linear  speedup from 9.3X to  51.9X because  a  smaller  configuration  space  notably  reduces  the overhead of ALLSAT  solving  (Section  IV-E).  Tiny  subspaces achieve  a  speedup  from  7.6X  to  83.0X  in  the  experiments that  turn  off  interval  filtering,  with  the  same  reason  as  the parallel  solving.  Nevertheless,  as  the  subspace  size  getting smaller,  the  speedup  grows  slowly  and  then  drops  (from the  size  of  50)  because  the  overhead  of  ALLSAT  solving is  no  longer  noticeable  while  the  management  cost  of tiny subspaces  increases.  Interval  filtering  demonstrates  dramatic improvements  in  the  equipartition  experiments  and  achieves a  maximal  speedup  of  17892.1X.  The reason is that DnnSAT divides  the  original  configuration  space  on  hidden  size  to keep  as  long  a  continuous  interval  of batch  size  as  possible since FLOPs  is monotonically  increasing  with regard  to  batch size. Therefore,  DnnSAT  solves  only  a  small  number  of configurations  to reach  the  conclusion  that the  entire  subspace satisfies  the  constraint.  However,  if  there  exist  many  short intervals  of batch  size (e.g., the  domain  of batch  size  is  small or tiny  subspaces  are used),  the effect  of interval  filtering  will not be  so  significant.</p>
<h2>VI.  Dis c u s s i o n</h2>
<h2>A. Extensibility of DnnSAT</h2>
<p>Currently,  DnnSAT  supports  70+  commonly  used  operators. DnnSAT is extensible, and users can incorporate new operators and  constraints.  To  add  a new  operator,  users  formulate  the analytic resource cost functions based on its semantics and then implement the SMT-LIB templates. To support a new constraint, users  formulate the analytic restriction  function using  defined hyperparameters and carry out the above operator-adding  steps for  each  of the  operators  under consideration.  Besides,  users</p>
<p>TABLE IV</p>
<table><caption><div class="caption">Ru n t i m e Pe r f o r m a n c e o f Dn n SAT u n d e r t h e FLOPs  Co n s t r a i n t Wit h   Op t i m i z a t i o n Te c h n i q u e s o f Pa r a l l e l So l v i n g ,   T i n y Su b s p a c e s , a n d In t e r v a l Fi l t e r i n g .</div></caption><tbody><tr><th rowspan="3">Subspace Size</th><th colspan="8">Number of Threads</th></tr><tr><th colspan="4">Interval Filtering OFF</th><th colspan="4">Interval Filtering ON</th></tr><tr><td></td><td></td><th>8</th><th>12</th><td></td><td></td><th>8</th><th>12</th></tr><tr><td>Equipartition</td><td>10 . (19681.0s)</td><td>9.3 (2116.2s)</td><td>26.7 (737.1s)</td><td>51.9 (379.2s)</td><td>17892.1 (1.1s)</td><td>17,730.6 (1.11s)</td><td>17,572.3 (1.12s)</td><td>15,496.8 (1.27s)</td></tr><tr><td>10 00</td><td>88 . (2236.4s)</td><td>28.9 (681.0s)</td><td>50.3 (391.2s)</td><td>68.5 (287.3s)</td><td>1640.1 (12.0s)</td><td>5046.4 (3.9s)</td><td>7872.5 (2.5s)</td><td>9840.6 (2.0s)</td></tr><tr><td>500</td><td>10.5 (1874.3s)</td><td>33.7 (584.0s)</td><td>57.0 (345.2s)</td><td>77.0 (255.6s)</td><td>841.0 (23.4s)</td><td>2659.6 (7.4s)</td><td>4100.2 (4.8s)</td><td>5319.2 (3.7s)</td></tr><tr><td>10 0</td><td>1. 21 (1626.5s)</td><td>38.9 (505.9s)</td><td>63.2 (311.4s)</td><td>83.0 (237.1s)</td><td>169.8 (115.9s)</td><td>560.7 (35.1s)</td><td>882.5 (22.3s)</td><td>1063.8 (18.5s)</td></tr><tr><td>50</td><td>1.6 1 (1696.6s)</td><td>37.6 (523.4s)</td><td>60.0 (328.0s)</td><td>76.0 (258.9s)</td><td>85.2 (231.0s)</td><td>258.2 (76.2s)</td><td>441.2 (44.6s)</td><td>533.3 (36.9s)</td></tr><tr><td>1 0</td><td>7.6 (2589.6s)</td><td>25.7 (765.8s)</td><td>40.0 (492.0s)</td><td>48.9 (402.4s)</td><td>17.1 (1150.9s)</td><td>57.3 (343.4s)</td><td>89.5 (219.9s)</td><td>107.1 (183.7s)</td></tr></tbody></table>
<p>Note:  The two values  in  a cell represent the  speedup  and execution time  (in  seconds).</p>
<p>may need to reimplement the  graph  traversal to  compute more accurate current resource consumption by employing additional information,  including  visited  operators,  edges,  and  previously calculated resource consumption.</p>
<h2>B. Threats  to  Validity</h2>
<p>We discuss the following threats to the  validity  of our work:</p>
<ul>
<li>1) Resource  cost functions . We examine the source code of  frameworks  to  extract  the  resource  cost  functions of DL operators  for inferring resource  usage.  However, the  implementation  of  operators  can  call  proprietary NVIDIA CUDA, cuDNN, or cuBLAS  APIs, which may introduce  some  fluctuations  in  the  cost  functions.  For example, cudnnConvolutionForward() could use temporary  GPU memory  called workspace to  improve the  runtime  performance.  Nevertheless,  the  workspace size  is  convolution  algorithm-dependent  and thus  unpredictable. We mitigated this threat by refining the resource cost  functions  after  carefully  referring  to  the  NVIDIA development documentation,  dynamically profiling  the APIs using NVIDIA nvprof ,  and analyzing the framework runtime logs.</li>
<li>2) Hidden  factors . There  are  a number  of hidden  framework factors that can observably  affect the GPU memory consumption  and  inference  time  of  a  DL  model.  For example,  the  GPU  memory  consumption  has  complicated dependencies  on  the allocation policy (e.g., tensor fragmentation,  alignment,  garbage collection,  and reservation),  internal  usage  ( e.g. ,  cUDA  context),  operator scheduling, etc. To  mitigate  this  threat,  we  referred  to the framework source code carefully to  identify hidden factors. However,  it  is very  challenging  to  directly formulate  all  such  factors (e.g., garbage  collection  and reservation)  analytically.  Hence,  DnnSAT  conservatively calculates  the  resource  usage  to  reduce  the  influence of hidden  factors.  For  instance,  DnnSAT  adopts  a  simplified  yet  common  approach  to  accumulate  the  GPU memory required by each operator under only forward propagation  (Section  IV-B),  which  computes  a  smaller value  than  the actual  GPU memory  consumption.  If the computed value (a conservative value)  already  exceeds</li>
</ul>
<p>the GPu memory upper bound, the corresponding model configuration  indeed  does  not  satisfy  the  constraint. Therefore,  valid  (satisfiable)  model  configurations  will not be discarded.  However,  some  invalid  (unsatisfiable) model configurations may not be eliminated correctly due to  the  inaccuracy  in  the  calculation  of hidden  factors.  In the experiment on VGG-16 (Section V-B), we notice that on  average  9.53%  of model  configurations  are actually invalid under four constraints yet passed DnnSAT, but no valid model  configurations  are discarded.  In the  future, we  will  identify  more hidden  factors  and  design  more accurate  restriction  functions  to  obtain  more  precise results.</p>
<ul>
<li>3) SMT solving . The simple and non-intrusive integration with  existing  AutoML  tools  is  to  run  DnnSAT  in  the interactive mode.  According to  the  experimental results in  Section V, the cost of solving one model configuration is  very  low.  In  the pruning  mode,  it takes  a  longer  time for  constraint  solving  because of the large configuration space.  For example,  Table IV  in  SectionV-C  shows  that DnnSAT  spends  19681  seconds  (about  5.5  hours)  to solve  the  24000-configuration  space  of  Seq2Seq.  We currently propose  some effective optimization techniques in  Section  IV-E to  increase the  scalability  of constraint solving:  (1)  DnnSAT  supports  parallel  and  distributed solving  (via  Spark),  in  which  the  full  configuration space can be partitioned into any number of independent subspaces  and  solved concurrently by  different threads and  machines;  (2 )  the  use  of  tiny  subspaces  reduces the  complexity  of solving  individual  subspaces,  removes the  computation  skew  across  subspaces,  balances  the workload among threads,  and thus  avoids  stragglers  ( i.e. , threads that take an unusually long time to finish) [40]; (3) interval filtering can further reduce the solving complexity significantly if the restriction  function  of a constraint  is monotonic  with  regard  to  some  hyperparameters.  The experimental  results  in  Table  IV  confirm  the  strong effectiveness of  our  optimizations. However,  as  the number of hyperparameters and their domains increase, the configuration  space  can  enlarge exponentially,  thus the computational  complexity  may still exceed  the capabilities  of an  SMT  solver.  In  addition to  advancing the  solvers, it is possible to tackle this problem by trying larger-scale distributed solving with better load balancing.</li>
</ul>
<h2>VII. Re l a t e d W o r k</h2>
<p>Many software systems are highly configurable by providing a  rich set of  configuration  options. Configuration  options are  also  considered  as  features  in  the  software  product  line context.  However,  it is  very  time-consuming  and error-prone for  manual  configuration  tuning  due  to  a  large  number  of option  combinations.  Software  engineering  researchers  have proposed  various  approaches  to  predicting  the  performance of  configurable  systems  [44],  [46],  [47],  checking  the  consistency  of configurations  [48]-[51],  and understanding  how configuration  options  and their interactions  influence  system performance  [52], [53]. Like  traditional  software  systems, DL  models  are  also  highly  configurable.  In  this  work,  we analyze DL models and propose to optimize the configuration exploration (AutoML) through resource-guided space reduction.</p>
<p>Google  Vizier  [54]  and  Microsoft  HyperDrive  [55]  are representative AutoML systems, which concentrate more on the system design  and operation.  Hyperband  [2]  is an  HPO  search method and focuses  on  speeding  up  random  search  through adaptive resource allocation and early stopping. ENAS  [3] uses a  controller to  discover various  neural  architectures  and search for the  optimal one.  These methods and systems are  not aware of the constraints  imposed by computational resources,  which can cause  an  expensive waste of shared resources.  Our work can help them efficiently reduce  the  configuration  space  ahead of time  and accelerate training.</p>
<p>The  authors  of  [56], [57]  analyzed  the  resource  budget constraint for HPO.  However, they encoded the budget into the algorithm instead of formulating  explicit resource constraints, and  thus  their  method  cannot  be  applied  to  other  AutoML methods  directly.  Hernandez-Lobato et  al. [58]  considered constraints  for  Bayesian  Optimization,  but the  work  is  for  a specific algorithm.  Gordon et al. [59]  proposed an approach to automate the design of neural structures via a resource-weighted sparsifying regularizer. AMS [60] generated the AutoML search space  from  an  unordered  set  of API  components.  Our  work formulates  common constraints imposed by resources and uses a unified analytic approach to eliminate the unsatisfiable model configurations  in advance.</p>
<p>There have been  many program analysis methods  [61]-[64] to  determine the quantitative resource usage (e.g., memory and heap  space)  of computer programs.  For example,  Hoffmann et al. [61] used the automatic amortized resource analysis (AARA) technique  in  analyzing  the  worst-case resource  consumption of arbitrary multivariate polynomial  functions.  Jost et al. [64] employed a type-based approach by exploiting  linearity  and focusing  on  the  reference  number to  an  object.  However,  such work  usually  targets  higher-order  functional  programs  and cannot be  directly applied to  DL models because of the wide differences in the representation  structures.  Our work proposes an  analytic  and  framework-independent  cost  model  to  infer the  resource consumption  of a DL model and utilizes  an  SMT solver to  obtain  all  the  satisfiable  model  configurations.</p>
<h2>VIII. Co n c l u s i o n</h2>
<p>In this paper, we have presented DnnSAT, a resource-guided AutoML  approach  for  deep  learning  models  to  efficiently reduce the configuration  space under computational constraints. Powered  by  DnnSAT,  we  demonstrate  that  commonly  used AutoML  methods  can  efficiently  prune  unsatisfiable  model configurations ahead of time to avoid unnecessary training costs and achieve significant speedups. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.</p>
<h2>Re f e r e n c e s</h2>
<p>[1] NNI,  'Nni  (neural  network  intelligence):  a  lightweight  but  powerful toolkit to help users  automate feature engineering,  neural  architecture</p>
<ul>
<li>search,  hyperparameter tuning  and model  compression.'  https://github. com/microsoft/nni,  2019.</li>
<li>[2] L.  Li,  K.  Jamieson,  G.  DeSalvo,  A.  Rostamizadeh,  and A.  Talwalkar, 'Hyperband: A novel bandit-based approach to hyperparameter optimization,' J.  Mach.  Learn.  Res., vol.  18,  no.  1,  pp.  6765-6816,  Jan. 2017.  [Online].  Available:  http://dl.acm.org/citation.cfm?id=3122009. 3242042</li>
<li>[3] H.  Pham,  M.  Y.  Guan,  B.  Zoph,  Q.  V.  Le,  and  J.  Dean,  'Efficient neural  architecture  search  via parameter  sharing,' CoRR, 2018.  [Online]. Available:  http://arxiv.org/abs/1802.03268</li>
<li>[4] H.  Jin, Q.  Song,  and  X.  Hu, 'Efficient  neural architecture  search with network morphism,' CoRR, vol.  abs/1806.10282,  2018.  [Online]. Available:  http://arxiv.org/abs/1806.10282</li>
<li>[5] K. He, X. Zhang, S.  Ren,  and J.  Sun,  'Deep residual learning for image recognition,'  in 2016 IEEE  Conference on  Computer Vision  and Pattern Recognition  (CVPR), June  2016,  pp.  770-778.</li>
<li>[6] Y. Gao,  Y.  Liu,  H.  Zhang,  Z.  Li,  Y.  Zhu,  H.  Lin, and  M.  Yang, Estimating  GPU  Memory  Consumption  o f  Deep  Learning  M odels, ser. ESEc/FSE 2020. New York, NY, USA:  Association for Computing  Machinery, 2020, pp. 1342-1352. [Online]. Available: https://doi.org/10.1145/3368089.3417050</li>
<li>[7] R. Zhang, W. Xiao, H. Zhang, Y. Liu, H. Lin, and M. Yang, ' An  empirical  study  on  program  failures  of  deep  learning  jobs,'  in Proceedings  o f  the  ACM /IEEE  42nd  International Conference on Software Engineering, ser.  ICSE  '20. New York, NY, USA: Association for  Computing  Machinery,  2020,  pp.  1159-1170.  [Online].  Available: https://doi.org/10.1145/3377811.3380362</li>
<li>[8] H. cai, L.  Zhu,  and S.  Han,  'Proxylessnas: Direct neural  architecture search on target task and hardware,' CoRR, vol.  abs/1812.00332,  2018. [Online].  Available:  http://arxiv.org/abs/1812.00332</li>
<li>[9] C.  Hsu,  S.  Chang,  D.  Juan,  J.  Pan,  Y.  Chen,  W.  Wei,  and  S.  Chang, 'MONAS: multi-objective neural architecture search using reinforcement learning,' CoRR , vol. abs/1806.10332, 2018. [Online]. Available: http://arxiv.org/abs/1806.10332</li>
<li>[10] M. Tan  and  Q.  Le, 'EfficientNet: Rethinking  model scaling  for convolutional neural networks,'  in Proceedings  o f the 36th International Conference on Machine Learning , ser.  Proceedings of Machine Learning Research,  K.  Chaudhuri  and  R.  Salakhutdinov,  Eds.,  vol.  97. Long Beach,  California,  USA:  PMLR,  09-15  Jun  2019,  pp.  6105-6114. [Online].  Available:  http://proceedings.mlr.press/v97/tan19a.html</li>
<li>[11] S.  Russell  and P.  Norvig, Artificial Intelligence: A   M odern Approach, 3rd ed. USA:  Prentice Hall Press,  2009.</li>
<li>[12] L. De Moura and N. Bj0rner, 'Z3: An efficient smt solver,' in Proceedings o f  the  Theory  and Practice  o f Software,  14th  International  Conference on  Tools  and Algorithms fo r  the  Construction  and Analysis  o f Systems, ser.  TACAS  '08/ETAPS  '08. Berlin, Heidelberg:  Springer-Verlag, 2008, pp.  337-340.</li>
<li>[13] A.  Klein  and  F.  Hutter,  'Tabular  benchmarks  for  joint  architecture and  hyperparameter  optimization,' CoRR , 2019.  [Online].  Available: http://arxiv.org/abs/1905.04970</li>
<li>[14] C.  Ying,  A.  Klein,  E.  Real,  E.  Christiansen,  K.  Murphy,  and F.  Hutter, 'Nas-bench-101: Towards reproducible neural architecture search,' CoRR, 2019.  [Online].  Available:  http://arxiv.org/abs/1902.09635</li>
<li>[15] J. Bergstra and  Y. Bengio, 'Random  search  for  hyper-parameter optimization,' Journal o f Machine Learning Research, vol.  13,  no.  10, pp. 281-305, 2012. [Online]. Available:  http://jmlr.org/papers/v13/ bergstra12a.html</li>
<li>[16] E.  Real,  A.  Aggarwal,  Y .  Huang,  and Q.  V .  Le,  'Regularized evolution for image classifier architecture search,' CoRR ,   2018. [Online]. Available: http://arxiv.org/abs/1802.01548</li>
<li>[17] K.  Simonyan  and  A.  Zisserman,  'Very  deep  convolutional  networks for  large-scale image recognition,'  in 3rd International  Conference  on Learning Representations,  ICLR  2015,  San Diego,  CA,  USA,  M ay  7-9, 2015,  Conference  Track Proceedings, Y.  Bengio  and  Y .  LeCun,  Eds., 2015.  [Online].  Available:  http://arxiv.org/abs/1409.1556</li>
<li>[18] S.  Hochreiter  and J.  Schmidhuber,  'Long  short-term memory,' Neural Comput., vol.  9,  no.  8,  pp.  1735-1780,  Nov.  1997.  [Online].  Available: https://doi.org/10.1162/neco.1997.9.8.1735</li>
<li>[19] I.  Sutskever,  O.  Vinyals,  and  Q.  V .  Le,  'Sequence to  sequence  learning with neural networks,'  in Proceedings  o f the  27th International  Conference  on Neural  Information  Processing  Systems  -  Volume  2, ser.  NIPS' 14. Cambridge,  MA, USA:  MIT Press,  2014, pp.  3104-3112.</li>
<li>[20] I.  Goodfellow, Y .  Bengio,  and A. Courville, Deep Learning . MIT Press, 2016, http://www.deeplearningbook.org.</li>
<li>[21] M.  Abadi,  P. Barham, J. Chen, Z. Chen,  A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker,  V. Vasudevan, P. Warden, M.  Wicke, Y. Yu, and X.  Zheng,  'Tensorflow:  A  system for  large-scale  machine  learning,' in 12th USENIX  Symposium on Operating Systems Design and Implementation  (OSDI  16), 2016,  pp.  265-283.  [Online].  Available: https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf</li>
<li>[22] A. Paszke,  S.  Gross,  F.  Massa,  A.  Lerer,  J.  Bradbury,  G.  Chanan, T. Killeen, Z.  Lin, N. Gimelshein, L. Antiga, A. Desmaison, A.  Kopf,  E.  Yang,  Z.  DeVito,  M.  Raison,  A.  Tejani,  S.  Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, 'Pytorch: An imperative style, high-performance deep learning library,' in Advances  in  Neural  Information  Processing  Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, Eds., vol. 32. Curran Associates,  Inc., 2019, pp. 8026-8037.  [Online].  Available:  https://proceedings.neurips.cc/paper/ 2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf</li>
<li>[23] T. Chen, M.  Li, Y. Li, M.  Lin, N. Wang, M.  Wang, T. Xiao, B. Xu, C. Zhang, and Z.  Zhang, 'MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems,' CoRR, vol. abs/1512.01274, 2015. [Online]. Available: http://arxiv.org/abs/1512.01274</li>
<li>[24] F.  Chollet et al., 'Keras,' https://keras.io,  2015.</li>
<li>[25] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and F. Hutter, 'Efficient and robust automated machine  learning,'  in Advances  in  Neural  Information  Processing Systems , C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, Eds., vol. 28. Curran Associates,  Inc., 2015, pp. 2962-2970.  [Online].  Available:  https://proceedings.neurips.cc/paper/ 2015/file/11d0e6287202fced83f79975ec59a3a6-Paper.pdf</li>
<li>[26] B.  Zoph  and Q.  V.  Le,  'Neural  architecture  search with reinforcement learning,' CoRR, vol. abs/1611.01578, 2016. [Online]. Available: http://arxiv.org/abs/1611.01578</li>
<li>[27] R.  M. Larsen and T. Shpeisman, 'Tensorflow graph optimizations,' 2019.</li>
<li>[28] PyTorch,  'The  topological  sorting  algorithm  for  computation  graphs in pytorch,' https://github.com/pytorch/pytorch/blob/v1.2.0/caffe2/core/ nomnigraph/include/nomnigraph/Graph/TopoSort.h#L26,  2019.</li>
<li>[29] A.  MXNet, 'The topological  sorting  algorithm  for computation graphs in apache mxnet,' https://github.com/apache/incubator-mxnet/blob/ 4149f8b8752989fce5d80cc13f92d99774988b4f/src/executor/simple_ partition_pass.h#L67,  2019.</li>
<li>[30] H.  B.  Enderton, Elements  o f Set  Theory, H.  B.  Enderton,  Ed. San Diego:  Academic Press,  1977.</li>
<li>[31] C.  Barrett,  P .  Fontaine,  and C.  Tinelli,  'The  SMT-LIB  Standard:  Version 2.6,' Department of Computer Science,  The University of Iowa,  Tech. Rep.,  2017,  available  at www.SMT-LIB.org .</li>
<li>[32] S.  Han,  H.  Mao,  and  W.  J.  Dally,  'Deep  compression:  Compressing deep  neural  network  with  pruning,  trained  quantization  and  huffman coding,'  in 4th  International  Conference  on  Learning  Representations, ICLR  2016,  San Juan,  Puerto Rico,  M ay  2-4,  2016,  Conference  Track Proceedings, Y .  Bengio  and Y .  LeCun,  Eds.,  2016.  [Online].  Available: http://arxiv.org/abs/1510.00149</li>
<li>[33] R.  Tang,  W.  Wang,  Z.  Tu,  and  J.  Lin,  ' An  experimental  analysis  of the power consumption of convolutional neural networks for keyword spotting,'  in 2018 IEEE International  Conference  on Acoustics,  Speech and Signal Processing  (ICASSP), 2018,  pp.  5479-5483.</li>
<li>[34] PyTorch, 'Pytorch conv2d api,' https://pytorch.org/docs/stable/generated/ torch.nn.Conv2d.html.</li>
<li>[35] H. Qi, E. R. Sparks,  and A. Talwalkar, 'Paleo: A performance model for deep neural networks,' in Proceedings  o f the  International  Conference on Learning Representations, 2017.</li>
<li>[36] L. Wang, J. Ye, Y . Zhao, W. Wu, A. Li, S. L. Song, Z. Xu,  and T. Kraska, 'Superneurons:  Dynamic  gpu  memory  management for  training  deep neural networks,'  in Proceedings o f the 23rd ACM  SIGPLAN Symposium on  Principles  and Practice  o f Parallel Programming, ser.  PPoPP  '18. New York, NY, USA: Association for Computing Machinery, 2018, pp. 41-53.  [Online]. Available:  https://doi.org/10.1145/3178487.3178491</li>
<li>[37] M.  Zaharia,  R.  S.  Xin,  P.  Wendell,  T.  Das,  M.  Armbrust,  A.  Dave, X.  Meng,  J.  Rosen,  S.  Venkataraman,  M.  J.  Franklin,  A.  Ghodsi, J.  Gonzalez,  S.  Shenker,  and I.  Stoica,  'Apache spark: A unified engine for  big  data  processing,' Commun. ACM , vol.  59,  no.  11,  pp.  56-65, Oct.  2016.  [Online].  Available:  https://doi.org/10.1145/2934664</li>
</ul>
<ul>
<li>[38] G.  Ananthanarayanan,  S. Agarwal,  S.  Kandula, A.  Greenberg,  I.  Stoica, D.  Harlan, and  E. Harris, 'Scarlett: Coping  with  skewed  content popularity in mapreduce clusters,' in Proceedings o f the Sixth Conference on Computer  Systems, ser. EuroSys '11. New  York,  NY,  USA: Association  for  Computing  Machinery,  2011,  pp.  287-300.  [Online]. Available:  https://doi.org/10.1145/1966445.1966472</li>
<li>[39] Y. Kwon, M. Balazinska, B. Howe, and J. Rolia, 'Skewtune: Mitigating skew in mapreduce applications,' in Proceedings o f the  2012  A C M   S1GMOD  International  Conference  on  Management o f Data, ser. SIGMOD  '12. New  York,  NY,  USA:  Association for Computing Machinery, 2012, pp. 25-36. [Online]. Available: https://doi.org/10.1145/2213836.2213840</li>
<li>[40] K. Ousterhout, A. Panda, J. Rosen, S. Venkataraman, R. Xin, S.  Ratnasamy,  S.  Shenker,  and  I.  Stoica,  'The  case  for  tiny  tasks  in compute clusters,' in 14th  Workshop  on Hot Topics  in  Operating Systems (HotOS  X1V). Santa  Ana  Pueblo,  NM:  USENIX  Association,  May 2013.  [Online].  Available:  https://www.usenix.org/conference/hotos13/ session/ousterhout</li>
<li>[41] T. Toda and T. Soh, 'Implementing efficient all solutions SAT solvers,' CoRR, vol. abs/1510.00523, 2015. [Online]. Available: http://arxiv.org/abs/1510.00523</li>
<li>[42] R. D. Blumofe and C. E. Leiserson, 'Scheduling multithreaded computations by work  stealing,' Journal  o f the ACM , vol.  46,  no.  5,  pp. 720-748, Sep.  1999.  [Online].  Available:  https://doi.org/10.1145/324133. 324234</li>
<li>[43] A.  Krizhevsky,  'Learning  multiple layers of features  from tiny  images,' University  of Toronto,  Tech. Rep.,  2009.</li>
<li>[44] H. Ha and H. Zhang, 'Deepperf: Performance prediction for configurable software with deep sparse neural network,' in Proceedings o f the 41st International Conference on Software Engineering, ser. ICSE  '19. IEEE  Press,  2019,  pp. 1095-1106.  [Online].  Available: https://doi.org/10.1109/ICSE.2019.00113</li>
<li>[45] S.  Gopinath,  N.  Ghanathe,  V.  Seshadri,  and  R.  Sharma,  'Compiling kb-sized machine learning models  to tiny  iot devices,'  in Proceedings o f   the  40th  A C M   S1GPLAN  Conference  on  Programming  Language Design and 1mplementation , ser.  PLDI  2019. New  York,  NY,  USA: Association  for  Computing  Machinery,  2019,  pp.  79-95. [Online]. Available:  https://doi.org/10.1145/3314221.3314597</li>
<li>[46] N. Siegmund,  S.  S. Kolesnikov, C. Kastner, S. Apel,  D. Batory, M. Rosenmüller,  and G.  Saake,  'Predicting performance via automated feature-interaction detection,'  in Proceedings  o f the  34th  International Conference  on  Software Engineering, ser.  ICSE  '12. IEEE Press,  2012, pp.  167-177.</li>
<li>[47] J. Guo, D. Yang, N. Siegmund, S. Apel, A. Sarkar, P. Valov, K.  Czarnecki,  A.  Wasowski,  and  H.  Yu,  'Data-efficient  performance learning  for  configurable  systems,' Empirical  Softw.  Engg., vol.  23, no. 3, pp. 1826-1867, Jun. 2018. [Online]. Available: https: //doi.org/10.1007/s10664-017-9573-6</li>
<li>[48] D.  Batory,  D.  Benavides,  and A.  Ruiz-Cortes,  'Automated analysis  of feature  models:  Challenges  ahead,' Commun.  ACM, vol.  49,  no.  12,  pp. 45-47, Dec. 2006.  [Online]. Available: https://doi.org/10.1145/1183236. 1183264</li>
<li>[49] J. Sun,  H.  Zhang,  Y.  Fang,  and  L.  Wang,  'Formal  semantics  and verification  for  feature  modeling,'  in 1EEE  1nternational  Conference on  Engineering o f Complex Computer Systems  (1CECCS'05), June  2005, pp.  303-312.</li>
<li>[50] K.  Czarnecki  and  C.  Kim,  'Cardinality-based  feature  modeling  and constraints  :   A progress report,'  2005.</li>
<li>[51] D.  Benavides,  S.  Segura,  and A. Ruiz-Cortes,  'Automated analysis  of feature models 20 years later:  A literature review,' Information Systems, vol.  35,  pp.  615-636,  09  2010.</li>
<li>[52] N.  Siegmund,  A.  Grebhahn,  S.  Apel,  and  C.  Kastner,  'Performanceinfluence models for highly configurable systems,' in Proceedings o f the 2015  10th Joint Meeting  on Foundations  o f Software  Engineering, ser. ESEC/FSE 2015,  2015, pp.  284-294.</li>
<li>[53] H. Ha and H. Zhang, 'Performance-influence model for highly configurable software with  fourier learning and  lasso  regression,' in 2019  1EEE  International  Conference  on  Software  M aintenance and  Evolution, 1CSME  2019, Cleveland, OH, USA, September  29 -October  4,  2019. IEEE,  2019,  pp.  470-480.  [Online].  Available: https://doi.org/10.1109/ICSME.2019.00080</li>
<li>[54] D. Golovin, B. Solnik, S. Moitra, G. Kochanski, J. Karro, and D. Sculley, 'Google vizier:  A  service for black-box  optimization,' in Proceedings o f   the 23rd ACM   S1GKDD  International  Conference  on  Knowledge</li>
<li>Discovery  and  Data  M ining , ser.  KDD  '17. New  York,  NY,  USA: Association for Computing Machinery,  2017, pp.  1487-1495.  [Online]. Available:  https://doi.org/10.1145/3097983.3098043</li>
<li>[55] J.  Rasley,  Y .  He,  F.  Yan,  O.  Ruwase,  and  R.  Fonseca,  'Hyperdrive: Exploring hyperparameters with pop  scheduling,'  in Proceedings  o f the 18th ACM/IFIP/USENIX Middleware  Conference, ser.  Middleware  '17. New York,  NY, USA: Association for Computing Machinery, 2017, pp. 1-13.  [Online].  Available:  https://doi.org/10.1145/3135974.3135994</li>
<li>[56] M. Li, E. Yumer,  and D. Ramanan, 'Budgeted training:  Rethinking deep neural  network  training  under  resource  constraints,'  in International Conference  on  Learning  Representations, 2020.  [Online].  Available: https://openreview.net/forum?id=HyxLRTVKPH</li>
<li>[57] Z. Lu, L. Chen, C.-K. Chiang, and F. Sha, 'Hyper-parameter tuning under a budget constraint,'  in Proceedings  o f the  Twenty-Eighth International Joint  Conference  on  Artificial  Intelligence,  IJCAI-19. International Joint  Conferences  on  Artificial  Intelligence  Organization,  7  2019,  pp. 5744-5750.  [Online]. Available:  https://doi.org/10.24963/ijcai.2019/796</li>
<li>[58] J.  M.  Hernandez-Lobato,  M. A. Gelbart,  R.  P.  Adams,  M.  W.  Hoffman, and  Z.  Ghahramani,  'A  general  framework  for  constrained  bayesian optimization  using  information-based  search,' J. Mach.  Learn.  Res., vol.  17, pp.  160:1-160:53, 2016.</li>
<li>[59] A. Gordon, E. Eban, O. Nachum, B. Chen, T. Yang, and E. Choi, 'Morphnet:  Fast  &amp;  simple  resource-constrained  structure learning of deep networks,' CoRR, 2017. [Online]. Available: http://arxiv.org/abs/1711.06798</li>
<li>[60] J. P. Cambronero,  J.  Cito, and  M. C. Rinard, 'Ams:  Generating automl  search  spaces  from  weak  specifications,'  in Proceedings  o f the 28th  ACM   Joint  Meeting on  European  Software  Engineering Conference and Symposium on the Foundations of Software Engineering, ser. ESEC/FSE 2020. New York, NY, USA: Association for Computing Machinery, 2020, pp. 763-774. [Online]. Available: https://doi.org/10.1145/3368089.3409700</li>
<li>[61] J. Hoffmann,  K.  Aehlig,  and  M.  Hofmann,  'Multivariate  amortized resource  analysis,' ACM Trans.  Program.  Lang.  Syst., vol.  34,  no.  3, Nov. 2012. [Online]. Available: https://doi.org/10.1145/2362389.2362393</li>
<li>[62] J.  Hoffmann,  A.  Das,  and  S.-C.  Weng,  'Towards  automatic  resource bound analysis for ocaml,' in Proceedings  o f the  44th A C M  SIGPLAN Symposium  on Principles  o f Programming Languages, ser.  POPL  2017. New York,  NY, USA: Association for Computing Machinery, 2017, pp. 359-373.  [Online]. Available:  https://doi.org/10.1145/3009837.3009842</li>
<li>[63] M.  Hofmann  and  S.  Jost, 'Static  prediction  of  heap  space  usage for first-order functional programs,' in Proceedings o f the 30th ACM SIGPLAN-SIGACT  Symposium  on  Principles  o f Programming Languages , ser. POPL  '03. New  York, NY, USA: Association for  Computing  Machinery,  2003,  pp. 185-197.  [Online].  Available: https://doi.org/10.1145/604131.604148</li>
<li>[64] S. Jost, K. Hammond, H.-W. Loidl, and M. Hofmann, 'Static determination of quantitative resource usage for higher-order programs,' in Proceedings  o f the  37th Annual ACM  SIGPLAN-SIGACT Symposium on  Principles  o f Programming  Languages, ser.  POPL  '10. New York, NY, USA:  Association for Computing  Machinery,  2010, pp.  223-236. [Online].  Available:  https://doi.org/10.1145/1706299.1706327</li>
</ul>
</div>
</body>
</html>