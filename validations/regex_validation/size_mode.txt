<--page_start:1-->
Evolvable SPL management with partial knowledge: an<--size=17.2-->
application to anomaly detection in time series<--size=17.2-->

Yassine El Amraoui<--size=12.0-->
UniversitÃ© CÃ´te dâ€™Azur, CNRS, I3S<--size=10.0-->
EZAKO<--size=10.0-->
Sophia Antipolis, France<--size=10.0-->
yassine.elamraoui@ezako.com<--size=10.0-->

Mireille Blay-Fornarino<--size=12.0-->
UniversitÃ© CÃ´te dâ€™Azur, CNRS, I3S<--size=10.0-->
Sophia Antipolis, France<--size=10.0-->
Mireille.blay@univ-cotedazur.fr<--size=10.0-->

Philippe Collet<--size=12.0-->
UniversitÃ© CÃ´te dâ€™Azur, CNRS, I3S<--size=10.0-->
Sophia Antipolis, France<--size=10.0-->
philippe.collet@univ-cotedazur.fr<--size=10.0-->

FrÃ©dÃ©ric Precioso<--size=12.0-->
UniversitÃ© CÃ´te dâ€™Azur, Inria, CNRS,<--size=10.0-->
I3S<--size=10.0-->
Sophia Antipolis, France<--size=10.0-->
frederic.precioso@univ-cotedazur.fr<--size=10.0-->

Julien Muller<--size=12.0-->
EZAKO<--size=10.0-->
Sophia Antipolis, France<--size=10.0-->
julien.muller@ezako.com<--size=10.0-->

ABSTRACT<--size=10.9-->

In Machine Learning (ML), the resolution of anomaly detection<--size=9.1-->
problems in time series presents a great diversity of practices as it<--size=8.9-->
can correspond to many different contexts. These practices cover<--size=9.0-->
both grasping the business problem and designing the solution<--size=9.1-->
itself. By practice, we designate explicit and implicit steps toward<--size=9.0-->
resolving a problem, while a solution corresponds to a combination<--size=8.9-->
of algorithms selected for their performance on a given problem.<--size=9.1-->
Two related issues arise. The first one is that the practices are<--size=9.1-->
individual and not explicitly mutualized. The second one is that<--size=9.1-->
choosing one solution over another is all the more difficult to justify<--size=8.9-->
because the space of solutions and the evaluation criteria are vast<--size=9.0-->
and evolve rapidly with the advances in ML. To solve these issues<--size=9.0-->
and tame the evolving diversity in ML, a Software Product Line<--size=9.1-->
(SPL) approach can be envisaged to represent the variable set of<--size=9.1-->
solutions. However, this requires characterizing an ML business<--size=9.1-->
problem through an explicit set of criteria and justifying one ML<--size=9.1-->
solution over all others. The resolution of anomaly detection prob-<--size=8.9-->
lems is thus different from finding the best configuration workflow<--size=8.9-->
from past configurations but lies more in guiding the configuration<--size=8.9-->
towards a solution that may never have been studied before. This<--size=9.0-->
paper proposes an SPL approach that capitalizes on past practices<--size=9.0-->
by exploiting a variability-aware representation to detect new cri-<--size=9.0-->
teria and constraints when practices adopt different solutions to<--size=9.1-->
seemingly similar problems. We report on the evaluation of our<--size=9.1-->
approach using a set of applications from the literature and an ML<--size=8.9-->
software company. We show how the analysis of practices makes it<--size=8.9-->
possible to consolidate the knowledge contained in the SPL.<--size=9.0-->

Permission to make digital or hard copies of all or part of this work for personal or<--size=7.0-->
classroom use is granted without fee provided that copies are not made or distributed<--size=6.9-->
for profit or commercial advantage and that copies bear this notice and the full citation<--size=6.9-->
on the first page. Copyrights for components of this work owned by others than the<--size=7.0-->
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or<--size=6.9-->
republish, to post on servers or to redistribute to lists, requires prior specific permission<--size=6.9-->
and/or a fee. Request permissions from permissions@acm.org.<--size=7.0-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.<--size=7.0-->
ACM ISBN 978-1-4503-9443-7/22/09...$15.00<--size=7.0-->
https://doi.org/10.1145/3546932.3547008<--size=7.0-->

CCS CONCEPTS<--size=10.9-->

â€¢ Software product line; â€¢ Machine learning; â€¢ Configurations;<--size=8.9-->

KEYWORDS<--size=10.9-->

Software Product Line, Machine Learning, Evolution, Metrics<--size=9.0-->

ACM Reference Format:<--size=8.0-->
Yassine El Amraoui, Mireille Blay-Fornarino, Philippe Collet, FrÃ©dÃ©ric Pre-<--size=8.0-->
cioso, and Julien Muller. 2022. Evolvable SPL management with partial<--size=8.0-->
knowledge: an application to anomaly detection in time series. In 26th<--size=8.0-->
ACM International Systems and Software Product Line Conference - Volume A<--size=7.9-->
(SPLC â€™22), September 12â€“16, 2022, Graz, Austria. ACM, New York, NY, USA,<--size=7.9-->
12 pages. https://doi.org/10.1145/3546932.3547008<--size=8.0-->

1<--size=10.9-->
INTRODUCTION<--size=10.9-->

Building learning systems are increasingly complex, as industry<--size=9.1-->
data, human and organizational factors, and application domains<--size=9.0-->
define different contexts that require tailored practices[11]. By prac-<--size=8.9-->
tices, for the Machine Learning (ML) community, we mean the<--size=9.1-->
entire process of producing ML workflows, from analyzing the cus-<--size=8.9-->
tomerâ€™s data, business goals, and constraints to delivering the ML<--size=9.0-->
model built by composing algorithms. To address this variability<--size=9.1-->
of contexts, data scientists are developing a great deal of expertise,<--size=8.9-->
including developing dedicated algorithms within companies and<--size=9.0-->
tracking the evolution of theory and practice through literature<--size=9.1-->
and collaborations with researchers. However, with the profusion<--size=9.0-->
of algorithms and the diversity of industry problems, connecting<--size=9.0-->
problems to appropriate practices requires increasing capabilities<--size=9.0-->
and resources.<--size=9.0-->
To make this connection between real-world problems and undis-<--size=8.9-->
covered scientific knowledge, we chose to focus on time series<--size=9.1-->
anomaly detection, such as stock price outlier detection, which<--size=9.1-->
presents a wide variety of challenges and practices [11, 25]. Scien-<--size=9.0-->
tific knowledge in this area remains to be discovered as the data<--size=9.1-->
and application domains require developing new solutions. While<--size=9.0-->
building an ML model involves a composition of algorithms that<--size=9.1-->
takes a long time to design and test, the available experiments only<--size=8.9-->
partially cover the large variability of the domain, especially in<--size=9.1-->
industrial applications (see Section 2.1).<--size=9.0-->

<--image width=1003.0 height=1004.0-->
<--page_end:1-->

<--page_start:2-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
El Amraoui et al.<--size=7.0-->

In this paper, we argue that a Software Product Line (SPL) ap-<--size=9.1-->
proach allows for linking partial configurations of ML problems<--size=9.1-->
with appropriate workflows. The originality of the approach is<--size=9.1-->
to exploit past configurations to enrich the knowledge captured<--size=9.1-->
by the SPL. In this context, we identify the following functional<--size=9.1-->
requirements.<--size=9.0-->
R1- Identifying similarities between partially described problems.<--size=9.0-->
Looking for similarities and differences with previous problems is a<--size=8.9-->
natural first thought for data scientists but remains a difficult task.<--size=8.9-->
Indeed, the nature of the source data often makes their characteri-<--size=9.0-->
zation challenging, especially since precisely defining an anomaly<--size=9.0-->
in a time series can be difficult, including for the customer who<--size=9.1-->
delivered the data. Being able to deal with partially characterized<--size=9.0-->
source data is therefore mandatory, including proposing algorithms<--size=8.9-->
that will be able to manage the variability of the data sets, for ex-<--size=9.0-->
ample, when the type of anomaly is not known (singular points,<--size=9.1-->
global anomalies, or pattern anomalies [12]).<--size=9.0-->
R2-Consolidating knowledge according to the evolution of prac-<--size=9.1-->
tices. Considering new solutions (i.e., new ML workflow) from the<--size=9.0-->
literature requires characterizing the boundaries of the problems<--size=9.0-->
targeted by these solutions. Thus, it is not only a matter of selecting<--size=8.9-->
new algorithms but also new criteria such as evaluation metrics or<--size=8.9-->
business requirements. [8, 17]. For instance, detecting anomalies<--size=9.0-->
in scarce resource environments such as IoT embedded systems,<--size=9.1-->
has an impact on the entire ML model production chain [10, 43].<--size=9.1-->
To consolidate the knowledge, we must be able to compare appli-<--size=9.0-->
cations. By application, we mean not only the solutions and their<--size=9.0-->
performances, but also the targeted problems, i.e., the data and the<--size=8.9-->
business requirements. In particular, we want to identify data sets<--size=9.0-->
and business requirements that appear similar but have different<--size=9.0-->
solutions to highlight new criteria or the obsolescence of some past<--size=8.9-->
criteria. For example, in the literature, when, for the same dataset,<--size=9.0-->
two different compositions of algorithms work well, it is interesting<--size=8.9-->
to identify, if possible, which requirement criteria could distinguish<--size=8.9-->
them.<--size=9.0-->
To highlight these requirements, we propose three scenarios in<--size=9.0-->
which Lucile, a data scientist persona, uses our framework named<--size=9.0-->
ROCKâ€™n RWL1(RRW).<--size=9.0-->

Scenario 1:Lucile uses RRW to search for a solution to a new anom-<--size=8.9-->
aly detection problem over a given dataset. Through a dedicated<--size=9.1-->
interface, Alice indicates the business requirements and some ad-<--size=9.0-->
ditional information about the dataset. RRW narrows the solution<--size=9.0-->
space to the suitable components and selects the most relevant<--size=9.1-->
ones. In addition, if previous applications match the same criteria,<--size=9.0-->
RRW helps Alice compare and browse them as her analysis evolves.<--size=8.9-->

Scenario 2: Lucile wishes to enrich the SPL with a new set of appli-<--size=8.9-->
cations that she considers interesting. RRW analyses the information<--size=8.9-->
related to these applications. After checking that it does not contra-<--size=8.9-->
dict previous knowledge, RRW makes them available to other data<--size=8.9-->
scientists.<--size=9.0-->

Scenario 3: Lucile wants to evaluate the SPL.. RRW informs her<--size=9.1-->
of the equivalences between the descriptions of the data sets, the<--size=9.0-->
business requirements, and the solutions of the various registered<--size=9.0-->
applications. RRW can then draw her attention to various issues.<--size=9.1-->

1Request your Own Convenient Knowledge flow and Run your ML WorkfLows<--size=7.0-->

For example, RRW points out applications that address similar prob-<--size=8.9-->
lems, i.e., seemingly identical business requirements and dataset<--size=9.1-->
characteristics, but use different solutions. It also identifies criteria<--size=8.9-->
that are never used or always used. These warnings are intended<--size=9.0-->
to help identifying new data spaces to be tested, for example, new<--size=9.0-->
criteria for comparing problems, and solution updates.<--size=9.0-->

Research Vision and approach. The exploratory nature of ma-<--size=9.1-->
chine learning makes an exhaustive analysis of the domain difficult,<--size=8.9-->
if not impossible. According to Drummond [17] and from our own<--size=8.9-->
experience, "any advantage indicated by a simple scalar measure<--size=9.1-->
may be illusory if it hides situation-dependent performance differ-<--size=9.1-->
ences." Despite the generalization power of ML and the substantial<--size=8.9-->
evolution of the field, we advocate that SPLs are well suited to<--size=9.1-->
explore this complexity of dependencies between data, business<--size=9.1-->
goals, and algorithm composition. Our contribution then concerns:<--size=8.9-->
â€¢ Exploiting the SPL to guide the data scientist in narrowing<--size=9.0-->
the solution space and more easily pinpointing past solutions<--size=8.9-->
that solved similar problems.<--size=9.0-->
â€¢ Leveraging the SPL to reason about past solutions, making<--size=9.0-->
new knowledge explicit, and exploiting it to consolidate the<--size=8.9-->
SPL.<--size=9.0-->
The principle is then the following. The configurations of the<--size=9.1-->
applications in the SPL incrementally capture our partial knowl-<--size=9.1-->
edge of the problems and solutions. The SPL progressively supports<--size=8.9-->
capitalizing on what is not explicit by reasoning about these con-<--size=9.0-->
figurations. Then the main issue is not to determine the configura-<--size=8.9-->
tion workflow that best suits the actors according to the previous<--size=9.0-->
configurations [45], but to guide them in composing a solution<--size=9.1-->
for an unprecedentedly studied problem. Concurrently, it is not<--size=9.1-->
a question of generating random samples [23], whose relevance<--size=9.1-->
could not be precisely verified (e.g., stuffing the SPL with all the<--size=9.1-->
available algorithms and pre-processing components from the liter-<--size=8.9-->
ature). Instead, it is more a matter of enriching our knowledge by<--size=9.0-->
systematically studying new validated configurations.<--size=9.0-->
This paper shows how we apply these principles in construct-<--size=9.1-->
ing an SPL for anomaly detection in time series. We explain the<--size=9.1-->
difficulties specific to this domain of ML and why we consider that<--size=8.9-->
an adapted SPL can help in its analysis (see Section 2). Then we<--size=9.1-->
present the principles of the SPL, particularly the patterns used<--size=9.1-->
to identify knowledge from past applications (see Section 3). We<--size=9.1-->
validate this proposal on the first three steps of the SPL construc-<--size=9.0-->
tion. The first phase consists in building the SPL proactively by<--size=9.1-->
domain analysis. Then we enrich the SPL with practices extracted<--size=9.0-->
from the partner company, and a third phase adds some applica-<--size=9.1-->
tions of OpenML [53] to the SPL. We then discuss the limits and<--size=9.1-->
perspectives of the approach (see Section 5) before concluding this<--size=8.9-->
paper.<--size=9.0-->

2<--size=10.9-->
FROM PARTIAL KNOWLEDGE TO AN SPL<--size=10.9-->

Designing ML workflow has become essential to almost any scien-<--size=8.9-->
tific field with Deep Learning advances in the past decade. The field<--size=8.9-->
increased fast and in many directions based on different models,<--size=9.1-->
yet it is not a rich and shared knowledge enough to be organized<--size=9.0-->
and made accessible to non-experts. Most of the knowledge one can<--size=8.9-->
hold is partial, shared through non-conventional channels such as<--size=8.9-->
personal blogs of other data scientists, webinars, and tricks shared<--size=8.9-->

<--page_end:2-->

<--page_start:3-->
Evolvable SPL management with partial knowledge: an application to anomaly detection in time series<--size=7.0-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->

orally during conferences. This situation does not help to adopt<--size=9.1-->
these new techniques efficiently, particularly in companies. From<--size=9.0-->
our point of view, software engineering should play a more vital<--size=9.1-->
role in solving this central issue.<--size=9.0-->

2.1<--size=10.9-->
ML Workflows for anomaly detection in<--size=10.9-->
time series<--size=10.9-->

Defining anomalies in a given business context requires business<--size=9.0-->
goals and constraints to be precised. Depending on the anomaly<--size=9.1-->
detection problem, it remains challenging to construct appropri-<--size=9.1-->
ate workflows [4] because the interactions between the current<--size=9.1-->
data, the composition of algorithms, and the business requirements<--size=8.9-->
are substantial and not always well understood. Sculley et al. sum-<--size=8.9-->
marise these interactions as follows: "changing anything, changes<--size=9.0-->
everything" [47].<--size=9.0-->
Events predicted as statistically abnormal by the model may<--size=9.1-->
not be relevant anomalies for the end-user, if they are unrelated<--size=9.1-->
to business requirements, as for instance sensor failures. In order<--size=9.0-->
for the model to distinguish relevant from non-relevant anomalies,<--size=8.9-->
selecting the proper data preparation algorithm is then part of the<--size=8.9-->
final ML workflow solution and often a crucial part of it.<--size=9.0-->
Testing the variability of workflows is all the more problematic<--size=9.0-->
as the resources required to train models can be very important in<--size=8.9-->
terms of time, memory, computation, but also in terms of human<--size=9.0-->
investment, and not only from data scientists. Indeed, in the ab-<--size=9.1-->
sence of a normality reference or threshold, decisions on whether<--size=9.0-->
values are abnormal or not have to be made by end-users, which<--size=9.1-->
is time-consuming. Therefore, it is essential to reduce the solution<--size=8.9-->
space to those more appropriate to solve the problem. However,<--size=9.1-->
identifying the problem itself can also be complex and resource-<--size=9.1-->
consuming. Thus, even if deep learning-based solutions were the<--size=9.0-->
solution to all problems [19], it would still be necessary to take into<--size=8.9-->
account the variability of upstream processing to prepare the data<--size=8.9-->
and downstream processing to maintain the models in production.<--size=8.9-->

2.2<--size=10.9-->
On meta-learning and AutoML<--size=10.9-->

Automated machine learning methods (AutoML) have been pro-<--size=9.1-->
posed and are focusing the efforts of many industries and research<--size=8.9-->
teams [21]. However, most AutoML algorithms aim only at solving<--size=8.9-->
a specific problem on specific datasets and do not provide end-<--size=9.1-->
users with the ability to acquire reasoned knowledge. Therefore,<--size=9.1-->
these systems are high-value solution components that we have<--size=9.1-->
introduced into the SPL to solve specific problems. More generally,<--size=8.9-->
many SE4AI2works addressed the issue of classification workflow<--size=8.9-->
selection in a generic way like in the work of MartÃ­nez-FernÃ¡ndez<--size=8.9-->
et al. [34] or by using a meta-learning-based portfolio like in the<--size=9.1-->
work of Kerschke et al. [29] and Degroote et al. [14]. Furthermore,<--size=8.9-->
these automated approaches entail massive needs for computation,<--size=8.9-->
memory, and time resources. One standard solution is to limit the<--size=9.0-->
solution space on which to train: measure choice, algorithms, or<--size=9.1-->
composition of algorithms.<--size=9.0-->
However, we aim at the contrary at enriching our SPL by regu-<--size=9.0-->
larly adding new algorithms, new business requirements criteria,<--size=9.0-->
etc. We also aim to help in the evaluation measure choice, according<--size=8.9-->
to the prediction performance [24] and ensure that the solutions<--size=9.1-->

2Software Engineering for AI<--size=5.5-->

deployed will scale in production [48], in particular, in anomaly<--size=9.1-->
detection, automation is difficult since end-users must validate<--size=9.1-->
anomalies, as explained in the previous section. We are advocating<--size=8.9-->
a reverse approach that is drastically less computationally, memory-<--size=8.9-->
intensive, and more suitable for scientific and reasoned knowledge<--size=8.9-->
acquisition directed by and for humans.<--size=9.0-->

2.3<--size=10.9-->
Towards an evolvable SPL<--size=10.9-->

This work is based on a collaboration between academic researchers<--size=8.9-->
in software engineering and data scientists from a company provid-<--size=8.9-->
ing ML workflows for business customers. The work thus targets<--size=9.0-->
various applications, involving diverse industrial datasets and busi-<--size=8.9-->
ness problems.<--size=9.0-->
Despite the constantly evolution of the domain (not to say the<--size=9.0-->
volatility of the domain), everything changes ... in an unpredictable<--size=9.0-->
way [44], choosing to capitalize on the different solutions designed<--size=8.9-->
by data scientists through an SPL seemed to be the best option. Our<--size=8.9-->
interviews revealed that, based on their experience, the domain<--size=9.1-->
experts had already identified most of the main dimensions of vari-<--size=8.9-->
abilities and commonalities of their domain. However, the domain<--size=8.9-->
analysis quickly showed that a proactive adoption scenario was not<--size=8.9-->
suitable, it is not yet possible to strictly separate domain analysis<--size=9.0-->
from the practices of data scientists seeking to solve new problems.<--size=8.9-->
Therefore, we have opted for a "reactive adoption strategy" for the<--size=8.9-->
product line and managed its evolution by integrating the practices<--size=8.9-->
of data scientists [30].<--size=9.0-->
Effectively managing the evolution of variant-rich software in-<--size=9.0-->
volves bridging the gap between software solutions and the capture<--size=8.9-->
of domain variability [28]. However, in our case, while data variabil-<--size=8.9-->
ity is partially identifiable automatically [8], the context variability<--size=8.9-->
is not entirely identifiable from software solutions. Our goal is<--size=9.1-->
therefore to obtain this non-explicit information with a minimum<--size=9.0-->
of manual effort. To this end, we introduce into the round-trip en-<--size=9.0-->
gineering process proposed by Promote-pl [30], a feedback phase<--size=9.0-->
on the content of the SPL itself. This consists in identifying, when<--size=8.9-->
integrating new applications, those that can provide new informa-<--size=9.0-->
tion by comparison with past applications. To meet this objective,<--size=9.0-->
we started from the following assumption: "Any customer can gen-<--size=8.9-->
erate the software they want, as long as they can describe it in<--size=9.1-->
the SPL"3. We use this assumption as a postulate to investigate the<--size=8.9-->
practices, and to hopefully identify new knowledge. The principle<--size=9.0-->
is that if two equivalent descriptions of problems correspond to<--size=9.1-->
two different solutions, then it is not the same problem; otherwise,<--size=8.9-->
we would not know which software to generate. Thus integrating<--size=9.0-->
an application to the SPL involves interactions with data scientists<--size=8.9-->
only when it is not possible to distinguish the contexts that led to<--size=9.0-->
two different solutions.<--size=9.0-->
Evolving an SPL in an ad hoc manner is error-prone because the<--size=8.9-->
configuration space is large and involves taking into account many<--size=8.9-->
interdependent artefacts. Thus, the definition of a reactive approach<--size=8.9-->
integrates the need to foresee "a typical pattern for maintaining<--size=9.1-->
and evolving a product line during its lifetime" [5]. In [49], while<--size=9.1-->
defining safe evolution the authors state "that the resulting SPL<--size=9.1-->
must be able to generate products that behaviorally match all of<--size=9.1-->
the original SPL products". We are in a slightly simpler application<--size=9.0-->

3Loosely adapted from Henry Ford.<--size=5.5-->

<--page_end:3-->

<--page_start:4-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
El Amraoui et al.<--size=7.0-->

context since we aim at composing only a unique stated version of<--size=8.9-->
each algorithm. It is not the productsâ€™ behavior that changes, but<--size=9.0-->
the logic of assembling the workflows that evolves. Our objective<--size=9.0-->
is, thus, to detect configurations that are no longer valid due to<--size=9.1-->
changes in the SPL, possibly because of a past error.<--size=9.0-->

2.4<--size=10.9-->
From the requirements to the SPL paradigm<--size=10.9-->

According to newly identified practices, the only artefacts that<--size=9.1-->
evolve in our SPL are the feature model (i.e., CUD4operations on<--size=9.0-->
features, feature groups, and constraints) and the assets by addition<--size=8.9-->
or removal of algorithms and workflows (i.e., CD operations). The<--size=9.0-->
configuration knowledge does not evolve as such. It consists only<--size=9.0-->
of bijections between the algorithmsâ€™ code and the corresponding<--size=9.0-->
feature. Analyzing the impact of the evolution operations on past<--size=9.0-->
configurations is part of our perspectives inspired by the prelimi-<--size=9.0-->
nary work of Nieke et al. [36].<--size=9.0-->
Besides, given the very high variability of the domain and its<--size=9.1-->
constant evolution, it is neither possible, at least for the time be-<--size=9.1-->
ing, to consider sampling techniques for workflows on which to<--size=9.1-->
learn [27], nor to support the configuration process in an optimal<--size=9.0-->
way [37].<--size=9.0-->
To answer the requirements stated in the introduction, we refor-<--size=8.9-->
mulate them into technical requirements (RT) for an SPL dedicated<--size=8.9-->
to the composition of ML workflows in the context of anomaly de-<--size=8.9-->
tection in time series. To meet R1, the solution search corresponds<--size=8.9-->
to configuring a feature model, producing a valid configuration,<--size=9.1-->
partial in the problem specification and complete in the solution def-<--size=8.9-->
inition (RT1.1). The configurations must be comparable on the sub-<--size=8.9-->
spaces: the dataset description, the business requirements, and the<--size=8.9-->
solution components (RT1.2). The evolution of knowledge-driven<--size=9.0-->
by practices (R2) requires that the criteria of the domain analysis<--size=9.0-->
evolve without impacting the solution space (RT2.1). We must set<--size=9.0-->
up comparison patterns among configurations corresponding to<--size=9.1-->
the applications to guide the discovery of new knowledge (RT2.2).<--size=8.9-->

3<--size=10.9-->
DESIGNING AN EVOLVABLE SPL WITH<--size=10.9-->
PARTIAL KNOWLEDGE<--size=10.9-->

The RRW SPL defines a set of ML practices with well-defined varia-<--size=8.9-->
bilities and commonalities. A combination of features (i.e., config-<--size=9.0-->
uration) identifies each product, and results in an application, i.e.,<--size=9.0-->
an ML workflow with its performance, its evaluation strategy, its<--size=9.0-->
deployment environment, etc. The format of the applications varies<--size=8.9-->
from notebooks, references to runs in OpenML, and references in<--size=9.0-->
the company tool. The set of valid feature combinations is specified<--size=8.9-->
in a feature model (FM) whose structure aims at facilitating the SPL<--size=8.9-->
evolution. Implementation artefacts are essentially references to<--size=9.1-->
algorithms and workflow models expressed in BPMN [39]. Map-<--size=9.1-->
pings express the relationships between solution features and these<--size=8.9-->
artefacts. The SPL supports the generation of BPMN workflows<--size=9.1-->
based on the selected workflow model and algorithms. Since the<--size=9.1-->
construction of the SPL depends on the new applications created,<--size=9.0-->
we set up different mechanisms to control its evolution.<--size=9.0-->
We developed tools to validate the overall process: (i) configu-<--size=9.1-->
ration and search of past applications (configurator), (ii) generation<--size=8.9-->
of ML workflows (generator), (iii) integration of applications in<--size=9.1-->

4create (C), update (U) or delete (D) operation [33]<--size=5.5-->

the SPL (integrator), (iv) reconfiguration of past configurations (re-<--size=8.9-->
configurator), (v) evaluation of the knowledge carried by the SPL<--size=9.0-->
concerning the recorded applications (analyzer). We only present<--size=9.0-->
the concepts in this article. The configurator dynamically reads the<--size=8.9-->
feature model and a CSV file with helpful information for present-<--size=8.9-->
ing the features (question, description, links to external elements).<--size=9.0-->
To help data scientists understand the feature selection, we as-<--size=9.1-->
sociate descriptions with the constraints related to the selected<--size=9.1-->
features during the configuration. The possibility to import/export<--size=8.9-->
configurations allows proceeding by enrichment and, in the case of<--size=8.9-->
reconfiguration, manually adapting the past problematic configu-<--size=9.0-->
rations. The reusable artifacts are then the previous experiments<--size=9.1-->
(codes and configurations to adapt), a set of algorithms and work-<--size=9.0-->
flow models. The reusable artifacts are, therefore, the previous<--size=9.1-->
experiments (codes and configurations to adapt), a set of algorithms,<--size=8.9-->
and workflow models.<--size=9.0-->

3.1<--size=10.9-->
FM structure to tame the SPL evolution<--size=10.9-->

We structure the knowledge captured by the FM according to six<--size=9.0-->
main concepts, which organized the top of the FM hierarchy as de-<--size=8.9-->
picted in figure 1. Information about the data sources, the business<--size=8.9-->
requirements, and the solution are mandatory as they are required<--size=8.9-->
to identify new applications. Information sources, states, and appli-<--size=8.9-->
cations help the user in her analysis; they are optional. The numbers<--size=8.9-->
correspond in our case study to the number of features under each<--size=8.9-->
branch. The following paragraphs detail the content of the FM.<--size=9.0-->

Data set properties. InitialData branch of the FM characterizes<--size=9.0-->
the space of datasets containing time series. Some properties are<--size=9.1-->
automatically extracted from the dataset (the sampling frequency,<--size=9.0-->
the time series number of dimensions, the stationarity verdict,...),<--size=9.0-->
while we can only get others through interacting with the business<--size=8.9-->
expert, such as how to interpret the missing values. No outgoing<--size=9.0-->
constraints from this branch to another branch are allowed since the<--size=8.9-->
dataset properties do not inherently imply algorithms or business<--size=9.0-->
requirements (RT2.1). For example, in our case study, this branch<--size=9.0-->
under the feature InitialData contains 28 other features.<--size=9.0-->

Business requirement characteristics. BusinessRequirements bran-<--size=8.9-->
ch captures requirements, such as limited memory usage to comply<--size=8.9-->
with hardware constraints or the solutionâ€™s ability to provide expla-<--size=8.9-->
nations.<--size=9.0-->

Solution components & states. The Solution branch groups and<--size=9.1-->
structures the algorithms used for solving anomaly detection pro-<--size=9.0-->
blems and the types of workflows used in learning and deployment.<--size=8.9-->
The states branch represents the states through which the data<--size=9.1-->
passes. We express preconditions and the impact of a solution<--size=9.1-->
component through constraints relative to a state. Therefore, we<--size=9.1-->
forbid solution components to refer directly to the features of the<--size=9.0-->
initial data set but only to the corresponding state (RT2.1). For<--size=9.1-->
instance, an algorithm can require the state of the data to be scaled<--size=8.9-->
but not need that to be the initial state of the data.<--size=9.0-->

Application & sources. The Application branch is only used in<--size=9.1-->
the configurator to facilitate direct access to past applications by<--size=9.1-->
filtering them according to the initial problem or the components of<--size=8.9-->
the Solution used. Similarly, the Sources branch helps remember<--size=9.0-->

<--page_end:4-->

<--page_start:5-->
Evolvable SPL management with partial knowledge: an application to anomaly detection in time series<--size=7.0-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->

from which literature article some features and constraints have<--size=9.1-->
been extracted and who are the authors of the applications.<--size=9.0-->

<--image width=937.0 height=625.0-->
Figure 1: Feature Model Structure<--size=9.0-->

3.2<--size=10.9-->
Capturing knowledge through configuration<--size=10.8-->
management<--size=10.9-->

To build and develop the SPL from past applications, we memorize<--size=8.9-->
the valuable elements to find the associated codes and the context<--size=8.9-->
in which they were defined.<--size=9.0-->

Configurations. Configurations are our primary tool for deter-<--size=9.1-->
mining application context. As we work on applications whose<--size=9.1-->
context is difficult to define and the SPL evolves, the configurations<--size=8.9-->
associated with the applications may be partial (RT1.1). Therefore,<--size=9.0-->
we consider any feature neither selected nor deselected as "un-<--size=9.1-->
known."<--size=9.0-->
A partial configuration ğ‘is defined as a set of selected, deselected,<--size=8.9-->
and undefined features. The intersection is empty between these<--size=9.1-->
three subsets.<--size=9.0-->
Let âŸ¦ğ¹ğ‘€âŸ§be the set of valid configurations of a feature model ğ¹ğ‘€.<--size=9.0-->
A partial configuration ğ‘of a feature model ğ¹ğ‘€is valid iff<--size=9.0-->
âˆ€ğ‘“ğ‘–âˆˆğ‘, ğ‘“ğ‘–âˆˆğ¹ğ‘€âˆ§âˆƒğ‘ğ‘˜âˆˆâŸ¦ğ¹ğ‘€âŸ§,ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘) âŠ†ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘ğ‘˜) âˆ§<--size=9.0-->
ğ‘‘ğ‘’ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘) âŠ†ğ‘‘ğ‘’ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘ğ‘˜). By extension, we note ğ‘âˆˆâŸ¦ğ¹ğ‘€âŸ§.<--size=9.0-->
A configuration is complete relatively to a set of features ğ¹, when<--size=9.0-->
âˆ€ğ‘“ğ‘–âˆˆğ¹, ğ‘“ğ‘–âˆˆğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘) âˆªğ‘‘ğ‘’ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘). Thus, in RRW , configura-<--size=9.0-->
tions must be complete only relative to the Solution branch since<--size=8.9-->
we know whether the solution components are part of the appli-<--size=9.1-->
cation or not (RT1.1). In the following we refer to configurations,<--size=9.1-->
even for partial configurations.<--size=9.0-->
To evaluate the evolution of our knowledge, we preserve the<--size=9.1-->
information on the manual or automatic selection/deselection. So<--size=9.0-->
we denote a configuration as a set of pairs: (ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’,ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘ ) where<--size=9.0-->
ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘ âˆˆ{ğ‘šğ‘ ,ğ‘šğ‘‘,ğ‘ğ‘ ,ğ‘ğ‘‘,ğ‘¢}, where m for manual, a for automatic, s<--size=9.0-->
for selected, d for deselected, u for undefined. For example,<--size=9.0-->
ğ‘= {(ğ‘“1,ğ‘šğ‘ ), (ğ‘“2,ğ‘ğ‘ ), (ğ‘“3,ğ‘¢), (ğ‘“4,ğ‘ğ‘‘)},<--size=9.0-->
ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘) = {ğ‘“1, ğ‘“2},ğ‘‘ğ‘’ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘) = {ğ‘“4}<--size=9.0-->

Data sets. For each dataset involved in an application, we pre-<--size=9.1-->
serve the associated partial configuration relative to the branch<--size=9.1-->
initialData (see Figure1). This record supports a consolidation<--size=9.0-->
mechanism. New applications dealing with known datasets should<--size=8.9-->
describe them in compliance with previous records and further<--size=9.1-->
complete them (RT2.2).<--size=9.0-->

Required information about applications. The information associ-<--size=8.9-->
ated with an application is its name (used as a reference), its initial<--size=8.9-->
configuration, the feature model used to define it, a reference to the<--size=8.9-->
dataset, its author, and the associated codes. The connection to the<--size=8.9-->
author makes it possible to identify the practices and preferences<--size=9.0-->
of the data scientists and allows a data scientist to reduce the space<--size=8.9-->
of the applications by authors. The reference to codes corresponds,<--size=8.9-->
to Jupyter NoteBooks, to runs in OpenML or to a workflow in the<--size=9.0-->
industrial partner platform.<--size=9.0-->

3.3<--size=10.9-->
Pattern detection based on the premise: a<--size=10.9-->
problem has a unique solution<--size=10.9-->

In the same way as Tornhill [50], we seek to identify "hotspots" to<--size=8.9-->
narrow our study of applications to a few critical patterns that are<--size=9.0-->
most likely to guide us in the extraction of new knowledge (RT2.2).<--size=8.9-->
Therefore, the principle is that if two similar problems correspond<--size=8.9-->
to different solutions, then it is not the same problem.<--size=9.0-->
We defined the first pattern: 2 problems evaluated as equivalent<--size=9.1-->
have a different solution. This pattern allows us to detect different<--size=9.0-->
situations. (i) One of the solutions is not adapted to the problem,<--size=9.1-->
and in this case, in retrospect, the data scientist should not have<--size=9.1-->
used it. We must enrich the feature model to prohibit it. (ii) The<--size=9.1-->
two problems are different, but we had not yet identified these<--size=9.1-->
discriminative criteria in the feature model; we must enrich the<--size=9.1-->
feature model with these new criteria.<--size=9.0-->
Because the configurations partially characterize the problems,<--size=9.0-->
we define a second pattern: 2 problems evaluated as unifiable have a<--size=8.9-->
different solution. It can indeed be two similar problems partially<--size=9.1-->
filled in. In this case, we expect the same solution as before. But, the<--size=8.9-->
data scientist may also have designed a different solution to address<--size=8.9-->
the lack of information about the problem, such as not knowing<--size=9.1-->
the anomaly types.<--size=9.0-->
The fact that several problems have the same solution can also<--size=9.0-->
induce an insensitivity of the solution to certain features. Despite<--size=9.0-->
the small number of applications, this situation allowed us to iden-<--size=8.9-->
tify a feature that we apprehended at a level too detailed to be<--size=9.1-->
discriminating. Detecting these patterns occurs in Scenario 3 and<--size=9.0-->
meets the requirement RT2.2.<--size=9.0-->
We now specify the notions of equivalence classes and their unifi-<--size=8.9-->
ability. To explain these concepts, we use the feature model pre-<--size=9.1-->
sented in Figure 2 and the configurations described in Table 1.<--size=9.1-->
Table 3 shows the identified equivalence classes.<--size=9.0-->

<--image width=651.0 height=413.0-->
Figure 2: Feature model for explaining metrics<--size=9.0-->

<--page_end:5-->

<--page_start:6-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
El Amraoui et al.<--size=7.0-->

Table 1: Examples of application configurations.<--size=9.0-->

XP Name<--size=9.0-->
d1<--size=9.0-->
d2<--size=9.0-->
d3<--size=9.0-->
p3<--size=9.0-->
ğ‘4<--size=9.0-->
p1<--size=9.0-->
p2<--size=9.0-->
a1<--size=9.0-->
a2<--size=9.0-->
b1<--size=9.0-->
b2<--size=9.0-->
ğ‘†ğ‘œğ‘¢ğ‘Ÿğ‘ğ‘’ğ‘ <--size=9.0-->
r1<--size=9.0-->
r2<--size=9.0-->
app1<--size=9.0-->
as<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
as<--size=9.0-->
ad<--size=9.0-->
ms<--size=9.0-->
md<--size=9.0-->
as<--size=9.0-->
ms<--size=9.0-->
md<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
app2<--size=9.0-->
as<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
as<--size=9.0-->
ad<--size=9.0-->
ms<--size=9.0-->
ad<--size=9.0-->
ms<--size=9.0-->
ms<--size=9.0-->
md<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
app3<--size=9.0-->
as<--size=9.0-->
as<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
as<--size=9.0-->
ms<--size=9.0-->
ad<--size=9.0-->
ad<--size=9.0-->
ms<--size=9.0-->
ms<--size=9.0-->
md<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
app4<--size=9.0-->
as<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
as<--size=9.0-->
ad<--size=9.0-->
ms<--size=9.0-->
ms<--size=9.0-->
ad<--size=9.0-->
ms<--size=9.0-->
md<--size=9.0-->
as<--size=9.0-->
ms<--size=9.0-->
u<--size=9.0-->
s=selected, d=deselected, u=undefined, a=automatic, m=manual<--size=9.0-->

Table 2: Metrics related to the FM in<--size=9.1-->
fig. 2 and its configurations (tab. 1)<--size=9.0-->

ğ‘ğ‘œğ¹<--size=9.0-->
20<--size=9.0-->
ğ‘ğ‘œğ´<--size=9.0-->
4<--size=9.0-->
ğ‘ğ‘™ğ‘’ğ‘ğ‘“<--size=9.0-->
12<--size=9.0-->
ğ¶ğ‘œğ‘£<--size=9.0-->
66 %<--size=9.0-->
ğ‘ğ‘œğ¶<--size=9.0-->
2<--size=9.0-->
ğ‘ğ‘œğ¸ğ¶<--size=9.0-->
3<--size=9.0-->
ğ¶ğ‘‡ğ¶ğ‘…<--size=9.0-->
20 %<--size=9.0-->
ğ¶ğ‘œğ‘š<--size=9.0-->
37,5 %<--size=9.0-->

Let a feature model ğ¹ğ‘€and ğ´a set of valid partial configurations<--size=8.9-->
of ğ¹ğ‘€, ğ´âŠ†âŸ¦ğ¹ğ‘€âŸ§.<--size=9.0-->

3.3.1<--size=9.0-->
Equivalence Classes in ğ´. An equivalence class on a subset<--size=9.0-->
of features ğ¹of ğ¹ğ‘€is defined as a set of valid configurations [ğ‘1] =<--size=9.0-->
{ğ‘1, ...ğ‘ğ‘˜}, ğ‘ğ‘–âˆˆğ´, such as âˆ€ğ‘“ğ‘–âˆˆğ¹,<--size=9.0-->
ğ‘“ğ‘–âˆˆâˆ©ğ‘˜<--size=9.0-->
1ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘ğ‘—) Ã âˆ©ğ‘˜<--size=9.0-->
1ğ‘‘ğ‘’ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘ğ‘—) Ã âˆ©ğ‘˜<--size=9.0-->
1ğ‘¢ğ‘›ğ‘‘ğ‘’ğ‘“ğ‘–ğ‘›ğ‘’ğ‘‘(ğ‘ğ‘—).<--size=9.0-->
In Table 3, two equivalence classes are identified on the Initial-<--size=9.0-->
Data subtree. CDS1={app1, app2, app4}. CDS1 can also be noted:<--size=9.0-->
{(ğ‘‘1,ğ‘ ), (ğ‘‘2,ğ‘¢), (ğ‘‘3,ğ‘¢)} and CDS2: {(ğ‘‘1,ğ‘ ), (ğ‘‘2,ğ‘ ), (ğ‘‘3,ğ‘¢)}<--size=9.0-->
Problem equivalence classes are defined on the sub-features of<--size=9.1-->
InitialData and BusinessRequirements.<--size=9.0-->
For example CP1={app1, app2, app4}.<--size=9.0-->
ğ¶ğ‘ƒ1 : {(ğ‘‘1,ğ‘ ), (ğ‘‘2,ğ‘¢), (ğ‘‘3,ğ‘¢), (ğ‘1,ğ‘‘), (ğ‘2,ğ‘ ), (ğ‘3,ğ‘¢), (ğ‘4,ğ‘ )}<--size=9.0-->
Solution equivalence classes are defined on the sub-features of<--size=9.1-->
Solution.<--size=9.0-->
Two configurations are equivalent in ğ¹ğ‘€if they are member of<--size=9.0-->
the same Problem and Solution equivalence classes. In our example,<--size=8.9-->
app1 and app2 are equivalent.<--size=9.0-->
We note ğ‘ğ‘œğ¸ğ¶the number of equivalent classes.<--size=9.0-->

3.3.2<--size=9.0-->
Unifiable classes. Two equivalence classes [ğ‘1] and [ğ‘2] de-<--size=9.0-->
fined on a same set of features ğ¹are unifiable if âˆ€ğ‘“ğ‘–âˆˆğ¹,<--size=9.0-->
ğ‘“ğ‘–âˆˆâˆ©2<--size=9.0-->
1(ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘ğ‘—) âˆªğ‘¢ğ‘›ğ‘‘ğ‘’ğ‘“ğ‘–ğ‘›ğ‘’ğ‘‘(ğ‘ğ‘—))<--size=9.0-->
Ã âˆ©2<--size=9.0-->
1(ğ‘‘ğ‘’ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘ğ‘—) âˆªğ‘¢ğ‘›ğ‘‘ğ‘’ğ‘“ğ‘–ğ‘›ğ‘’ğ‘‘(ğ‘ğ‘—))<--size=9.0-->
In the example, CDS1 and CDS2 are unifiable.<--size=9.0-->

3.4<--size=10.9-->
Metrics for evaluating the SPL<--size=10.9-->

To explain the following metrics, we use the feature model pre-<--size=9.1-->
sented in Figure 2 and the configurations presented in table 1.<--size=9.0-->

3.4.1<--size=9.0-->
Feature model metrics. We selected some standard metrics<--size=9.0-->
[18] to assess the state of the feature model and, by comparison, its<--size=8.9-->
evolution.<--size=9.0-->
The number of Features (ğ‘ğ‘œğ¹) and the number of features with<--size=8.9-->
no children (ğ‘ğ‘™ğ‘’ğ‘ğ‘“) are a way to measure the scope of the SPL.<--size=9.1-->
Our objective is to integrate new solutions while identifying better<--size=8.9-->
and better the problems solved. We analyze these metrics in the<--size=9.1-->
different spaces. In our example in Figure 2 , the number of leaves is<--size=8.9-->
twelve overall, and it is four in the Solution subtree (Solution Space).<--size=8.9-->
The evolution of the number of cross-constraints (ğ‘ğ‘œğ¶), together<--size=9.0-->
with the tree-cross-constraint ratio (ğ¶ğ‘‡ğ¶ğ‘…)5, gives a numerical in-<--size=9.0-->
dication of the identified interactions. In our example, four features<--size=8.9-->
are involved in constraints, so the ğ¶ğ‘‡ğ¶ğ‘…is 20%. The theoretical<--size=9.1-->
number of possible configurations is not only not calculable but also<--size=8.9-->
does not provide any information. Indeed, it is likely that a part of<--size=8.9-->

5number of distinct features involved in cross-tree constraints and divides them<--size=5.5-->
through the total number of features in the feature model<--size=7.0-->

the valid configurations does not correspond to suitable solutions.<--size=9.0-->
Moreover, this partial knowledge of the domain combined with<--size=9.1-->
the very high cost of ML workflows evaluations does not allow to<--size=9.0-->
test the SPL by generating examples, except to consume a lot of<--size=9.0-->
resources without any assurance of a real gain.<--size=9.0-->
We now propose to evaluate the feature model ğ¹ğ‘€according<--size=9.1-->
to the set of valid configurations ğ´âŠ†âŸ¦ğ¹ğ‘€âŸ§that correspond to<--size=9.0-->
applications integrated in the SPL.<--size=9.0-->

3.4.2<--size=8.9-->
Feature-level metrics based on past configurations. Common-<--size=8.9-->
ality of feature (ğ¶ğ‘œğ‘š(ğ‘“)) indicates the selection ratio (manual or<--size=9.0-->
automatic) of a feature f in ğ´. This ratio identifies the "unused vari-<--size=8.9-->
ability" smell (i.e., feature always selected, e.g., ğ¶ğ‘œğ‘š(ğ‘1) = 1)[5].<--size=9.1-->
The rate of deselection (ğ·ğ‘’ğ‘ (ğ‘“) identifies the "unused feature" smell<--size=8.9-->
(i.e., feature always deselected, e.g., ğ¶ğ‘œğ‘š(ğ‘2) = 0, ğ·ğ‘’ğ‘ (ğ‘2) = 1)[5].<--size=9.0-->
We also compute the rate of undefined occurrences(ğ‘ˆğ‘›ğ‘‘(ğ‘“)) that<--size=9.1-->
may identify an obscure feature that is not well related to the scope<--size=8.9-->
of the SPL (e.g., ğ‘ˆğ‘›ğ‘‘(ğ‘‘3) = 1, ğ·ğ‘’ğ‘ (ğ‘‘3) = 0,ğ¶ğ‘œğ‘š(ğ‘‘3) = 0).<--size=9.0-->
We globalize these metrics to all feature model leaves, which in<--size=8.9-->
our case study characterize practices.<--size=9.0-->

3.4.3<--size=9.0-->
Feature Model Coverage. The Feature Model Coverage rate<--size=9.0-->
(ğ¶ğ‘œğ‘£) measures (in percentage) the degree of leaf selections in a<--size=9.0-->
set of configurations ğ´.<--size=9.0-->
Feature model coverage rate = number of feature leaves selected in<--size=9.1-->
ğ´/number of feature leaves * 100.<--size=9.0-->
In our example, eight leaves are selected at least once, ğ¶ğ‘œğ‘£= 66%.<--size=9.0-->
For the Solution subtree, as three leaves were chosen at least one<--size=9.0-->
time, ğ¶ğ‘œğ‘£ğ‘†ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›= 75%<--size=9.0-->
Feature Model Coverage does not correspond to ğ‘¡-ğ‘¤ğ‘–ğ‘ ğ‘’covera-<--size=9.0-->
ge [22]. Unlike the latter, it only provides a measure of the feature<--size=8.9-->
selection rate in a given set of configurations, it does not allow<--size=9.1-->
to assess the coverage of feature interactions. Nevertheless, it has<--size=9.0-->
the advantage of not requiring to compute the number of possible<--size=8.9-->
configurations.<--size=9.0-->

3.4.4<--size=9.0-->
Feature Model Commonality Rate. The Feature Model Com-<--size=9.0-->
monality Rate (ğ¶ğ‘œğ‘š) measures (in percentage) the selection ratio<--size=9.0-->
of leaf selections in ğ´.<--size=9.0-->
Feature model Commonality Rate = number of selection of feature<--size=9.1-->
leaves in ğ´/ number of feature leaves * #ğ´* 100, where #ğ´denotes<--size=8.9-->
the cardinality of the set ğ´.<--size=9.0-->
In our example, 18 selections of leaves for 4 configurations and<--size=9.0-->
12 leaves, ğ¶ğ‘œğ‘š= 37, 5%. ğ¶ğ‘œğ‘šğ‘†ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›= 50%<--size=9.0-->
Intuitively, the confidence in the feature model suggestions is pro-<--size=9.0-->
portional to its commonality rate.<--size=9.0-->

<--page_end:6-->

<--page_start:7-->
Evolvable SPL management with partial knowledge: an application to anomaly detection in time series<--size=7.0-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->

Table 3: Equivalence classes and pattern detection.<--size=9.0-->

App Name<--size=9.0-->
InitialDSClass<--size=9.0-->
InitialPBClass<--size=9.0-->
SolutionClass<--size=9.0-->
EquivalentApp<--size=9.0-->
Warning<--size=9.0-->
SameSolution<--size=9.0-->
app1<--size=9.0-->
CDS1<--size=9.0-->
CP1<--size=9.0-->
CS1<--size=9.0-->
{app2}<--size=9.0-->
{app4}<--size=9.0-->
{app3}<--size=9.0-->
app2<--size=9.0-->
CDS1<--size=9.0-->
CP1<--size=9.0-->
CS1<--size=9.0-->
{app1}<--size=9.0-->
[]<--size=9.0-->
[]<--size=9.0-->
app4<--size=9.0-->
CDS1<--size=9.0-->
CP1<--size=9.0-->
CS2<--size=9.0-->
[]<--size=9.0-->
{app1}<--size=9.0-->
[]<--size=9.0-->
app3<--size=9.0-->
CDS2<--size=9.0-->
CP2<--size=9.0-->
CS1<--size=9.0-->
[]<--size=9.0-->
[]<--size=9.0-->
{app1}<--size=9.0-->
app1, app2 and app4 handles the same equivalence classes of dataset (CDS1) and problem (CP1). app1 and app2 are equivalent. app3 deals with the same<--size=8.0-->
equivalence class of solutions (CS1) than app1 (and therefore app2). While app4 handles the same equivalence class of problem than app1, it proposes a new<--size=8.0-->
solution, a warning is raised. According to Table 1, CDS1 and CDS2 are unifiable.<--size=8.0-->

3.5<--size=10.9-->
Systematically mastering the evolutions of<--size=10.9-->
the SPL<--size=10.9-->

To promote safe reactive development of the SPL, we suggest the<--size=9.0-->
following process.<--size=9.0-->
At step ğ‘‡, the SPL is coherent, i.e., all applications correspond<--size=9.1-->
to valid configurations of the feature model. We create new appli-<--size=9.0-->
cations with the configurator. We integrate them to the SPL to<--size=9.1-->
make them available to other users (scenario 2, see Section 3.5.1). At<--size=8.9-->
stepğ‘‡+1, we update the feature model by adding, renaming, and re-<--size=8.9-->
moving features and constraints. It is then necessary to ensure that<--size=8.9-->
the configurations related to the previously developed applications<--size=8.9-->
remain valid and do not contradict the new knowledge captured by<--size=8.9-->
the feature model (see Section 3.5.2). We use the new feature model<--size=8.9-->
to build solutions to new problems. We use the patterns presented<--size=8.9-->
above, coupled with metrics to evaluate the SPL and detect new<--size=9.1-->
knowledge (see Section 3.5.3). We use the same means to analyze<--size=9.0-->
the evolution of the SPL (see Section 3.5.4).<--size=9.0-->

3.5.1<--size=8.9-->
Making applications identifiable in the configurator. Integra-<--size=8.9-->
ting in the feature model an application named ğ‘ğ‘ğ‘on a dataset<--size=9.1-->
namedğ‘‘and defined by a valid configurationğ‘consists in adding, in<--size=8.9-->
the Applications branch of the feature model, the features ğ‘‘and<--size=8.9-->
ğ‘ğ‘ğ‘if they are not already there. Then, the minimal constraints6<--size=9.0-->

linkingğ‘‘to the selected and deselected features of the initialData<--size=8.9-->
space are added starting from the manually selected and unselected<--size=8.9-->
leaves. The constraint ğ‘ğ‘ğ‘â‡’ğ‘‘is then added. We then proceed in<--size=9.0-->
the same way to link ğ‘ğ‘ğ‘to the rest of the feature model, starting<--size=9.0-->
with the problem space. When the dataset ğ‘‘is already present<--size=9.1-->
in the feature model, there should be no contradiction with its<--size=9.1-->
constraints. However, they can be completed. This step corresponds<--size=8.9-->
to scenario two and is essential in scenario one to identify datasets<--size=8.9-->
or applications with the same features as the current configuration.<--size=8.9-->
For example, adding the application ğ‘ğ‘ğ‘5 created by John on<--size=9.1-->
dataset ğ‘‘ğ‘ 5 and defined by the following configuration<--size=9.0-->
{(ğ‘‘2,ğ‘ğ‘ ), (ğ‘‘3,ğ‘šğ‘ ), (ğ‘1,ğ‘šğ‘ ), (ğ‘2,ğ‘ğ‘‘), (ğ‘1,ğ‘šğ‘ ), (ğ‘1,ğ‘šğ‘‘), (ğ‘2,ğ‘šğ‘ ), ...}<--size=9.0-->
adds the features ğ‘ğ‘ğ‘5, ğ‘‘ğ‘ 5 and ğ½ğ‘œâ„ğ‘›in the branch Applications<--size=8.9-->
and the following constraints: ğ‘‘ğ‘ 5 â‡’ğ‘‘3,ğ‘ğ‘ğ‘5 â‡’ğ‘‘ğ‘ 5,ğ‘ğ‘ğ‘5 â‡’<--size=9.0-->
ğ‘1,ğ‘ğ‘ğ‘5 â‡’ğ‘2 âˆ§Â¬ğ‘1 âˆ§ğ‘1,ğ‘ğ‘ğ‘5 â‡’ğ½ğ‘œâ„ğ‘›<--size=9.0-->

3.5.2<--size=9.0-->
Application-preserving refactoring against practice evolution.<--size=9.0-->
Refactorings of the feature model may lead to past applications<--size=9.1-->
(i.e., their related configurations) being detected as conflicting with<--size=8.9-->
the current feature model [5]. To promote a safe evolution, a recon-<--size=8.9-->
figuration step is performed on all past configurations. For now,<--size=9.1-->

6features automatically selected or deselected during the configuration are not involved<--size=5.5-->
in new constraints<--size=7.0-->

reconfiguring ağ‘ğ‘ configuration into ağ‘ğ‘¡configuration with respect<--size=8.9-->
to a new feature model ğ¹ğ‘€consists in (i) renaming in ğ‘ğ‘¡some of<--size=9.0-->
the features of ğ‘ğ‘ , (ii) omitting the features that disappeared in ğ¹ğ‘€<--size=8.9-->
with a warning if they were selected or deselected in ğ‘ğ‘ , (iii) adding<--size=8.9-->
in ğ‘ğ‘¡the new features of ğ¹ğ‘€whose value is known, (iv) copying in<--size=8.9-->
ğ‘ğ‘¡the other features, then (v) replaying ğ‘ğ‘¡in ğ¹ğ‘€to obtain a new<--size=9.0-->
valid configuration or to raise an error in the contrary situation. If<--size=8.9-->
past configurations cannot be rendered valid in ğ¹ğ‘€, RTFS excludes<--size=8.9-->
them with a warning. The new valid configurations related to ap-<--size=9.0-->
plications can then be integrated into the ğ¹ğ‘€using the previous<--size=9.1-->
operation.<--size=9.0-->
For example, if we add the constraint Â¬ğ‘1âˆ¨Â¬ğ‘‘2 in FM of Figure 2,<--size=9.0-->
the configuration corresponding to ğ‘ğ‘ğ‘3 is no more valid, while all<--size=8.9-->
the other configurations are automatically updated with (ğ‘‘2,ğ‘ğ‘‘).<--size=9.0-->

3.5.3<--size=9.0-->
Knowledge extraction driven by SPL assessment. Regarding<--size=9.0-->
scenario 3, the identification of the patterns presented above and<--size=9.0-->
the associated metrics allow us to evaluate the SPL to extract new<--size=9.0-->
knowledge and orient future evolutions, notably according to the<--size=9.0-->
spaces covered or not by the applications.<--size=9.0-->

3.5.4<--size=9.1-->
Knowledge extraction driven by SPL evolution assessment.<--size=9.1-->
The metrics and the detection of patterns also make it possible to<--size=9.0-->
evaluate the evolution of the SPL.<--size=9.0-->
Have more features been used? Do unifiable problems become<--size=9.0-->
equivalent? Conversely, does the enrichment now allow us to dis-<--size=9.0-->
tinguish previously equivalent problems? Both of these cases can<--size=9.0-->
occur when the addition of constraints affects previously undefined<--size=8.9-->
features.<--size=9.0-->

4<--size=10.9-->
APPLICATION<--size=10.9-->

We now report on the first three steps of the SPLâ€™s construction,<--size=9.1-->
showing how the practices contributed to its enrichment. Figure 3<--size=8.9-->
summarizes this construction process. The configurations and the<--size=9.0-->
results of the analyses are accessible online7<--size=9.0-->

4.1<--size=10.9-->
First three steps of the SPL construction<--size=10.9-->
process<--size=10.9-->

In each of the steps presented below we have integrated the appli-<--size=8.9-->
cations into the FM, which did not raise any significant issue.<--size=9.0-->

Initial product line version from literature study. Following a first<--size=8.9-->
analysis of the domain, we built the SPLâ€™s initial version (ğ‘†ğ‘ƒğ¿ğ‘‡0).<--size=9.1-->
The feature model (ğ¹ğ‘€ğ‘‡0) integrates some solutions from the liter-<--size=8.9-->
ature dealing with the detection of anomalies in time series. The<--size=9.1-->

7https://anonymous.4open.science/r/RFTS-SPLC2022-D508/<--size=5.5-->

<--page_end:7-->

<--page_start:8-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
El Amraoui et al.<--size=7.0-->

<--image width=691.0 height=283.0-->
Figure 3: Three main steps of our SPL construction<--size=9.0-->

applications correspond to experiments carried out on reference<--size=9.1-->
data sets [13].<--size=9.0-->

Enrichment of the product line through industrial practices. At ğ‘‡1,<--size=8.9-->
we leverage the practices of the partner companyâ€™s data scientists<--size=9.0-->
to build ğ‘†ğ‘ƒğ¿ğ‘‡1 by enriching ğ‘†ğ‘ƒğ¿ğ‘‡0. The interest in exploiting indus-<--size=8.9-->
trial applications is to broaden the scope of the SPL to the processing<--size=8.9-->
of industrial data. The industrial partner uses a custom tool to sum-<--size=8.9-->
marize all the applications on their customerâ€™s data. We update<--size=9.1-->
the feature model (ğ¹ğ‘€ğ‘‡0 â†’ğ¹ğ‘€ğ‘‡1) by including company-specific<--size=9.0-->
solution components, new initial dataset properties relevant to an-<--size=8.9-->
alyzing customer datasets, and new features necessary to describe<--size=8.9-->
the customer business requirements. Then we collect applications<--size=9.0-->
conducted by the companyâ€™s data scientists, keeping only the solu-<--size=8.9-->
tions from deployed workflows and solving customersâ€™ anomaly<--size=9.1-->
detection problems. We have thus selected six workflows whose<--size=9.1-->
resulting product models are in production. The production of these<--size=8.9-->
workflows can take several months for the data scientists. We have<--size=8.9-->
generated partial configurations containing information about the<--size=8.9-->
dataset and solution based on automatic solution extraction and<--size=9.1-->
data analysis tools. We used these partial configurations to initialize<--size=8.9-->
the configurator. We then completed the source data and business<--size=9.0-->
requirements parts via a discussion with the application authors.<--size=9.0-->

Consolidation by extraction of OpenML workflows. At ğ‘‡2, we ex-<--size=9.0-->
tract some practices from the OpenML platform. OpenML is an<--size=9.1-->
automated machine learning environment [53], from which ML<--size=9.1-->
practices can be downloaded and uploaded i.e., solutions (runs and<--size=8.9-->
flows in OpenML) to a given problem (task and dataset in OpenML).<--size=8.9-->
The interest in exploiting OpenMLâ€™s practices is to analyze the<--size=9.1-->
impact of upgrading the SPL with external sources. In OpenML, we<--size=8.9-->
selected time-series datasets and associated tasks of type Supervised<--size=8.9-->
Learning and Unsupervised learning since anomaly detection is su-<--size=9.0-->
pervised or unsupervised learning with unbalanced classes. We only<--size=8.9-->
had four datasets that matched these criteria. We kept 4 tasks of Su-<--size=8.9-->
pervised learning that had runs associated with them. Among these<--size=8.9-->
runs, we selected only the best runs on F1-score evaluation criterion<--size=8.9-->
as evaluations on other measures such as user CPU-time were not<--size=8.9-->
available for these runs. We preferred the runs using the scikitlearn<--size=8.9-->
library when we had the choice. We then extracted the associated<--size=9.0-->
flows and generated the associated partial configurations for each<--size=9.0-->
run. We had already studied in ğ‘‡0the meta-features proposed by<--size=9.0-->
OpenML to characterize datasets, so we only updated the feature<--size=9.0-->
model(ğ¹ğ‘€ğ‘‡1 â†’ğ¹ğ‘€ğ‘‡2) by adding new solution components.<--size=9.0-->

4.2<--size=10.9-->
Knowledge extraction driven by SPL<--size=10.9-->
assessment<--size=10.9-->

We explain in the following subsections how we exploit pattern<--size=9.1-->
and metric analysis in our use case.<--size=9.0-->

Two different solutions for the same problem: algorithms<--size=9.1-->
side effects. At ğ‘‡0, we encountered the following scenario. For<--size=9.1-->
two equivalent problems, the solutions used two different scaling<--size=9.0-->
techniques in each workflow, min-max scaler and robust scaler [40].<--size=8.9-->
This equivalence of problems and not solutions raised a warning.<--size=9.0-->
We analyzed workflows for both experiments and observed that for<--size=8.9-->
the second workflow, the robust scaler results were equivalent to the<--size=8.9-->
min-max scaler results due to the data properties. In this scenario,<--size=9.0-->
we were able to confirm that the main particularity of the robust<--size=9.0-->
scaler was not required8. Therefore only the first application with<--size=8.9-->
min-max scaler was kept. We added a constraint to the selection of<--size=8.9-->
this algorithm to prevent the error from being repeated. i.e., data<--size=9.1-->
without outliers will not anymore be scaled with robust scaler.<--size=9.0-->

Two different solutions for the same problem: Data Scien-<--size=9.1-->
tist preferences impact. The data scientistâ€™s preferences bias her<--size=9.0-->
choice of the solution components. Atğ‘‡1, we identified two applica-<--size=8.9-->
tions that presented different solutions to equivalent problems. The<--size=8.9-->
two authors could not justify the difference in the choice of Solution<--size=8.9-->
components other than by their expertise in selected algorithms.<--size=9.1-->
Therefore, we have kept these two applications distinguishable by<--size=8.9-->
their author, with a warning for possible future treatment.<--size=9.0-->

Two problems same solution: factorizing unnecessary varia-<--size=8.9-->
bility. At ğ‘‡0, two problems differ only in acquisition sampling;<--size=9.1-->
data acquisition sampling is in seconds for one and in microseconds<--size=8.9-->
for the second. Otherwise, the data are similar, and the anomaly<--size=9.1-->
detection requirements are equivalent. After the detection of this<--size=9.0-->
pattern, we checked the impact of acquisition sampling on the<--size=9.1-->
algorithms and factorized all four regular sampling features into<--size=9.1-->
regularSampling for the SPL at ğ‘‡1.<--size=9.0-->

4.3<--size=10.9-->
Knowledge extraction driven by SPL<--size=10.9-->
evolution assessment<--size=10.9-->

We also exploited the analysis of the evolution of patterns and<--size=9.1-->
metrics as another source of information. We use Pbğ›¼and Pbğ›½to<--size=9.0-->
refer to the problem part of the configurations (i.e., the features of<--size=8.9-->
the InitialData and BusinessRequirements branches) and Sğ›¼<--size=9.1-->
and Sğ›½to refer to the solution part.<--size=9.0-->

4.3.1<--size=9.1-->
Pattern evolution and knowledge consolidation. At T0, Pbğ›¼<--size=9.1-->
and Pbğ›½are equivalent, but solved by two different clustering mod-<--size=8.9-->
els9, kmeans [26] on the one hand and Dbscan [46] on the other. At<--size=8.9-->
T0, we did not know which to delete; we kept both applications. At<--size=8.9-->
T1, we reconfigured the configurations to align with the new feature<--size=8.9-->
model, which now incorporates features detailing business expert<--size=9.0-->
insights into possible outliers in the data10. The feature model also<--size=8.9-->
includes associated constraints expressing compatibility between<--size=9.0-->

8Usage of the robust scaler is interesting only if outliers are within the values of the<--size=5.5-->
time series<--size=7.0-->
9Solution workflows vary according to machine learning algorithms<--size=5.5-->
10The data scientists can decide whether outliers are anomalies or not in the context<--size=5.5-->
of the experiment<--size=7.0-->

<--page_end:8-->

<--page_start:9-->
Evolvable SPL management with partial knowledge: an application to anomaly detection in time series<--size=7.0-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->

Table 4: Metrics Evolution in times and spaces<--size=9.0-->

ğ‘ğ‘œğ¹<--size=8.0-->
ğ‘ğ‘™ğ‘’ğ‘ğ‘“<--size=8.0-->
ğ¶ğ‘œğ‘£<--size=8.0-->
ğ¶ğ‘œğ‘š<--size=8.0-->
ğ‘ğ‘œğ¸ğ¶<--size=8.0-->
ğ‘ğ‘œğ´<--size=8.0-->
ğ‘ğ‘œğ¶<--size=8.0-->
ğ¶ğ‘‡ğ¶ğ‘…<--size=8.0-->

ğ‘‡0<--size=8.0-->

InitialData<--size=8.0-->
23<--size=8.0-->
16<--size=8.0-->
37%<--size=8.0-->
19%<--size=8.0-->
5<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
BusinessRequirements<--size=8.0-->
33<--size=8.0-->
24<--size=8.0-->
41%<--size=8.0-->
21,25 %<--size=8.0-->
7<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
Solution<--size=8.0-->
51<--size=8.0-->
25<--size=8.0-->
52%<--size=8.0-->
16.8 %<--size=8.0-->
7<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
Globalâˆ—<--size=8.0-->
156<--size=8.0-->
96<--size=8.0-->
35,04 %<--size=8.0-->
14,68 %<--size=8.0-->
10<--size=8.0-->
10<--size=8.0-->
25<--size=8.0-->
21,19%<--size=8.0-->

ğ‘‡1<--size=8.0-->

InitialData<--size=8.0-->
28<--size=8.0-->
19<--size=8.0-->
42,10 %<--size=8.0-->
18,94 %<--size=8.0-->
9<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
BusinessRequirements<--size=8.0-->
43<--size=8.0-->
33<--size=8.0-->
54,55 %<--size=8.0-->
17 %<--size=8.0-->
13<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
Solution<--size=8.0-->
67<--size=8.0-->
37<--size=8.0-->
48,64 %<--size=8.0-->
8,64 %<--size=8.0-->
11<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
Globalâˆ—<--size=8.0-->
194<--size=8.0-->
124<--size=8.0-->
40,32 %<--size=8.0-->
10,86 %<--size=8.0-->
14<--size=8.0-->
15<--size=8.0-->
31<--size=8.0-->
21,76%<--size=8.0-->

ğ‘‡2<--size=8.0-->

InitialData<--size=8.0-->
28<--size=8.0-->
19<--size=8.0-->
57,9 %<--size=8.0-->
18,00 %<--size=8.0-->
14<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
BusinessRequirements<--size=8.0-->
43<--size=8.0-->
33<--size=8.0-->
57,6 %<--size=8.0-->
17,24 %<--size=8.0-->
17<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
Solution<--size=8.0-->
74<--size=8.0-->
42<--size=8.0-->
57,14 %<--size=8.0-->
7,89 %<--size=8.0-->
15<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
Globalâˆ—<--size=8.0-->
203<--size=8.0-->
131<--size=8.0-->
47,32 %<--size=8.0-->
10,41 %<--size=8.0-->
18<--size=8.0-->
19<--size=8.0-->
32<--size=8.0-->
21,78%<--size=8.0-->
âˆ—The difference between the global figures and the figures of the 3 spaces corresponds to the branches Sources and states.<--size=8.0-->
The feature model hierarchy is six levels deep for the Solution branch, and four for the InitialData and BusinessRequirements branches.<--size=8.0-->

solution components and these new features. The reconfiguration<--size=9.0-->
made it possible to distinguish the two problems and the adequacy<--size=8.9-->
of the two different solutions.<--size=9.0-->

4.3.2<--size=8.9-->
Pattern evolution and knowledge extraction. Pbğ›¼and Pbğ›½are<--size=8.9-->
equivalent in T0, Sğ›¼includes a dimension reduction process through<--size=8.9-->
PCA [1] while Sğ›½skips this step. Like in the previous example,<--size=9.1-->
we kept both applications. At T1, we extended the InitialData<--size=9.1-->
space with features to explicit time series dimensionalities and<--size=9.1-->
automated their evaluation by dataset analysis. The reconfiguration<--size=8.9-->
step indicated that in Pbğ›¼, the time series were multivariate. In<--size=9.1-->
contrast, Pbğ›½â€™s time series were uni-variate [2]. This unique change<--size=8.9-->
in configuration highlighted the link between PCA and time series<--size=9.0-->
dimensionalities.<--size=9.0-->

4.4<--size=10.9-->
Exploiting the metrics<--size=10.9-->

In sections 4.2 and 4.3, we established that the analysis of equiva-<--size=9.0-->
lence classes on both the problems and the solutions helps to trace<--size=8.9-->
the applications and their common points. We will now describe<--size=9.1-->
how the metrics defined in section 3.4 help us assess the evolution<--size=8.9-->
of the practices in each space.<--size=9.0-->

InitialData. The coverage rate (ğ¶ğ‘œğ‘£) increased from ğ‘‡0to ğ‘‡1,<--size=8.9-->
while the number of features (ğ‘ğ‘œğ¹) also increased. This increase in-<--size=8.9-->
dicates that the industrial applications cover different data set prop-<--size=8.9-->
erties from the first applications on benchmark datasets. Between<--size=9.0-->
ğ‘‡1and ğ‘‡2the coverage increased while the number of features<--size=9.0-->
did not change. New applications did involve new features of the<--size=9.1-->
InitialData. We rely on commonality analysis to better understand<--size=8.9-->
the variations between industrial and benchmark datasets. It shows<--size=8.9-->
that at ğ‘‡0all the features related to Missingvalues were unused<--size=9.1-->
features11which means that the datasets did not have missing val-<--size=8.9-->
ues of any type. Atğ‘‡1MCARMV12, and StructuralMV13features had<--size=8.9-->
a ğ‘ğ‘œğ‘š(ğ‘“) > 1, which means that the new datasets were exhibit-<--size=9.0-->
ing these two types of missing values. Similarly, we identify the<--size=9.1-->
emergence of irregular sampling time series at ğ‘‡1.<--size=9.0-->

11always deselected<--size=5.5-->
12Missing value completely at random<--size=5.5-->
13Missing values of structural nature<--size=5.5-->

BusinessRequirements. Within this feature space, we sought<--size=9.0-->
to identify the questions that experts answered the least. These<--size=9.1-->
questions may need rephrasing. The principle is then to identify<--size=9.1-->
the most undefined features of the penultimate level. We did not<--size=9.1-->
meet such a case yet, which was confirmed by the data scientists.<--size=9.0-->
The coverage and commonality analysis highlight the require-<--size=9.0-->
ments of industrial applications for memory, CPU, or energy con-<--size=9.0-->
sumption optimization. The features representing these hardware<--size=9.0-->
constraints are either undefined or deselected at ğ‘‡0and ğ‘‡2. They<--size=9.0-->
are selected at ğ‘‡1only.<--size=9.0-->

Solution. . The coverage rate decreases at stepğ‘‡1and increases<--size=8.9-->
atğ‘‡2, while the number of features increases strictly. The evolution<--size=8.9-->
of these two metrics indicates: (i) on the one hand, that industrial<--size=9.0-->
applications use new solution components; (ii) and on the other<--size=9.1-->
hand, that the applications we integrated at ğ‘‡2consolidate our SPL<--size=8.9-->
by reusing existing solution components. The commonality rate de-<--size=8.9-->
creases to reach 7.89%. However, a detailed analysis of the number<--size=8.9-->
of selections by feature indicates that some algorithms are used in<--size=8.9-->
several solutions, while others are never used. For instance, we ob-<--size=8.9-->
serve that each of LSTMAE (LSTM Auto-encoder) and MAE (mean<--size=8.9-->
absolute error) have been used 5 times out of 19, while padding,<--size=9.0-->
FrontFill and others have not been used. Therefore, correlated<--size=9.0-->
with broader coverage of problem space, this metric should help<--size=9.1-->
identify some of the preferences of data scientists and maybe some<--size=8.9-->
bias. Indeed, it is natural to think that data scientists generally rely<--size=8.9-->
on the algorithms they are comfortable with, sometimes maybe at<--size=8.9-->
the expense of the solution.<--size=9.0-->

5<--size=10.9-->
DISCUSSION<--size=10.9-->

In this section, we relate our findings to existing work, and discuss<--size=8.9-->
potential threats to validity and current limitations.<--size=9.0-->
Usability. . While we are confident that our SPL approach helps<--size=8.9-->
narrowing the problem, reducing the solution space, and identify-<--size=9.0-->
ing similar applications, these points remain to be proven through<--size=9.0-->
controlled experiments. To facilitate the use of the configurator,<--size=9.1-->
we rely on visualization techniques [41] since recommendation<--size=9.1-->
systems are not yet applicable [42, 51]. Yet, due to the increasing<--size=9.1-->
size and complexity of the feature model, one threat is that the<--size=9.1-->
configurator might become cumbersome to use because it exposed<--size=8.9-->

<--page_end:9-->

<--page_start:10-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
El Amraoui et al.<--size=7.0-->

too many questions and too many possible solutions. Controlling<--size=9.0-->
the evolution of the feature model is therefore essential to avoid<--size=9.1-->
irrelevant questions and poorly fitting solution components. Met-<--size=9.0-->
rics and patterns are part of the proposed solution to reduce this<--size=9.1-->
risk. Nevertheless, detecting patterns, especially those related to<--size=9.1-->
unification, can pose scalability issues on which we are currently<--size=9.0-->
working. Another threat to usability is related to the actual main-<--size=9.0-->
tenance of the SPL in response to metrics and patterns analysis.<--size=9.1-->
These tasks were performed by the SPL modelers, interacting with<--size=8.9-->
the data scientists. This point does not challenge the relevance of<--size=9.0-->
the approach, but we still need to demonstrate that the tools allows<--size=8.9-->
autonomous maintenance by data scientists and collaborative FM<--size=9.0-->
updating [31].<--size=9.0-->

Practice-driven feature modeling. To address the different percep-<--size=8.9-->
tions of domain concepts, we not only unified domain terminology<--size=8.9-->
with descriptive feature names, but also provided descriptions and<--size=8.9-->
sources that are accessible directly from the configurator. How-<--size=9.1-->
ever, we specified requirements only qualitatively (with proposi-<--size=9.1-->
tional FM) using an ordinal scale when necessary, instead of their<--size=9.0-->
scalar values (e.g., available memory greater than/lower than 1<--size=9.1-->
GB) [7]. To automate and ensure reproducibility of reasoning be-<--size=9.1-->
tween stakeholders, we scripted a mapping between time series<--size=9.1-->
metadata values and features. So far, these approximations have<--size=9.1-->
not hampered knowledge acquisition. Therefore, we did not need<--size=9.0-->
attributed feature models for which pattern detection has yet to be<--size=8.9-->
designed.<--size=9.0-->

Practice-driven evolution. . Our work follows a reactive SPL adop-<--size=8.9-->
tion process [20, 28], using different techniques to locate features [16].<--size=8.9-->
However, identifying the variations between workflows does not<--size=9.0-->
always enable us to understand the variations of the problem. The<--size=8.9-->
feature model then plays a crucial role in revealing undefined ele-<--size=9.0-->
ments of the problem from the known constraints on the solutions.<--size=8.9-->
It is therefore essential that the FM be rich enough. We have demon-<--size=8.9-->
strated through our case study that we can enrich it with pattern<--size=9.0-->
detection. Yet, other complementary avenues still need to be ex-<--size=9.1-->
plored to identify the relationships between solution components<--size=9.0-->
and source datasets. We are currently working on extracting the<--size=9.1-->
preconditions and effects of the algorithms by analyzing different<--size=9.0-->
techniques and ML environments [6, 35, 38, 52].<--size=9.0-->

Quality assurance. . When the feature model is modified, we<--size=9.1-->
check, through automatic reconfigurations [51], that the previous<--size=9.0-->
configurations are preserved or even enhanced. These systematic<--size=9.0-->
checks have already allowed us to identify errors in the defini-<--size=9.1-->
tion of new constraints. They participate in non-regression testing.<--size=8.9-->
However, SPL testing [15] and ML testing [55] are inherently dif-<--size=9.0-->
ficult activities that we do not yet address; Many algorithms built<--size=9.0-->
into SPL are too resource-intensive (CPU, memory, and time) to<--size=9.1-->
consider sampling techniques [23]. Nonetheless, we believe that<--size=9.1-->
some work on SPL configurations opens up new opportunities to<--size=9.0-->
help build portfolios for automatic algorithm selection [29]. For<--size=9.1-->
example, configuration similarity analysis should help analyze the<--size=8.9-->
coverage of the problem space [3, 15, 27], while modeled features<--size=9.0-->
provide additional information to the metadata usually considered<--size=8.9-->
in meta-learning [32].<--size=9.0-->

Generalizability. . External validity concerns the ability to gen-<--size=9.0-->
eralize the results to other environments [54]. Our study has been<--size=8.9-->

developed in the context of one company, taking into account in-<--size=9.0-->
dustrial applications. However, we have collected applications from<--size=8.9-->
three different sources, which mitigates the risk of dependency on<--size=8.9-->
the companyâ€™s applications. Pattern detection relies on our ability<--size=9.0-->
to distinguish between the problem space and the solution space,<--size=9.0-->
the essence of any SPL. However, we decided to showcase our<--size=9.1-->
work on the particular context of this SPL (i.e., focused on specific<--size=8.9-->
types of ML applications, with scientific knowledge yet to be dis-<--size=9.0-->
covered and with a small set of configurations) because it can be<--size=9.1-->
exploited industrially as is. We could generalize this approach to<--size=9.1-->
other systems as one of our most prized contributions is to build<--size=9.1-->
and evaluate an incremental SPL. However, t the particular context<--size=8.9-->
of this SPL (i.e., focused on specific types of ML applications, with<--size=8.9-->
scientific knowledge yet to be discovered and with a small set of<--size=9.1-->
configurations) does not allow us to state that our contribution<--size=9.1-->
is generalizable. Nevertheless, several subdomains of ML at least<--size=9.1-->
present the same characteristics.<--size=9.0-->

6<--size=10.9-->
CONCLUSION<--size=10.9-->

Recent technological advances have made possible to collect a large<--size=8.9-->
amount of data over time. The purpose of time series data mining<--size=9.0-->
is to enable classification, clustering, or outlier detection [9]. Our<--size=9.0-->
study focuses on this last task. In this paper, we have proposed<--size=9.1-->
a practice-driven approach to build an SPL as a first step toward<--size=9.1-->
allowing the design of generic solutions to detect anomalies in<--size=9.1-->
time series, while capturing new knowledge and capitalizing on<--size=9.1-->
the existing one.<--size=9.0-->
The incrementality in the acquisition of knowledge and the in-<--size=9.0-->
stability of the domain [44] are supported by the SPL through its<--size=9.0-->
structuring and the exploitation of partial configurations associ-<--size=9.1-->
ated with past applications. As far as we know, this is the first case<--size=8.9-->
of application of the SPL paradigm in such a context, and with a<--size=9.1-->
knowledge acquisition objective. We argue that using this para-<--size=9.1-->
digm to record and analyze practices will enable advances in the<--size=9.1-->
selection of ML workflows that are much less energy-intensive than<--size=8.9-->
meta-learning techniques, while assisting scientific knowledge pro-<--size=8.9-->
duction. By capturing practices in partial configurations, we obtain<--size=8.9-->
the abstractions to reason about datasets, solutions, and business<--size=9.0-->
requirements. The SPL is then used both to produce new solutions<--size=8.9-->
and compare them to past solutions, as well as to identify knowl-<--size=9.0-->
edge that was not explicit. The growing abstraction supported by<--size=9.0-->
the SPL also brings other benefits. In mentoring junior data sci-<--size=9.1-->
entists, we have observed a shift in the approach to creating ML<--size=9.1-->
workflows, focusing on analyzing problems before looking for sim-<--size=8.9-->
ilar applications, especially in choosing evaluation metrics. It is<--size=9.1-->
rather difficult for data scientists to explain the precise reasons for<--size=8.9-->
their choice. We observed that focusing only on particular cases<--size=9.1-->
identified as patterns makes the relevant criteria explicit.<--size=9.0-->
This preliminary work paves the way for new software engi-<--size=9.1-->
neering contributions to ML. Our SPL is now evolving through<--size=9.1-->
the various works of data scientists to enrich the knowledge of<--size=9.1-->
anomaly detection in time series. We are working on visualization<--size=8.9-->
tools to facilitate the exploitation of practices, and thus the SPL<--size=9.1-->
maintenance. Distinguishing the users of the SPL from those who<--size=9.0-->
maintain it is also part of our future plan in order to obtain an<--size=9.1-->
empirical validation.<--size=9.0-->

<--page_end:10-->

<--page_start:11-->
Evolvable SPL management with partial knowledge: an application to anomaly detection in time series<--size=7.0-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->

REFERENCES<--size=10.9-->

[1] HervÃ© Abdi and Lynne J Williams. 2010. Principal component analysis. Wiley<--size=7.0-->
interdisciplinary reviews: computational statistics 2, 4 (2010), 433â€“459.<--size=7.0-->
[2] Patrick Aboagye-Sarfo, Qun Mai, Frank M Sanfilippo, David B Preen, Louise M<--size=7.0-->
Stewart, and Daniel M Fatovich. 2015. A comparison of multivariate and univari-<--size=6.9-->
ate time series approaches to modelling and forecasting emergency department<--size=7.0-->
demand in Western Australia. Journal of biomedical informatics 57 (2015), 62â€“73.<--size=6.9-->
[3] M Al-Hajjaji, T ThÃ¼m, J Meinicke, M Lochau ... Software Product Line ..., and<--size=7.0-->
undefined 2014. 2014. Similarity-based prioritization in software product-line<--size=7.0-->
testing. dl.acm.org 1 (sep 2014), 197â€“206. https://doi.org/10.1145/2648511.2648532<--size=6.9-->
[4] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall,<--size=7.0-->
Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann.<--size=7.0-->
2019. Software Engineering for Machine Learning: A Case Study. In Proceedings<--size=6.9-->
- 2019 IEEE/ACM 41st International Conference on Software Engineering: Software<--size=7.0-->
Engineering in Practice, ICSE-SEIP 2019. IEEE, Montreal Quebec Canada, 291â€“300.<--size=6.9-->
https://doi.org/10.1109/ICSE-SEIP.2019.00042<--size=7.0-->
[5] S Apel, D Batory, C KÃ¤stner, and G Saake. 2016. Feature-oriented software product<--size=6.9-->
lines. Springer. https://link.springer.com/content/pdf/10.1007/978-3-642-37521-<--size=6.9-->
7.pdf<--size=7.0-->
[6] Benjamin Benni, Mireille Blay Fornarino, Sebastien Mosser, Frederic Precisio,<--size=7.0-->
and Gunther Jungbluth. 2019. When DevOps meets meta-learning: A portfolio<--size=7.0-->
to rule them all. In Proceedings - 2019 ACM/IEEE 22nd International Conference<--size=7.0-->
on Model Driven Engineering Languages and Systems Companion, MODELS-C<--size=7.0-->
2019. Institute of Electrical and Electronics Engineers Inc., 605â€“612.<--size=7.0-->
https:<--size=7.0-->
//doi.org/10.1109/MODELS-C.2019.00092<--size=7.0-->
[7] Thorsten Berger, Steven She, Rafael Lotufo, Andrzej Wasowski, and Krzysztof<--size=7.0-->
Czarnecki. 2013. A Study of Variability Models and Languages in the Systems<--size=7.0-->
Software Domain. IEEE Transactions on Software Engineering 39, 12 (2013), 1611â€“<--size=6.9-->
1640. https://doi.org/10.1109/TSE.2013.34<--size=7.0-->
[8] Besim Bilalli, Alberto AbellÃ³, and TomÃ s Aluja-Banet. 2017. On the predictive<--size=7.0-->
power of meta-features in OpenML. International Journal of Applied Mathematics<--size=6.9-->
and Computer Science 27, 4 (2017), 697â€“712.<--size=7.0-->
[9] Ane BlÃ¡zquez-GarcÃ­a, Angel Conde, Usue Mori, and Jose A. Lozano. 2021. A<--size=7.0-->
Review on Outlier/Anomaly Detection in Time Series Data. ACM Computing<--size=7.0-->
Surveys (CSUR) 54, 3 (feb 2021), 33. https://doi.org/10.1145/3444690<--size=7.0-->
[10] SÃ©rgio Branco, AndrÃ© G Ferreira, and Jorge Cabral. 2019. Machine learning in<--size=7.0-->
resource-scarce embedded systems, FPGAs, and end-devices: A survey. Electronics<--size=6.9-->
8, 11 (2019), 1289.<--size=7.0-->
[11] Mikel Canizo, Isaac Triguero, Angel Conde, and Enrique Onieva. 2019. Multi-head<--size=7.0-->
CNNâ€“RNN for multi-time series anomaly detection: An industrial case study.<--size=7.0-->
Neurocomputing 363 (2019), 246â€“260.<--size=7.0-->
[12] Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection:<--size=7.0-->
A survey. ACM computing surveys (CSUR) 41, 3 (2009), 1â€“58.<--size=7.0-->
[13] Hoang Anh Dau, Anthony J Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh,<--size=7.0-->
Yan Zhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn J<--size=6.9-->
Keogh. 2018. The UCR Time Series Archive. CoRR abs/1810.07758 (2018), 1â€“12.<--size=7.0-->
http://arxiv.org/abs/1810.07758<--size=7.0-->
[14] Hans Degroote, Bernd Bischl, Lars Kotthoff, and Patrick De Causmaecker. 2016.<--size=7.0-->
Reinforcement Learning for Automatic Online Algorithm Selection - an Empirical<--size=6.9-->
Study. In ITAT 2016 Proceedings, CEUR Workshop Proceedings Vol. 1649 (CEUR<--size=7.0-->
Workshop Proceedings, Vol. 1649), Brona BrejovÃ¡ (Ed.). CEUR-WS.org, 93â€“101.<--size=7.0-->
http://ceur-ws.org/Vol-1649/93.pdf<--size=7.0-->
[15] Xavier Devroey, Gilles Perrouin, Axel Legay, Pierre Yves Schobbens, and Patrick<--size=7.0-->
Heymans. 2015. Covering SPL behaviour with sampled configurations: An initial<--size=6.9-->
assessment. In Proceedings of the Ninth International Workshop on Variability<--size=7.0-->
Modelling of Software-Intensive Systems. ACM Press, Hildesheim, Germany, 59â€“<--size=7.0-->
66. https://doi.org/10.1145/2701319.2701325<--size=7.0-->
[16] Bogdan Dit, Meghan Revelle, Malcom Gethers, and Denys Poshyvanyk. 2013.<--size=7.0-->
Feature location in source code: A taxonomy and survey. Journal of software:<--size=7.0-->
Evolution and Process 25, 1 (jan 2013), 53â€“95. https://doi.org/10.1002/SMR.567<--size=7.0-->
[17] Chris Drummond. 2006. Machine learning as an experimental science (revisited).<--size=7.0-->
In AAAI workshop on evaluation methods for machine learning. AAAI Press,<--size=7.0-->
Phoenix, Arizona USA, 1â€“5.<--size=7.0-->
http://www.aaai.org/Library/Workshops/ws06-<--size=7.0-->
06.php<--size=7.0-->
[18] Sascha El-Sharkawy, Nozomi Yamagishi-Eichler, and Klaus Schmid. 2019. Metrics<--size=7.0-->
for analyzing variability and its implementation in software product lines: A<--size=7.0-->
systematic literature review. Information and Software Technology 106 (feb 2019),<--size=6.9-->
1â€“30. https://doi.org/10.1016/j.infsof.2018.08.015<--size=7.0-->
[19] Dennis ElbrÃ¤chter, Dmytro Perekrestenko, Philipp Grohs, and Helmut BÃ¶lcskei.<--size=7.0-->
2019. Deep neural network approximation theory. CoRR abs/1901.02220 (2019),<--size=7.0-->
1â€“43. http://arxiv.org/abs/1901.02220<--size=7.0-->
[20] Stefan Fischer, Lukas Linsbauer, Roberto E. Lopez-Herrejon, and Alexander Egyed.<--size=7.0-->
2015. The ECCO Tool: Extraction and Composition for Clone-and-Own. In<--size=7.0-->
Proceedings - International Conference on Software Engineering. IEEE, Florence,<--size=7.0-->
Italy, 665â€“668. https://doi.org/10.1109/ICSE.2015.218<--size=7.0-->
[21] Xin He, Kaiyong Zhao, and Xiaowen Chu. 2021. AutoML: A survey of the state-<--size=7.0-->
of-the-art. Knowledge-Based Systems 212 (2021), 106622.<--size=7.0-->

[22] Christopher Henard, Mike Papadakis, Gilles Perrouin, Jacques Klein, Patrick<--size=7.0-->
Heymans, and Yves Le Traon. 2014. Bypassing the combinatorial explosion:<--size=7.0-->
Using similarity to generate and prioritize t-wise test configurations for software<--size=6.9-->
product lines. IEEE Transactions on Software Engineering 40, 7 (2014), 650â€”-670.<--size=7.0-->
https://ieeexplore.ieee.org/abstract/document/6823132/<--size=7.0-->
[23] Ruben Heradio, David Fernandez-Amoros, JosÃ© A. Galindo, David Benavides,<--size=7.0-->
and Don Batory. 2022. Uniform and scalable sampling of highly configurable<--size=7.0-->
systems. Empirical Software Engineering 27, 2 (mar 2022), 44. https://doi.org/10.<--size=6.9-->
1007/s10664-021-10102-5<--size=7.0-->
[24] Mohammad Hossin and Md Nasir Sulaiman. 2015. A review on evaluation<--size=7.0-->
metrics for data classification evaluations. International journal of data mining &<--size=6.9-->
knowledge management process 5, 2 (2015), 1.<--size=7.0-->
[25] Jianglin Huang, Yan-Fu Li, and Min Xie. 2015. An empirical analysis of data<--size=7.0-->
preprocessing for machine learning-based software cost estimation. Information<--size=6.9-->
and software Technology 67 (2015), 108â€“127.<--size=7.0-->
[26] Xiaohui Huang, Yunming Ye, Liyan Xiong, Raymond YK Lau, Nan Jiang, and<--size=7.0-->
Shaokai Wang. 2016. Time series k-means: A new k-means type smooth subspace<--size=6.9-->
clustering for time series data. Information Sciences 367 (2016), 1â€“13.<--size=7.0-->
[27] Christian Kaltenecker, Alexander Grebhahn, Norbert Siegmund, Jianmei Guo,<--size=7.0-->
Sven Apel, and Michael Felderer. 2019. Distance-based sampling of software<--size=7.0-->
configuration spaces. In 2019 IEEE/ACM 41st International Conference on Software<--size=6.9-->
Engineering (ICSE). IEEE Press, 1084â€“1094. https://doi.org/10.1109/ICSE.2019.<--size=7.0-->
00112<--size=7.0-->
[28] T Kehrer, T ThÃ¼m, A Schultheis ... Conference on Software ..., and Undefined<--size=7.0-->
2021. 2021. Bridging the gap between clone-and-own and software product lines.<--size=6.9-->
In 2021 IEEE/ACM 43rd International Conference on Software Engineering: New<--size=7.0-->
Ideas and Emerging Results (ICSE-NIER). IEEE, 21â€“25. https://ieeexplore.ieee.org/<--size=6.9-->
abstract/document/9402254/<--size=7.0-->
[29] Pascal Kerschke, Holger H. Hoos, Frank Neumann, and Heike Trautmann. 2018.<--size=7.0-->
Automated algorithm selection: Survey and perspectives. Evolutionary Computa-<--size=6.9-->
tion 27, 1 (2018), 3â€“45. https://doi.org/10.1162/evco_a_00242 arXiv:1811.11597<--size=7.0-->
[30] Jacob KrÃ¼ger, Wardah Mahmood, and Thorsten Berger. 2020. Promote-pl: a<--size=7.0-->
round-trip engineering process model for adopting and evolving product lines.<--size=7.0-->
In Proceedings of the 24th ACM Conference on Systems and Software Product Line:<--size=6.9-->
Volume A-Volume A, Vol. Part F1642. Association for Computing Machinery,<--size=7.0-->
263â€“273. https://doi.org/10.1145/3382025.3414970<--size=7.0-->
[31] Elias Kuiter, Sebastian Krieter, Jacob KrÃ¼ger, Gunter Saake, and Thomas Leich.<--size=7.0-->
2021. variED: an editor for collaborative, real-time feature modeling. Empirical<--size=7.0-->
Software Engineering 26, 2 (mar 2021), 1â€“47. https://doi.org/10.1007/S10664-020-<--size=6.9-->
09892-X<--size=7.0-->
[32] Luc Lesoil, Hugo Martin, Mathieu Acher, Arnaud Blouin, and Jean-Marc JÃ©zÃ©quel.<--size=7.0-->
2022. Transferring Performance between Distinct Configurable Systems : A Case<--size=6.9-->
Study. Proceedings of the 16th International Working Conference on Variability<--size=7.0-->
Modelling of Software-Intensive Systems 6 (feb 2022), 1â€“6. https://doi.org/10.1145/<--size=6.9-->
3510466.3510486<--size=7.0-->
[33] MaÃ­ra Marques, Jocelyn Simmonds, Pedro O. Rossel, and MarÃ­a Cecilia Bastarrica.<--size=7.0-->
2019. Software product line evolution: A systematic literature review. , 190â€“<--size=7.0-->
208 pages. https://doi.org/10.1016/j.infsof.2018.08.014<--size=7.0-->
[34] Silverio MartÃ­nez-FernÃ¡ndez, Justus Bogner, Xavier Franch, Marc Oriol, Julien<--size=7.0-->
Siebert, Adam Trendowicz, Anna Maria Vollmer, and Stefan Wagner. 2021. Soft-<--size=6.9-->
ware Engineering for AI-Based Systems: A Survey. CoRR abs/2105.0 (may 2021),<--size=6.9-->
54. arXiv:2105.01984 https://arxiv.org/abs/2105.01984v1http://arxiv.org/abs/2105.<--size=6.9-->
01984<--size=7.0-->
[35] Hoan Anh Nguyen, Robert Dyer, Tien N. Nguyen, and Hridesh Rajan. 2014.<--size=7.0-->
Mining preconditions of APIs in large-scale code corpus. In Proceedings of the<--size=7.0-->
ACM SIGSOFT Symposium on the Foundations of Software Engineering (FSE). ACM,<--size=6.9-->
Hong Kong, China, 166â€“177. https://doi.org/10.1145/2635868.2635924<--size=7.0-->
[36] Michael Nieke, Gabriela Sampaio, Thomas ThÃ¼m, Christoph Seidl, Leopoldo<--size=7.0-->
Teixeira, and Ina Schaefer. 2022. Guiding the evolution of product-line con-<--size=7.0-->
figurations.<--size=7.0-->
Software and Systems Modeling 21 (jul 2022), 225â€“247.<--size=7.0-->
https:<--size=7.0-->
//doi.org/10.1007/S10270-021-00906-W/TABLES/5<--size=7.0-->
[37] Lina Ochoa, Juliana Alves Pereira, Oscar GonzÃ¡lez-Rojas, Harold Castro, and<--size=7.0-->
Gunter Saake. 2017. A survey on scalability and performance concerns in ex-<--size=7.0-->
tended product lines configuration. In Proceedings of the Eleventh International<--size=7.0-->
Workshop on Variability Modelling of Software-intensive Systems. Association for<--size=6.9-->
Computing Machinery, 5â€“12. https://doi.org/10.1145/3023956.3023959<--size=7.0-->
[38] Pascal Olz, Conny and Biundo, Susanne and Bercher. 2021. Revealing Hidden<--size=7.0-->
Preconditions and Effects of Compound HTN Planning Tasksâ€“A Complexity<--size=7.0-->
Analysis. In 35th AAAI Conference on Artificial Intelligence (AAAI). AAAI Press.<--size=7.0-->
AAAI Press, Virtual Event, 1903â€“11912. https://www.aaai.org/AAAI21Papers/<--size=7.0-->
AAAI-655.OlzC.pdf<--size=7.0-->
[39] OMG. 2006.<--size=7.0-->
Business Process Modeling Notation (BPMN) Specification, Final<--size=7.0-->
Adopted Specification. Technical Report. Object Management Group (OMG).<--size=7.0-->
[40] S. Gopal Krishna Patro and Kishore Kumar Sahu. 2015. Normalization: A Prepro-<--size=7.0-->
cessing Stage. CoRR abs/1503.06462 (2015), 1â€“3. http://arxiv.org/abs/1503.06462<--size=6.9-->
[41] Juliana Alves Pereira, Sebastian Krieter, Jens Meinicke, Reimar SchrÃ¶ter, Gunter<--size=7.0-->
Saake, and Thomas Leich. 2016. FeatureIDE: Scalable product configuration of<--size=7.0-->
variable systems. In International Conference on Software Reuse, Lecture Notes in<--size=7.0-->

<--page_end:11-->

<--page_start:12-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
El Amraoui et al.<--size=7.0-->

Computer Science, Vol. 9679. Springer Verlag, 397â€“401. https://doi.org/10.1007/<--size=7.0-->
978-3-319-35122-3_27<--size=7.0-->
[42] Juliana Alves Pereira, Pawel Matuszyk, Sebastian Krieter, Myra Spiliopoulou,<--size=7.0-->
and Gunter Saake. 2018. Personalized recommender systems for product-line<--size=7.0-->
configuration processes. Computer Languages, Systems and Structures 54 (2018),<--size=7.0-->
451â€“471. https://doi.org/10.1016/j.cl.2018.01.003<--size=7.0-->
[43] Nelishia Pillay, Rong Qu, Dipti Srinivasan, Barbara Hammer, and Kenneth<--size=7.0-->
Sorensen. 2018. Automated design of machine learning and search algorithms<--size=7.0-->
[guest editorial]. IEEE Computational intelligence magazine 13, 2 (2018), 16â€“17.<--size=7.0-->
[44] Klaus Pohl, GÃ¼nter BÃ¶ckle, and Frank J van der Linden. 2005. Software Product<--size=7.0-->
Line Engineering: Foundations, Principles and Techniques. Springer-Verlag.<--size=7.0-->
[45] BelÃ©n Ramos-GutiÃ©rrez, Ãngel JesÃºs Varela-Vaca, JosÃ© A. Galindo, MarÃ­a Teresa<--size=7.0-->
GÃ³mez-LÃ³pez, and David Benavides. 2021. Discovering configuration workflows<--size=6.9-->
from existing logs using process mining. Empir. Softw. Eng. 26, 1 (jan 2021), 11.<--size=7.0-->
https://doi.org/10.1007/s10664-020-09911-x<--size=7.0-->
[46] Erich Schubert, JÃ¶rg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei Xu.<--size=7.0-->
2017. DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.<--size=6.9-->
ACM Transactions on Database Systems (TODS) 42, 3 (2017), 1â€“21.<--size=7.0-->
[47] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar<--size=7.0-->
Ebner, Vinay Chaudhary, and Michael Young. 2015. Machine Learning: The High<--size=6.9-->
Interest Credit Card of Technical Debt. In Proceedings of the 28th International<--size=7.0-->
Conference on Neural Information Processing Systems - Volume 2, Ghahramani<--size=7.0-->
Zoubin, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger<--size=6.9-->
(Eds.). MIT Press, Montreal, Canada, 2503â€“2511. https://ai.google/research/pubs/<--size=6.9-->
pub43146<--size=7.0-->
[48] Marina Sokolova, Nathalie Japkowicz, and Stan Szpakowicz. 2006. Beyond ac-<--size=7.0-->
curacy, F-score and ROC: a family of discriminant measures for performance<--size=7.0-->
evaluation. In Australasian joint conference on artificial intelligence. Springer,<--size=7.0-->

Springer, Berlin, Heidelberg, 1015â€“1021.<--size=7.0-->
[49] Leopoldo Teixeira, Rohit Gheyi, and Paulo Borba. 2020. Safe Evolution of Product<--size=7.0-->
Lines Using Configuration Knowledge Laws. In Brazilian Symposium on Formal<--size=7.0-->
Methods, Lecture Notes in Computer Science, Vol. 12475 LNCS. Springer, Cham,<--size=7.0-->
210â€“227. https://doi.org/10.1007/978-3-030-63882-5_13<--size=7.0-->
[50] A Tornhill. 2015. Your Code as a Crime Scene. Pragmatic Bookshelf.<--size=7.0-->
https:<--size=7.0-->
//books.google.fr/books?id=l7dDnQAACAAJ<--size=7.0-->
[51] Mathias Uta, Alexander Felfernig, Viet Man Le, Andrei Popescu, Thi Ngoc Trang<--size=7.0-->
Tran, and Denis Helic. 2021. Evaluating recommender systems in feature model<--size=6.9-->
configuration. In Proceedings of the 25th ACM International Systems and Software<--size=6.9-->
Product Line Conference, Vol. Part F1716. ACM, New York, NY, USA, 58â€“63. https:<--size=6.9-->
//doi.org/10.1145/3461001.3471144<--size=7.0-->
[52] Jan N. Van Rijn and Joaquin Vanschoren. 2015. Sharing RapidMiner workflows<--size=7.0-->
and experiments with OpenML. In CEUR Workshop Proceedings, Vol. 1455. CEUR-<--size=6.9-->
WS, 93â€“103.<--size=7.0-->
[53] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. 2013. OpenML:<--size=7.0-->
Networked Science in Machine Learning. SIGKDD Explorations 15, 2 (2013), 49â€“60.<--size=6.9-->
https://doi.org/10.1145/2641190.2641198 arXiv:1407.7722<--size=7.0-->
[54] C Wohlin, P Runeson, M HÃ¶st, MC Ohlsson, and B Regnell. 2012.<--size=7.0-->
Experimentation<--size=7.0-->
in<--size=7.0-->
software<--size=7.0-->
engineering.<--size=7.0-->
Springer.<--size=7.0-->
1â€“236<--size=7.0-->
pages.<--size=7.0-->
https://books.google.com/books?hl=fr&lr=&id=QPVsM1_U8nkC&oi=fnd&pg=<--size=7.0-->
PR5&dq=Experimentation+in+Software+Engineering.&ots=GPx7rciRCu&sig=<--size=7.0-->
KyBLRUIbGY48ZlXMyE9nRVCbP_o<--size=7.0-->
[55] Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2022.<--size=7.0-->
Machine Learn-<--size=7.0-->
ing Testing: Survey, Landscapes and Horizons. IEEE Transactions on Software<--size=7.0-->
Engineering 48, 01 (jan 2022), 1â€“36. https://doi.org/10.1109/TSE.2019.2962027<--size=7.0-->
arXiv:1906.10742<--size=7.0-->

<--page_end:12-->

<--page_start:1-->
Evolvable SPL management with partial knowledge: an<--size=17.2-->
application to anomaly detection in time series<--size=17.2-->

Yassine El Amraoui<--size=12.0-->
UniversitÃ© CÃ´te dâ€™Azur, CNRS, I3S<--size=10.0-->
EZAKO<--size=10.0-->
Sophia Antipolis, France<--size=10.0-->
yassine.elamraoui@ezako.com<--size=10.0-->

Mireille Blay-Fornarino<--size=12.0-->
UniversitÃ© CÃ´te dâ€™Azur, CNRS, I3S<--size=10.0-->
Sophia Antipolis, France<--size=10.0-->
Mireille.blay@univ-cotedazur.fr<--size=10.0-->

Philippe Collet<--size=12.0-->
UniversitÃ© CÃ´te dâ€™Azur, CNRS, I3S<--size=10.0-->
Sophia Antipolis, France<--size=10.0-->
philippe.collet@univ-cotedazur.fr<--size=10.0-->

FrÃ©dÃ©ric Precioso<--size=12.0-->
UniversitÃ© CÃ´te dâ€™Azur, Inria, CNRS,<--size=10.0-->
I3S<--size=10.0-->
Sophia Antipolis, France<--size=10.0-->
frederic.precioso@univ-cotedazur.fr<--size=10.0-->

Julien Muller<--size=12.0-->
EZAKO<--size=10.0-->
Sophia Antipolis, France<--size=10.0-->
julien.muller@ezako.com<--size=10.0-->

ABSTRACT<--size=10.9-->

In Machine Learning (ML), the resolution of anomaly detection<--size=9.1-->
problems in time series presents a great diversity of practices as it<--size=8.9-->
can correspond to many different contexts. These practices cover<--size=9.0-->
both grasping the business problem and designing the solution<--size=9.1-->
itself. By practice, we designate explicit and implicit steps toward<--size=9.0-->
resolving a problem, while a solution corresponds to a combination<--size=8.9-->
of algorithms selected for their performance on a given problem.<--size=9.1-->
Two related issues arise. The first one is that the practices are<--size=9.1-->
individual and not explicitly mutualized. The second one is that<--size=9.1-->
choosing one solution over another is all the more difficult to justify<--size=8.9-->
because the space of solutions and the evaluation criteria are vast<--size=9.0-->
and evolve rapidly with the advances in ML. To solve these issues<--size=9.0-->
and tame the evolving diversity in ML, a Software Product Line<--size=9.1-->
(SPL) approach can be envisaged to represent the variable set of<--size=9.1-->
solutions. However, this requires characterizing an ML business<--size=9.1-->
problem through an explicit set of criteria and justifying one ML<--size=9.1-->
solution over all others. The resolution of anomaly detection prob-<--size=8.9-->
lems is thus different from finding the best configuration workflow<--size=8.9-->
from past configurations but lies more in guiding the configuration<--size=8.9-->
towards a solution that may never have been studied before. This<--size=9.0-->
paper proposes an SPL approach that capitalizes on past practices<--size=9.0-->
by exploiting a variability-aware representation to detect new cri-<--size=9.0-->
teria and constraints when practices adopt different solutions to<--size=9.1-->
seemingly similar problems. We report on the evaluation of our<--size=9.1-->
approach using a set of applications from the literature and an ML<--size=8.9-->
software company. We show how the analysis of practices makes it<--size=8.9-->
possible to consolidate the knowledge contained in the SPL.<--size=9.0-->

Permission to make digital or hard copies of all or part of this work for personal or<--size=7.0-->
classroom use is granted without fee provided that copies are not made or distributed<--size=6.9-->
for profit or commercial advantage and that copies bear this notice and the full citation<--size=6.9-->
on the first page. Copyrights for components of this work owned by others than the<--size=7.0-->
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or<--size=6.9-->
republish, to post on servers or to redistribute to lists, requires prior specific permission<--size=6.9-->
and/or a fee. Request permissions from permissions@acm.org.<--size=7.0-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.<--size=7.0-->
ACM ISBN 978-1-4503-9443-7/22/09...$15.00<--size=7.0-->
https://doi.org/10.1145/3546932.3547008<--size=7.0-->

CCS CONCEPTS<--size=10.9-->

â€¢ Software product line; â€¢ Machine learning; â€¢ Configurations;<--size=8.9-->

KEYWORDS<--size=10.9-->

Software Product Line, Machine Learning, Evolution, Metrics<--size=9.0-->

ACM Reference Format:<--size=8.0-->
Yassine El Amraoui, Mireille Blay-Fornarino, Philippe Collet, FrÃ©dÃ©ric Pre-<--size=8.0-->
cioso, and Julien Muller. 2022. Evolvable SPL management with partial<--size=8.0-->
knowledge: an application to anomaly detection in time series. In 26th<--size=8.0-->
ACM International Systems and Software Product Line Conference - Volume A<--size=7.9-->
(SPLC â€™22), September 12â€“16, 2022, Graz, Austria. ACM, New York, NY, USA,<--size=7.9-->
12 pages. https://doi.org/10.1145/3546932.3547008<--size=8.0-->

1<--size=10.9-->
INTRODUCTION<--size=10.9-->

Building learning systems are increasingly complex, as industry<--size=9.1-->
data, human and organizational factors, and application domains<--size=9.0-->
define different contexts that require tailored practices[11]. By prac-<--size=8.9-->
tices, for the Machine Learning (ML) community, we mean the<--size=9.1-->
entire process of producing ML workflows, from analyzing the cus-<--size=8.9-->
tomerâ€™s data, business goals, and constraints to delivering the ML<--size=9.0-->
model built by composing algorithms. To address this variability<--size=9.1-->
of contexts, data scientists are developing a great deal of expertise,<--size=8.9-->
including developing dedicated algorithms within companies and<--size=9.0-->
tracking the evolution of theory and practice through literature<--size=9.1-->
and collaborations with researchers. However, with the profusion<--size=9.0-->
of algorithms and the diversity of industry problems, connecting<--size=9.0-->
problems to appropriate practices requires increasing capabilities<--size=9.0-->
and resources.<--size=9.0-->
To make this connection between real-world problems and undis-<--size=8.9-->
covered scientific knowledge, we chose to focus on time series<--size=9.1-->
anomaly detection, such as stock price outlier detection, which<--size=9.1-->
presents a wide variety of challenges and practices [11, 25]. Scien-<--size=9.0-->
tific knowledge in this area remains to be discovered as the data<--size=9.1-->
and application domains require developing new solutions. While<--size=9.0-->
building an ML model involves a composition of algorithms that<--size=9.1-->
takes a long time to design and test, the available experiments only<--size=8.9-->
partially cover the large variability of the domain, especially in<--size=9.1-->
industrial applications (see Section 2.1).<--size=9.0-->

<--image width=1003.0 height=1004.0-->
<--page_end:1-->

<--page_start:2-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
El Amraoui et al.<--size=7.0-->

In this paper, we argue that a Software Product Line (SPL) ap-<--size=9.1-->
proach allows for linking partial configurations of ML problems<--size=9.1-->
with appropriate workflows. The originality of the approach is<--size=9.1-->
to exploit past configurations to enrich the knowledge captured<--size=9.1-->
by the SPL. In this context, we identify the following functional<--size=9.1-->
requirements.<--size=9.0-->
R1- Identifying similarities between partially described problems.<--size=9.0-->
Looking for similarities and differences with previous problems is a<--size=8.9-->
natural first thought for data scientists but remains a difficult task.<--size=8.9-->
Indeed, the nature of the source data often makes their characteri-<--size=9.0-->
zation challenging, especially since precisely defining an anomaly<--size=9.0-->
in a time series can be difficult, including for the customer who<--size=9.1-->
delivered the data. Being able to deal with partially characterized<--size=9.0-->
source data is therefore mandatory, including proposing algorithms<--size=8.9-->
that will be able to manage the variability of the data sets, for ex-<--size=9.0-->
ample, when the type of anomaly is not known (singular points,<--size=9.1-->
global anomalies, or pattern anomalies [12]).<--size=9.0-->
R2-Consolidating knowledge according to the evolution of prac-<--size=9.1-->
tices. Considering new solutions (i.e., new ML workflow) from the<--size=9.0-->
literature requires characterizing the boundaries of the problems<--size=9.0-->
targeted by these solutions. Thus, it is not only a matter of selecting<--size=8.9-->
new algorithms but also new criteria such as evaluation metrics or<--size=8.9-->
business requirements. [8, 17]. For instance, detecting anomalies<--size=9.0-->
in scarce resource environments such as IoT embedded systems,<--size=9.1-->
has an impact on the entire ML model production chain [10, 43].<--size=9.1-->
To consolidate the knowledge, we must be able to compare appli-<--size=9.0-->
cations. By application, we mean not only the solutions and their<--size=9.0-->
performances, but also the targeted problems, i.e., the data and the<--size=8.9-->
business requirements. In particular, we want to identify data sets<--size=9.0-->
and business requirements that appear similar but have different<--size=9.0-->
solutions to highlight new criteria or the obsolescence of some past<--size=8.9-->
criteria. For example, in the literature, when, for the same dataset,<--size=9.0-->
two different compositions of algorithms work well, it is interesting<--size=8.9-->
to identify, if possible, which requirement criteria could distinguish<--size=8.9-->
them.<--size=9.0-->
To highlight these requirements, we propose three scenarios in<--size=9.0-->
which Lucile, a data scientist persona, uses our framework named<--size=9.0-->
ROCKâ€™n RWL1(RRW).<--size=9.0-->

Scenario 1:Lucile uses RRW to search for a solution to a new anom-<--size=8.9-->
aly detection problem over a given dataset. Through a dedicated<--size=9.1-->
interface, Alice indicates the business requirements and some ad-<--size=9.0-->
ditional information about the dataset. RRW narrows the solution<--size=9.0-->
space to the suitable components and selects the most relevant<--size=9.1-->
ones. In addition, if previous applications match the same criteria,<--size=9.0-->
RRW helps Alice compare and browse them as her analysis evolves.<--size=8.9-->

Scenario 2: Lucile wishes to enrich the SPL with a new set of appli-<--size=8.9-->
cations that she considers interesting. RRW analyses the information<--size=8.9-->
related to these applications. After checking that it does not contra-<--size=8.9-->
dict previous knowledge, RRW makes them available to other data<--size=8.9-->
scientists.<--size=9.0-->

Scenario 3: Lucile wants to evaluate the SPL.. RRW informs her<--size=9.1-->
of the equivalences between the descriptions of the data sets, the<--size=9.0-->
business requirements, and the solutions of the various registered<--size=9.0-->
applications. RRW can then draw her attention to various issues.<--size=9.1-->

1Request your Own Convenient Knowledge flow and Run your ML WorkfLows<--size=7.0-->

For example, RRW points out applications that address similar prob-<--size=8.9-->
lems, i.e., seemingly identical business requirements and dataset<--size=9.1-->
characteristics, but use different solutions. It also identifies criteria<--size=8.9-->
that are never used or always used. These warnings are intended<--size=9.0-->
to help identifying new data spaces to be tested, for example, new<--size=9.0-->
criteria for comparing problems, and solution updates.<--size=9.0-->

Research Vision and approach. The exploratory nature of ma-<--size=9.1-->
chine learning makes an exhaustive analysis of the domain difficult,<--size=8.9-->
if not impossible. According to Drummond [17] and from our own<--size=8.9-->
experience, "any advantage indicated by a simple scalar measure<--size=9.1-->
may be illusory if it hides situation-dependent performance differ-<--size=9.1-->
ences." Despite the generalization power of ML and the substantial<--size=8.9-->
evolution of the field, we advocate that SPLs are well suited to<--size=9.1-->
explore this complexity of dependencies between data, business<--size=9.1-->
goals, and algorithm composition. Our contribution then concerns:<--size=8.9-->
â€¢ Exploiting the SPL to guide the data scientist in narrowing<--size=9.0-->
the solution space and more easily pinpointing past solutions<--size=8.9-->
that solved similar problems.<--size=9.0-->
â€¢ Leveraging the SPL to reason about past solutions, making<--size=9.0-->
new knowledge explicit, and exploiting it to consolidate the<--size=8.9-->
SPL.<--size=9.0-->
The principle is then the following. The configurations of the<--size=9.1-->
applications in the SPL incrementally capture our partial knowl-<--size=9.1-->
edge of the problems and solutions. The SPL progressively supports<--size=8.9-->
capitalizing on what is not explicit by reasoning about these con-<--size=9.0-->
figurations. Then the main issue is not to determine the configura-<--size=8.9-->
tion workflow that best suits the actors according to the previous<--size=9.0-->
configurations [45], but to guide them in composing a solution<--size=9.1-->
for an unprecedentedly studied problem. Concurrently, it is not<--size=9.1-->
a question of generating random samples [23], whose relevance<--size=9.1-->
could not be precisely verified (e.g., stuffing the SPL with all the<--size=9.1-->
available algorithms and pre-processing components from the liter-<--size=8.9-->
ature). Instead, it is more a matter of enriching our knowledge by<--size=9.0-->
systematically studying new validated configurations.<--size=9.0-->
This paper shows how we apply these principles in construct-<--size=9.1-->
ing an SPL for anomaly detection in time series. We explain the<--size=9.1-->
difficulties specific to this domain of ML and why we consider that<--size=8.9-->
an adapted SPL can help in its analysis (see Section 2). Then we<--size=9.1-->
present the principles of the SPL, particularly the patterns used<--size=9.1-->
to identify knowledge from past applications (see Section 3). We<--size=9.1-->
validate this proposal on the first three steps of the SPL construc-<--size=9.0-->
tion. The first phase consists in building the SPL proactively by<--size=9.1-->
domain analysis. Then we enrich the SPL with practices extracted<--size=9.0-->
from the partner company, and a third phase adds some applica-<--size=9.1-->
tions of OpenML [53] to the SPL. We then discuss the limits and<--size=9.1-->
perspectives of the approach (see Section 5) before concluding this<--size=8.9-->
paper.<--size=9.0-->

2<--size=10.9-->
FROM PARTIAL KNOWLEDGE TO AN SPL<--size=10.9-->

Designing ML workflow has become essential to almost any scien-<--size=8.9-->
tific field with Deep Learning advances in the past decade. The field<--size=8.9-->
increased fast and in many directions based on different models,<--size=9.1-->
yet it is not a rich and shared knowledge enough to be organized<--size=9.0-->
and made accessible to non-experts. Most of the knowledge one can<--size=8.9-->
hold is partial, shared through non-conventional channels such as<--size=8.9-->
personal blogs of other data scientists, webinars, and tricks shared<--size=8.9-->

<--page_end:2-->

<--page_start:3-->
Evolvable SPL management with partial knowledge: an application to anomaly detection in time series<--size=7.0-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->

orally during conferences. This situation does not help to adopt<--size=9.1-->
these new techniques efficiently, particularly in companies. From<--size=9.0-->
our point of view, software engineering should play a more vital<--size=9.1-->
role in solving this central issue.<--size=9.0-->

2.1<--size=10.9-->
ML Workflows for anomaly detection in<--size=10.9-->
time series<--size=10.9-->

Defining anomalies in a given business context requires business<--size=9.0-->
goals and constraints to be precised. Depending on the anomaly<--size=9.1-->
detection problem, it remains challenging to construct appropri-<--size=9.1-->
ate workflows [4] because the interactions between the current<--size=9.1-->
data, the composition of algorithms, and the business requirements<--size=8.9-->
are substantial and not always well understood. Sculley et al. sum-<--size=8.9-->
marise these interactions as follows: "changing anything, changes<--size=9.0-->
everything" [47].<--size=9.0-->
Events predicted as statistically abnormal by the model may<--size=9.1-->
not be relevant anomalies for the end-user, if they are unrelated<--size=9.1-->
to business requirements, as for instance sensor failures. In order<--size=9.0-->
for the model to distinguish relevant from non-relevant anomalies,<--size=8.9-->
selecting the proper data preparation algorithm is then part of the<--size=8.9-->
final ML workflow solution and often a crucial part of it.<--size=9.0-->
Testing the variability of workflows is all the more problematic<--size=9.0-->
as the resources required to train models can be very important in<--size=8.9-->
terms of time, memory, computation, but also in terms of human<--size=9.0-->
investment, and not only from data scientists. Indeed, in the ab-<--size=9.1-->
sence of a normality reference or threshold, decisions on whether<--size=9.0-->
values are abnormal or not have to be made by end-users, which<--size=9.1-->
is time-consuming. Therefore, it is essential to reduce the solution<--size=8.9-->
space to those more appropriate to solve the problem. However,<--size=9.1-->
identifying the problem itself can also be complex and resource-<--size=9.1-->
consuming. Thus, even if deep learning-based solutions were the<--size=9.0-->
solution to all problems [19], it would still be necessary to take into<--size=8.9-->
account the variability of upstream processing to prepare the data<--size=8.9-->
and downstream processing to maintain the models in production.<--size=8.9-->

2.2<--size=10.9-->
On meta-learning and AutoML<--size=10.9-->

Automated machine learning methods (AutoML) have been pro-<--size=9.1-->
posed and are focusing the efforts of many industries and research<--size=8.9-->
teams [21]. However, most AutoML algorithms aim only at solving<--size=8.9-->
a specific problem on specific datasets and do not provide end-<--size=9.1-->
users with the ability to acquire reasoned knowledge. Therefore,<--size=9.1-->
these systems are high-value solution components that we have<--size=9.1-->
introduced into the SPL to solve specific problems. More generally,<--size=8.9-->
many SE4AI2works addressed the issue of classification workflow<--size=8.9-->
selection in a generic way like in the work of MartÃ­nez-FernÃ¡ndez<--size=8.9-->
et al. [34] or by using a meta-learning-based portfolio like in the<--size=9.1-->
work of Kerschke et al. [29] and Degroote et al. [14]. Furthermore,<--size=8.9-->
these automated approaches entail massive needs for computation,<--size=8.9-->
memory, and time resources. One standard solution is to limit the<--size=9.0-->
solution space on which to train: measure choice, algorithms, or<--size=9.1-->
composition of algorithms.<--size=9.0-->
However, we aim at the contrary at enriching our SPL by regu-<--size=9.0-->
larly adding new algorithms, new business requirements criteria,<--size=9.0-->
etc. We also aim to help in the evaluation measure choice, according<--size=8.9-->
to the prediction performance [24] and ensure that the solutions<--size=9.1-->

2Software Engineering for AI<--size=5.5-->

deployed will scale in production [48], in particular, in anomaly<--size=9.1-->
detection, automation is difficult since end-users must validate<--size=9.1-->
anomalies, as explained in the previous section. We are advocating<--size=8.9-->
a reverse approach that is drastically less computationally, memory-<--size=8.9-->
intensive, and more suitable for scientific and reasoned knowledge<--size=8.9-->
acquisition directed by and for humans.<--size=9.0-->

2.3<--size=10.9-->
Towards an evolvable SPL<--size=10.9-->

This work is based on a collaboration between academic researchers<--size=8.9-->
in software engineering and data scientists from a company provid-<--size=8.9-->
ing ML workflows for business customers. The work thus targets<--size=9.0-->
various applications, involving diverse industrial datasets and busi-<--size=8.9-->
ness problems.<--size=9.0-->
Despite the constantly evolution of the domain (not to say the<--size=9.0-->
volatility of the domain), everything changes ... in an unpredictable<--size=9.0-->
way [44], choosing to capitalize on the different solutions designed<--size=8.9-->
by data scientists through an SPL seemed to be the best option. Our<--size=8.9-->
interviews revealed that, based on their experience, the domain<--size=9.1-->
experts had already identified most of the main dimensions of vari-<--size=8.9-->
abilities and commonalities of their domain. However, the domain<--size=8.9-->
analysis quickly showed that a proactive adoption scenario was not<--size=8.9-->
suitable, it is not yet possible to strictly separate domain analysis<--size=9.0-->
from the practices of data scientists seeking to solve new problems.<--size=8.9-->
Therefore, we have opted for a "reactive adoption strategy" for the<--size=8.9-->
product line and managed its evolution by integrating the practices<--size=8.9-->
of data scientists [30].<--size=9.0-->
Effectively managing the evolution of variant-rich software in-<--size=9.0-->
volves bridging the gap between software solutions and the capture<--size=8.9-->
of domain variability [28]. However, in our case, while data variabil-<--size=8.9-->
ity is partially identifiable automatically [8], the context variability<--size=8.9-->
is not entirely identifiable from software solutions. Our goal is<--size=9.1-->
therefore to obtain this non-explicit information with a minimum<--size=9.0-->
of manual effort. To this end, we introduce into the round-trip en-<--size=9.0-->
gineering process proposed by Promote-pl [30], a feedback phase<--size=9.0-->
on the content of the SPL itself. This consists in identifying, when<--size=8.9-->
integrating new applications, those that can provide new informa-<--size=9.0-->
tion by comparison with past applications. To meet this objective,<--size=9.0-->
we started from the following assumption: "Any customer can gen-<--size=8.9-->
erate the software they want, as long as they can describe it in<--size=9.1-->
the SPL"3. We use this assumption as a postulate to investigate the<--size=8.9-->
practices, and to hopefully identify new knowledge. The principle<--size=9.0-->
is that if two equivalent descriptions of problems correspond to<--size=9.1-->
two different solutions, then it is not the same problem; otherwise,<--size=8.9-->
we would not know which software to generate. Thus integrating<--size=9.0-->
an application to the SPL involves interactions with data scientists<--size=8.9-->
only when it is not possible to distinguish the contexts that led to<--size=9.0-->
two different solutions.<--size=9.0-->
Evolving an SPL in an ad hoc manner is error-prone because the<--size=8.9-->
configuration space is large and involves taking into account many<--size=8.9-->
interdependent artefacts. Thus, the definition of a reactive approach<--size=8.9-->
integrates the need to foresee "a typical pattern for maintaining<--size=9.1-->
and evolving a product line during its lifetime" [5]. In [49], while<--size=9.1-->
defining safe evolution the authors state "that the resulting SPL<--size=9.1-->
must be able to generate products that behaviorally match all of<--size=9.1-->
the original SPL products". We are in a slightly simpler application<--size=9.0-->

3Loosely adapted from Henry Ford.<--size=5.5-->

<--page_end:3-->

<--page_start:4-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
El Amraoui et al.<--size=7.0-->

context since we aim at composing only a unique stated version of<--size=8.9-->
each algorithm. It is not the productsâ€™ behavior that changes, but<--size=9.0-->
the logic of assembling the workflows that evolves. Our objective<--size=9.0-->
is, thus, to detect configurations that are no longer valid due to<--size=9.1-->
changes in the SPL, possibly because of a past error.<--size=9.0-->

2.4<--size=10.9-->
From the requirements to the SPL paradigm<--size=10.9-->

According to newly identified practices, the only artefacts that<--size=9.1-->
evolve in our SPL are the feature model (i.e., CUD4operations on<--size=9.0-->
features, feature groups, and constraints) and the assets by addition<--size=8.9-->
or removal of algorithms and workflows (i.e., CD operations). The<--size=9.0-->
configuration knowledge does not evolve as such. It consists only<--size=9.0-->
of bijections between the algorithmsâ€™ code and the corresponding<--size=9.0-->
feature. Analyzing the impact of the evolution operations on past<--size=9.0-->
configurations is part of our perspectives inspired by the prelimi-<--size=9.0-->
nary work of Nieke et al. [36].<--size=9.0-->
Besides, given the very high variability of the domain and its<--size=9.1-->
constant evolution, it is neither possible, at least for the time be-<--size=9.1-->
ing, to consider sampling techniques for workflows on which to<--size=9.1-->
learn [27], nor to support the configuration process in an optimal<--size=9.0-->
way [37].<--size=9.0-->
To answer the requirements stated in the introduction, we refor-<--size=8.9-->
mulate them into technical requirements (RT) for an SPL dedicated<--size=8.9-->
to the composition of ML workflows in the context of anomaly de-<--size=8.9-->
tection in time series. To meet R1, the solution search corresponds<--size=8.9-->
to configuring a feature model, producing a valid configuration,<--size=9.1-->
partial in the problem specification and complete in the solution def-<--size=8.9-->
inition (RT1.1). The configurations must be comparable on the sub-<--size=8.9-->
spaces: the dataset description, the business requirements, and the<--size=8.9-->
solution components (RT1.2). The evolution of knowledge-driven<--size=9.0-->
by practices (R2) requires that the criteria of the domain analysis<--size=9.0-->
evolve without impacting the solution space (RT2.1). We must set<--size=9.0-->
up comparison patterns among configurations corresponding to<--size=9.1-->
the applications to guide the discovery of new knowledge (RT2.2).<--size=8.9-->

3<--size=10.9-->
DESIGNING AN EVOLVABLE SPL WITH<--size=10.9-->
PARTIAL KNOWLEDGE<--size=10.9-->

The RRW SPL defines a set of ML practices with well-defined varia-<--size=8.9-->
bilities and commonalities. A combination of features (i.e., config-<--size=9.0-->
uration) identifies each product, and results in an application, i.e.,<--size=9.0-->
an ML workflow with its performance, its evaluation strategy, its<--size=9.0-->
deployment environment, etc. The format of the applications varies<--size=8.9-->
from notebooks, references to runs in OpenML, and references in<--size=9.0-->
the company tool. The set of valid feature combinations is specified<--size=8.9-->
in a feature model (FM) whose structure aims at facilitating the SPL<--size=8.9-->
evolution. Implementation artefacts are essentially references to<--size=9.1-->
algorithms and workflow models expressed in BPMN [39]. Map-<--size=9.1-->
pings express the relationships between solution features and these<--size=8.9-->
artefacts. The SPL supports the generation of BPMN workflows<--size=9.1-->
based on the selected workflow model and algorithms. Since the<--size=9.1-->
construction of the SPL depends on the new applications created,<--size=9.0-->
we set up different mechanisms to control its evolution.<--size=9.0-->
We developed tools to validate the overall process: (i) configu-<--size=9.1-->
ration and search of past applications (configurator), (ii) generation<--size=8.9-->
of ML workflows (generator), (iii) integration of applications in<--size=9.1-->

4create (C), update (U) or delete (D) operation [33]<--size=5.5-->

the SPL (integrator), (iv) reconfiguration of past configurations (re-<--size=8.9-->
configurator), (v) evaluation of the knowledge carried by the SPL<--size=9.0-->
concerning the recorded applications (analyzer). We only present<--size=9.0-->
the concepts in this article. The configurator dynamically reads the<--size=8.9-->
feature model and a CSV file with helpful information for present-<--size=8.9-->
ing the features (question, description, links to external elements).<--size=9.0-->
To help data scientists understand the feature selection, we as-<--size=9.1-->
sociate descriptions with the constraints related to the selected<--size=9.1-->
features during the configuration. The possibility to import/export<--size=8.9-->
configurations allows proceeding by enrichment and, in the case of<--size=8.9-->
reconfiguration, manually adapting the past problematic configu-<--size=9.0-->
rations. The reusable artifacts are then the previous experiments<--size=9.1-->
(codes and configurations to adapt), a set of algorithms and work-<--size=9.0-->
flow models. The reusable artifacts are, therefore, the previous<--size=9.1-->
experiments (codes and configurations to adapt), a set of algorithms,<--size=8.9-->
and workflow models.<--size=9.0-->

3.1<--size=10.9-->
FM structure to tame the SPL evolution<--size=10.9-->

We structure the knowledge captured by the FM according to six<--size=9.0-->
main concepts, which organized the top of the FM hierarchy as de-<--size=8.9-->
picted in figure 1. Information about the data sources, the business<--size=8.9-->
requirements, and the solution are mandatory as they are required<--size=8.9-->
to identify new applications. Information sources, states, and appli-<--size=8.9-->
cations help the user in her analysis; they are optional. The numbers<--size=8.9-->
correspond in our case study to the number of features under each<--size=8.9-->
branch. The following paragraphs detail the content of the FM.<--size=9.0-->

Data set properties. InitialData branch of the FM characterizes<--size=9.0-->
the space of datasets containing time series. Some properties are<--size=9.1-->
automatically extracted from the dataset (the sampling frequency,<--size=9.0-->
the time series number of dimensions, the stationarity verdict,...),<--size=9.0-->
while we can only get others through interacting with the business<--size=8.9-->
expert, such as how to interpret the missing values. No outgoing<--size=9.0-->
constraints from this branch to another branch are allowed since the<--size=8.9-->
dataset properties do not inherently imply algorithms or business<--size=9.0-->
requirements (RT2.1). For example, in our case study, this branch<--size=9.0-->
under the feature InitialData contains 28 other features.<--size=9.0-->

Business requirement characteristics. BusinessRequirements bran-<--size=8.9-->
ch captures requirements, such as limited memory usage to comply<--size=8.9-->
with hardware constraints or the solutionâ€™s ability to provide expla-<--size=8.9-->
nations.<--size=9.0-->

Solution components & states. The Solution branch groups and<--size=9.1-->
structures the algorithms used for solving anomaly detection pro-<--size=9.0-->
blems and the types of workflows used in learning and deployment.<--size=8.9-->
The states branch represents the states through which the data<--size=9.1-->
passes. We express preconditions and the impact of a solution<--size=9.1-->
component through constraints relative to a state. Therefore, we<--size=9.1-->
forbid solution components to refer directly to the features of the<--size=9.0-->
initial data set but only to the corresponding state (RT2.1). For<--size=9.1-->
instance, an algorithm can require the state of the data to be scaled<--size=8.9-->
but not need that to be the initial state of the data.<--size=9.0-->

Application & sources. The Application branch is only used in<--size=9.1-->
the configurator to facilitate direct access to past applications by<--size=9.1-->
filtering them according to the initial problem or the components of<--size=8.9-->
the Solution used. Similarly, the Sources branch helps remember<--size=9.0-->

<--page_end:4-->

<--page_start:5-->
Evolvable SPL management with partial knowledge: an application to anomaly detection in time series<--size=7.0-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->

from which literature article some features and constraints have<--size=9.1-->
been extracted and who are the authors of the applications.<--size=9.0-->

<--image width=937.0 height=625.0-->
Figure 1: Feature Model Structure<--size=9.0-->

3.2<--size=10.9-->
Capturing knowledge through configuration<--size=10.8-->
management<--size=10.9-->

To build and develop the SPL from past applications, we memorize<--size=8.9-->
the valuable elements to find the associated codes and the context<--size=8.9-->
in which they were defined.<--size=9.0-->

Configurations. Configurations are our primary tool for deter-<--size=9.1-->
mining application context. As we work on applications whose<--size=9.1-->
context is difficult to define and the SPL evolves, the configurations<--size=8.9-->
associated with the applications may be partial (RT1.1). Therefore,<--size=9.0-->
we consider any feature neither selected nor deselected as "un-<--size=9.1-->
known."<--size=9.0-->
A partial configuration ğ‘is defined as a set of selected, deselected,<--size=8.9-->
and undefined features. The intersection is empty between these<--size=9.1-->
three subsets.<--size=9.0-->
Let âŸ¦ğ¹ğ‘€âŸ§be the set of valid configurations of a feature model ğ¹ğ‘€.<--size=9.0-->
A partial configuration ğ‘of a feature model ğ¹ğ‘€is valid iff<--size=9.0-->
âˆ€ğ‘“ğ‘–âˆˆğ‘, ğ‘“ğ‘–âˆˆğ¹ğ‘€âˆ§âˆƒğ‘ğ‘˜âˆˆâŸ¦ğ¹ğ‘€âŸ§,ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘) âŠ†ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘ğ‘˜) âˆ§<--size=9.0-->
ğ‘‘ğ‘’ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘) âŠ†ğ‘‘ğ‘’ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘ğ‘˜). By extension, we note ğ‘âˆˆâŸ¦ğ¹ğ‘€âŸ§.<--size=9.0-->
A configuration is complete relatively to a set of features ğ¹, when<--size=9.0-->
âˆ€ğ‘“ğ‘–âˆˆğ¹, ğ‘“ğ‘–âˆˆğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘) âˆªğ‘‘ğ‘’ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘). Thus, in RRW , configura-<--size=9.0-->
tions must be complete only relative to the Solution branch since<--size=8.9-->
we know whether the solution components are part of the appli-<--size=9.1-->
cation or not (RT1.1). In the following we refer to configurations,<--size=9.1-->
even for partial configurations.<--size=9.0-->
To evaluate the evolution of our knowledge, we preserve the<--size=9.1-->
information on the manual or automatic selection/deselection. So<--size=9.0-->
we denote a configuration as a set of pairs: (ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’,ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘ ) where<--size=9.0-->
ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘ âˆˆ{ğ‘šğ‘ ,ğ‘šğ‘‘,ğ‘ğ‘ ,ğ‘ğ‘‘,ğ‘¢}, where m for manual, a for automatic, s<--size=9.0-->
for selected, d for deselected, u for undefined. For example,<--size=9.0-->
ğ‘= {(ğ‘“1,ğ‘šğ‘ ), (ğ‘“2,ğ‘ğ‘ ), (ğ‘“3,ğ‘¢), (ğ‘“4,ğ‘ğ‘‘)},<--size=9.0-->
ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘) = {ğ‘“1, ğ‘“2},ğ‘‘ğ‘’ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘) = {ğ‘“4}<--size=9.0-->

Data sets. For each dataset involved in an application, we pre-<--size=9.1-->
serve the associated partial configuration relative to the branch<--size=9.1-->
initialData (see Figure1). This record supports a consolidation<--size=9.0-->
mechanism. New applications dealing with known datasets should<--size=8.9-->
describe them in compliance with previous records and further<--size=9.1-->
complete them (RT2.2).<--size=9.0-->

Required information about applications. The information associ-<--size=8.9-->
ated with an application is its name (used as a reference), its initial<--size=8.9-->
configuration, the feature model used to define it, a reference to the<--size=8.9-->
dataset, its author, and the associated codes. The connection to the<--size=8.9-->
author makes it possible to identify the practices and preferences<--size=9.0-->
of the data scientists and allows a data scientist to reduce the space<--size=8.9-->
of the applications by authors. The reference to codes corresponds,<--size=8.9-->
to Jupyter NoteBooks, to runs in OpenML or to a workflow in the<--size=9.0-->
industrial partner platform.<--size=9.0-->

3.3<--size=10.9-->
Pattern detection based on the premise: a<--size=10.9-->
problem has a unique solution<--size=10.9-->

In the same way as Tornhill [50], we seek to identify "hotspots" to<--size=8.9-->
narrow our study of applications to a few critical patterns that are<--size=9.0-->
most likely to guide us in the extraction of new knowledge (RT2.2).<--size=8.9-->
Therefore, the principle is that if two similar problems correspond<--size=8.9-->
to different solutions, then it is not the same problem.<--size=9.0-->
We defined the first pattern: 2 problems evaluated as equivalent<--size=9.1-->
have a different solution. This pattern allows us to detect different<--size=9.0-->
situations. (i) One of the solutions is not adapted to the problem,<--size=9.1-->
and in this case, in retrospect, the data scientist should not have<--size=9.1-->
used it. We must enrich the feature model to prohibit it. (ii) The<--size=9.1-->
two problems are different, but we had not yet identified these<--size=9.1-->
discriminative criteria in the feature model; we must enrich the<--size=9.1-->
feature model with these new criteria.<--size=9.0-->
Because the configurations partially characterize the problems,<--size=9.0-->
we define a second pattern: 2 problems evaluated as unifiable have a<--size=8.9-->
different solution. It can indeed be two similar problems partially<--size=9.1-->
filled in. In this case, we expect the same solution as before. But, the<--size=8.9-->
data scientist may also have designed a different solution to address<--size=8.9-->
the lack of information about the problem, such as not knowing<--size=9.1-->
the anomaly types.<--size=9.0-->
The fact that several problems have the same solution can also<--size=9.0-->
induce an insensitivity of the solution to certain features. Despite<--size=9.0-->
the small number of applications, this situation allowed us to iden-<--size=8.9-->
tify a feature that we apprehended at a level too detailed to be<--size=9.1-->
discriminating. Detecting these patterns occurs in Scenario 3 and<--size=9.0-->
meets the requirement RT2.2.<--size=9.0-->
We now specify the notions of equivalence classes and their unifi-<--size=8.9-->
ability. To explain these concepts, we use the feature model pre-<--size=9.1-->
sented in Figure 2 and the configurations described in Table 1.<--size=9.1-->
Table 3 shows the identified equivalence classes.<--size=9.0-->

<--image width=651.0 height=413.0-->
Figure 2: Feature model for explaining metrics<--size=9.0-->

<--page_end:5-->

<--page_start:6-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
El Amraoui et al.<--size=7.0-->

Table 1: Examples of application configurations.<--size=9.0-->

XP Name<--size=9.0-->
d1<--size=9.0-->
d2<--size=9.0-->
d3<--size=9.0-->
p3<--size=9.0-->
ğ‘4<--size=9.0-->
p1<--size=9.0-->
p2<--size=9.0-->
a1<--size=9.0-->
a2<--size=9.0-->
b1<--size=9.0-->
b2<--size=9.0-->
ğ‘†ğ‘œğ‘¢ğ‘Ÿğ‘ğ‘’ğ‘ <--size=9.0-->
r1<--size=9.0-->
r2<--size=9.0-->
app1<--size=9.0-->
as<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
as<--size=9.0-->
ad<--size=9.0-->
ms<--size=9.0-->
md<--size=9.0-->
as<--size=9.0-->
ms<--size=9.0-->
md<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
app2<--size=9.0-->
as<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
as<--size=9.0-->
ad<--size=9.0-->
ms<--size=9.0-->
ad<--size=9.0-->
ms<--size=9.0-->
ms<--size=9.0-->
md<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
app3<--size=9.0-->
as<--size=9.0-->
as<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
as<--size=9.0-->
ms<--size=9.0-->
ad<--size=9.0-->
ad<--size=9.0-->
ms<--size=9.0-->
ms<--size=9.0-->
md<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
app4<--size=9.0-->
as<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
u<--size=9.0-->
as<--size=9.0-->
ad<--size=9.0-->
ms<--size=9.0-->
ms<--size=9.0-->
ad<--size=9.0-->
ms<--size=9.0-->
md<--size=9.0-->
as<--size=9.0-->
ms<--size=9.0-->
u<--size=9.0-->
s=selected, d=deselected, u=undefined, a=automatic, m=manual<--size=9.0-->

Table 2: Metrics related to the FM in<--size=9.1-->
fig. 2 and its configurations (tab. 1)<--size=9.0-->

ğ‘ğ‘œğ¹<--size=9.0-->
20<--size=9.0-->
ğ‘ğ‘œğ´<--size=9.0-->
4<--size=9.0-->
ğ‘ğ‘™ğ‘’ğ‘ğ‘“<--size=9.0-->
12<--size=9.0-->
ğ¶ğ‘œğ‘£<--size=9.0-->
66 %<--size=9.0-->
ğ‘ğ‘œğ¶<--size=9.0-->
2<--size=9.0-->
ğ‘ğ‘œğ¸ğ¶<--size=9.0-->
3<--size=9.0-->
ğ¶ğ‘‡ğ¶ğ‘…<--size=9.0-->
20 %<--size=9.0-->
ğ¶ğ‘œğ‘š<--size=9.0-->
37,5 %<--size=9.0-->

Let a feature model ğ¹ğ‘€and ğ´a set of valid partial configurations<--size=8.9-->
of ğ¹ğ‘€, ğ´âŠ†âŸ¦ğ¹ğ‘€âŸ§.<--size=9.0-->

3.3.1<--size=9.0-->
Equivalence Classes in ğ´. An equivalence class on a subset<--size=9.0-->
of features ğ¹of ğ¹ğ‘€is defined as a set of valid configurations [ğ‘1] =<--size=9.0-->
{ğ‘1, ...ğ‘ğ‘˜}, ğ‘ğ‘–âˆˆğ´, such as âˆ€ğ‘“ğ‘–âˆˆğ¹,<--size=9.0-->
ğ‘“ğ‘–âˆˆâˆ©ğ‘˜<--size=9.0-->
1ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘ğ‘—) Ã âˆ©ğ‘˜<--size=9.0-->
1ğ‘‘ğ‘’ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘ğ‘—) Ã âˆ©ğ‘˜<--size=9.0-->
1ğ‘¢ğ‘›ğ‘‘ğ‘’ğ‘“ğ‘–ğ‘›ğ‘’ğ‘‘(ğ‘ğ‘—).<--size=9.0-->
In Table 3, two equivalence classes are identified on the Initial-<--size=9.0-->
Data subtree. CDS1={app1, app2, app4}. CDS1 can also be noted:<--size=9.0-->
{(ğ‘‘1,ğ‘ ), (ğ‘‘2,ğ‘¢), (ğ‘‘3,ğ‘¢)} and CDS2: {(ğ‘‘1,ğ‘ ), (ğ‘‘2,ğ‘ ), (ğ‘‘3,ğ‘¢)}<--size=9.0-->
Problem equivalence classes are defined on the sub-features of<--size=9.1-->
InitialData and BusinessRequirements.<--size=9.0-->
For example CP1={app1, app2, app4}.<--size=9.0-->
ğ¶ğ‘ƒ1 : {(ğ‘‘1,ğ‘ ), (ğ‘‘2,ğ‘¢), (ğ‘‘3,ğ‘¢), (ğ‘1,ğ‘‘), (ğ‘2,ğ‘ ), (ğ‘3,ğ‘¢), (ğ‘4,ğ‘ )}<--size=9.0-->
Solution equivalence classes are defined on the sub-features of<--size=9.1-->
Solution.<--size=9.0-->
Two configurations are equivalent in ğ¹ğ‘€if they are member of<--size=9.0-->
the same Problem and Solution equivalence classes. In our example,<--size=8.9-->
app1 and app2 are equivalent.<--size=9.0-->
We note ğ‘ğ‘œğ¸ğ¶the number of equivalent classes.<--size=9.0-->

3.3.2<--size=9.0-->
Unifiable classes. Two equivalence classes [ğ‘1] and [ğ‘2] de-<--size=9.0-->
fined on a same set of features ğ¹are unifiable if âˆ€ğ‘“ğ‘–âˆˆğ¹,<--size=9.0-->
ğ‘“ğ‘–âˆˆâˆ©2<--size=9.0-->
1(ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘ğ‘—) âˆªğ‘¢ğ‘›ğ‘‘ğ‘’ğ‘“ğ‘–ğ‘›ğ‘’ğ‘‘(ğ‘ğ‘—))<--size=9.0-->
Ã âˆ©2<--size=9.0-->
1(ğ‘‘ğ‘’ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘(ğ‘ğ‘—) âˆªğ‘¢ğ‘›ğ‘‘ğ‘’ğ‘“ğ‘–ğ‘›ğ‘’ğ‘‘(ğ‘ğ‘—))<--size=9.0-->
In the example, CDS1 and CDS2 are unifiable.<--size=9.0-->

3.4<--size=10.9-->
Metrics for evaluating the SPL<--size=10.9-->

To explain the following metrics, we use the feature model pre-<--size=9.1-->
sented in Figure 2 and the configurations presented in table 1.<--size=9.0-->

3.4.1<--size=9.0-->
Feature model metrics. We selected some standard metrics<--size=9.0-->
[18] to assess the state of the feature model and, by comparison, its<--size=8.9-->
evolution.<--size=9.0-->
The number of Features (ğ‘ğ‘œğ¹) and the number of features with<--size=8.9-->
no children (ğ‘ğ‘™ğ‘’ğ‘ğ‘“) are a way to measure the scope of the SPL.<--size=9.1-->
Our objective is to integrate new solutions while identifying better<--size=8.9-->
and better the problems solved. We analyze these metrics in the<--size=9.1-->
different spaces. In our example in Figure 2 , the number of leaves is<--size=8.9-->
twelve overall, and it is four in the Solution subtree (Solution Space).<--size=8.9-->
The evolution of the number of cross-constraints (ğ‘ğ‘œğ¶), together<--size=9.0-->
with the tree-cross-constraint ratio (ğ¶ğ‘‡ğ¶ğ‘…)5, gives a numerical in-<--size=9.0-->
dication of the identified interactions. In our example, four features<--size=8.9-->
are involved in constraints, so the ğ¶ğ‘‡ğ¶ğ‘…is 20%. The theoretical<--size=9.1-->
number of possible configurations is not only not calculable but also<--size=8.9-->
does not provide any information. Indeed, it is likely that a part of<--size=8.9-->

5number of distinct features involved in cross-tree constraints and divides them<--size=5.5-->
through the total number of features in the feature model<--size=7.0-->

the valid configurations does not correspond to suitable solutions.<--size=9.0-->
Moreover, this partial knowledge of the domain combined with<--size=9.1-->
the very high cost of ML workflows evaluations does not allow to<--size=9.0-->
test the SPL by generating examples, except to consume a lot of<--size=9.0-->
resources without any assurance of a real gain.<--size=9.0-->
We now propose to evaluate the feature model ğ¹ğ‘€according<--size=9.1-->
to the set of valid configurations ğ´âŠ†âŸ¦ğ¹ğ‘€âŸ§that correspond to<--size=9.0-->
applications integrated in the SPL.<--size=9.0-->

3.4.2<--size=8.9-->
Feature-level metrics based on past configurations. Common-<--size=8.9-->
ality of feature (ğ¶ğ‘œğ‘š(ğ‘“)) indicates the selection ratio (manual or<--size=9.0-->
automatic) of a feature f in ğ´. This ratio identifies the "unused vari-<--size=8.9-->
ability" smell (i.e., feature always selected, e.g., ğ¶ğ‘œğ‘š(ğ‘1) = 1)[5].<--size=9.1-->
The rate of deselection (ğ·ğ‘’ğ‘ (ğ‘“) identifies the "unused feature" smell<--size=8.9-->
(i.e., feature always deselected, e.g., ğ¶ğ‘œğ‘š(ğ‘2) = 0, ğ·ğ‘’ğ‘ (ğ‘2) = 1)[5].<--size=9.0-->
We also compute the rate of undefined occurrences(ğ‘ˆğ‘›ğ‘‘(ğ‘“)) that<--size=9.1-->
may identify an obscure feature that is not well related to the scope<--size=8.9-->
of the SPL (e.g., ğ‘ˆğ‘›ğ‘‘(ğ‘‘3) = 1, ğ·ğ‘’ğ‘ (ğ‘‘3) = 0,ğ¶ğ‘œğ‘š(ğ‘‘3) = 0).<--size=9.0-->
We globalize these metrics to all feature model leaves, which in<--size=8.9-->
our case study characterize practices.<--size=9.0-->

3.4.3<--size=9.0-->
Feature Model Coverage. The Feature Model Coverage rate<--size=9.0-->
(ğ¶ğ‘œğ‘£) measures (in percentage) the degree of leaf selections in a<--size=9.0-->
set of configurations ğ´.<--size=9.0-->
Feature model coverage rate = number of feature leaves selected in<--size=9.1-->
ğ´/number of feature leaves * 100.<--size=9.0-->
In our example, eight leaves are selected at least once, ğ¶ğ‘œğ‘£= 66%.<--size=9.0-->
For the Solution subtree, as three leaves were chosen at least one<--size=9.0-->
time, ğ¶ğ‘œğ‘£ğ‘†ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›= 75%<--size=9.0-->
Feature Model Coverage does not correspond to ğ‘¡-ğ‘¤ğ‘–ğ‘ ğ‘’covera-<--size=9.0-->
ge [22]. Unlike the latter, it only provides a measure of the feature<--size=8.9-->
selection rate in a given set of configurations, it does not allow<--size=9.1-->
to assess the coverage of feature interactions. Nevertheless, it has<--size=9.0-->
the advantage of not requiring to compute the number of possible<--size=8.9-->
configurations.<--size=9.0-->

3.4.4<--size=9.0-->
Feature Model Commonality Rate. The Feature Model Com-<--size=9.0-->
monality Rate (ğ¶ğ‘œğ‘š) measures (in percentage) the selection ratio<--size=9.0-->
of leaf selections in ğ´.<--size=9.0-->
Feature model Commonality Rate = number of selection of feature<--size=9.1-->
leaves in ğ´/ number of feature leaves * #ğ´* 100, where #ğ´denotes<--size=8.9-->
the cardinality of the set ğ´.<--size=9.0-->
In our example, 18 selections of leaves for 4 configurations and<--size=9.0-->
12 leaves, ğ¶ğ‘œğ‘š= 37, 5%. ğ¶ğ‘œğ‘šğ‘†ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›= 50%<--size=9.0-->
Intuitively, the confidence in the feature model suggestions is pro-<--size=9.0-->
portional to its commonality rate.<--size=9.0-->

<--page_end:6-->

<--page_start:7-->
Evolvable SPL management with partial knowledge: an application to anomaly detection in time series<--size=7.0-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->

Table 3: Equivalence classes and pattern detection.<--size=9.0-->

App Name<--size=9.0-->
InitialDSClass<--size=9.0-->
InitialPBClass<--size=9.0-->
SolutionClass<--size=9.0-->
EquivalentApp<--size=9.0-->
Warning<--size=9.0-->
SameSolution<--size=9.0-->
app1<--size=9.0-->
CDS1<--size=9.0-->
CP1<--size=9.0-->
CS1<--size=9.0-->
{app2}<--size=9.0-->
{app4}<--size=9.0-->
{app3}<--size=9.0-->
app2<--size=9.0-->
CDS1<--size=9.0-->
CP1<--size=9.0-->
CS1<--size=9.0-->
{app1}<--size=9.0-->
[]<--size=9.0-->
[]<--size=9.0-->
app4<--size=9.0-->
CDS1<--size=9.0-->
CP1<--size=9.0-->
CS2<--size=9.0-->
[]<--size=9.0-->
{app1}<--size=9.0-->
[]<--size=9.0-->
app3<--size=9.0-->
CDS2<--size=9.0-->
CP2<--size=9.0-->
CS1<--size=9.0-->
[]<--size=9.0-->
[]<--size=9.0-->
{app1}<--size=9.0-->
app1, app2 and app4 handles the same equivalence classes of dataset (CDS1) and problem (CP1). app1 and app2 are equivalent. app3 deals with the same<--size=8.0-->
equivalence class of solutions (CS1) than app1 (and therefore app2). While app4 handles the same equivalence class of problem than app1, it proposes a new<--size=8.0-->
solution, a warning is raised. According to Table 1, CDS1 and CDS2 are unifiable.<--size=8.0-->

3.5<--size=10.9-->
Systematically mastering the evolutions of<--size=10.9-->
the SPL<--size=10.9-->

To promote safe reactive development of the SPL, we suggest the<--size=9.0-->
following process.<--size=9.0-->
At step ğ‘‡, the SPL is coherent, i.e., all applications correspond<--size=9.1-->
to valid configurations of the feature model. We create new appli-<--size=9.0-->
cations with the configurator. We integrate them to the SPL to<--size=9.1-->
make them available to other users (scenario 2, see Section 3.5.1). At<--size=8.9-->
stepğ‘‡+1, we update the feature model by adding, renaming, and re-<--size=8.9-->
moving features and constraints. It is then necessary to ensure that<--size=8.9-->
the configurations related to the previously developed applications<--size=8.9-->
remain valid and do not contradict the new knowledge captured by<--size=8.9-->
the feature model (see Section 3.5.2). We use the new feature model<--size=8.9-->
to build solutions to new problems. We use the patterns presented<--size=8.9-->
above, coupled with metrics to evaluate the SPL and detect new<--size=9.1-->
knowledge (see Section 3.5.3). We use the same means to analyze<--size=9.0-->
the evolution of the SPL (see Section 3.5.4).<--size=9.0-->

3.5.1<--size=8.9-->
Making applications identifiable in the configurator. Integra-<--size=8.9-->
ting in the feature model an application named ğ‘ğ‘ğ‘on a dataset<--size=9.1-->
namedğ‘‘and defined by a valid configurationğ‘consists in adding, in<--size=8.9-->
the Applications branch of the feature model, the features ğ‘‘and<--size=8.9-->
ğ‘ğ‘ğ‘if they are not already there. Then, the minimal constraints6<--size=9.0-->

linkingğ‘‘to the selected and deselected features of the initialData<--size=8.9-->
space are added starting from the manually selected and unselected<--size=8.9-->
leaves. The constraint ğ‘ğ‘ğ‘â‡’ğ‘‘is then added. We then proceed in<--size=9.0-->
the same way to link ğ‘ğ‘ğ‘to the rest of the feature model, starting<--size=9.0-->
with the problem space. When the dataset ğ‘‘is already present<--size=9.1-->
in the feature model, there should be no contradiction with its<--size=9.1-->
constraints. However, they can be completed. This step corresponds<--size=8.9-->
to scenario two and is essential in scenario one to identify datasets<--size=8.9-->
or applications with the same features as the current configuration.<--size=8.9-->
For example, adding the application ğ‘ğ‘ğ‘5 created by John on<--size=9.1-->
dataset ğ‘‘ğ‘ 5 and defined by the following configuration<--size=9.0-->
{(ğ‘‘2,ğ‘ğ‘ ), (ğ‘‘3,ğ‘šğ‘ ), (ğ‘1,ğ‘šğ‘ ), (ğ‘2,ğ‘ğ‘‘), (ğ‘1,ğ‘šğ‘ ), (ğ‘1,ğ‘šğ‘‘), (ğ‘2,ğ‘šğ‘ ), ...}<--size=9.0-->
adds the features ğ‘ğ‘ğ‘5, ğ‘‘ğ‘ 5 and ğ½ğ‘œâ„ğ‘›in the branch Applications<--size=8.9-->
and the following constraints: ğ‘‘ğ‘ 5 â‡’ğ‘‘3,ğ‘ğ‘ğ‘5 â‡’ğ‘‘ğ‘ 5,ğ‘ğ‘ğ‘5 â‡’<--size=9.0-->
ğ‘1,ğ‘ğ‘ğ‘5 â‡’ğ‘2 âˆ§Â¬ğ‘1 âˆ§ğ‘1,ğ‘ğ‘ğ‘5 â‡’ğ½ğ‘œâ„ğ‘›<--size=9.0-->

3.5.2<--size=9.0-->
Application-preserving refactoring against practice evolution.<--size=9.0-->
Refactorings of the feature model may lead to past applications<--size=9.1-->
(i.e., their related configurations) being detected as conflicting with<--size=8.9-->
the current feature model [5]. To promote a safe evolution, a recon-<--size=8.9-->
figuration step is performed on all past configurations. For now,<--size=9.1-->

6features automatically selected or deselected during the configuration are not involved<--size=5.5-->
in new constraints<--size=7.0-->

reconfiguring ağ‘ğ‘ configuration into ağ‘ğ‘¡configuration with respect<--size=8.9-->
to a new feature model ğ¹ğ‘€consists in (i) renaming in ğ‘ğ‘¡some of<--size=9.0-->
the features of ğ‘ğ‘ , (ii) omitting the features that disappeared in ğ¹ğ‘€<--size=8.9-->
with a warning if they were selected or deselected in ğ‘ğ‘ , (iii) adding<--size=8.9-->
in ğ‘ğ‘¡the new features of ğ¹ğ‘€whose value is known, (iv) copying in<--size=8.9-->
ğ‘ğ‘¡the other features, then (v) replaying ğ‘ğ‘¡in ğ¹ğ‘€to obtain a new<--size=9.0-->
valid configuration or to raise an error in the contrary situation. If<--size=8.9-->
past configurations cannot be rendered valid in ğ¹ğ‘€, RTFS excludes<--size=8.9-->
them with a warning. The new valid configurations related to ap-<--size=9.0-->
plications can then be integrated into the ğ¹ğ‘€using the previous<--size=9.1-->
operation.<--size=9.0-->
For example, if we add the constraint Â¬ğ‘1âˆ¨Â¬ğ‘‘2 in FM of Figure 2,<--size=9.0-->
the configuration corresponding to ğ‘ğ‘ğ‘3 is no more valid, while all<--size=8.9-->
the other configurations are automatically updated with (ğ‘‘2,ğ‘ğ‘‘).<--size=9.0-->

3.5.3<--size=9.0-->
Knowledge extraction driven by SPL assessment. Regarding<--size=9.0-->
scenario 3, the identification of the patterns presented above and<--size=9.0-->
the associated metrics allow us to evaluate the SPL to extract new<--size=9.0-->
knowledge and orient future evolutions, notably according to the<--size=9.0-->
spaces covered or not by the applications.<--size=9.0-->

3.5.4<--size=9.1-->
Knowledge extraction driven by SPL evolution assessment.<--size=9.1-->
The metrics and the detection of patterns also make it possible to<--size=9.0-->
evaluate the evolution of the SPL.<--size=9.0-->
Have more features been used? Do unifiable problems become<--size=9.0-->
equivalent? Conversely, does the enrichment now allow us to dis-<--size=9.0-->
tinguish previously equivalent problems? Both of these cases can<--size=9.0-->
occur when the addition of constraints affects previously undefined<--size=8.9-->
features.<--size=9.0-->

4<--size=10.9-->
APPLICATION<--size=10.9-->

We now report on the first three steps of the SPLâ€™s construction,<--size=9.1-->
showing how the practices contributed to its enrichment. Figure 3<--size=8.9-->
summarizes this construction process. The configurations and the<--size=9.0-->
results of the analyses are accessible online7<--size=9.0-->

4.1<--size=10.9-->
First three steps of the SPL construction<--size=10.9-->
process<--size=10.9-->

In each of the steps presented below we have integrated the appli-<--size=8.9-->
cations into the FM, which did not raise any significant issue.<--size=9.0-->

Initial product line version from literature study. Following a first<--size=8.9-->
analysis of the domain, we built the SPLâ€™s initial version (ğ‘†ğ‘ƒğ¿ğ‘‡0).<--size=9.1-->
The feature model (ğ¹ğ‘€ğ‘‡0) integrates some solutions from the liter-<--size=8.9-->
ature dealing with the detection of anomalies in time series. The<--size=9.1-->

7https://anonymous.4open.science/r/RFTS-SPLC2022-D508/<--size=5.5-->

<--page_end:7-->

<--page_start:8-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
El Amraoui et al.<--size=7.0-->

<--image width=691.0 height=283.0-->
Figure 3: Three main steps of our SPL construction<--size=9.0-->

applications correspond to experiments carried out on reference<--size=9.1-->
data sets [13].<--size=9.0-->

Enrichment of the product line through industrial practices. At ğ‘‡1,<--size=8.9-->
we leverage the practices of the partner companyâ€™s data scientists<--size=9.0-->
to build ğ‘†ğ‘ƒğ¿ğ‘‡1 by enriching ğ‘†ğ‘ƒğ¿ğ‘‡0. The interest in exploiting indus-<--size=8.9-->
trial applications is to broaden the scope of the SPL to the processing<--size=8.9-->
of industrial data. The industrial partner uses a custom tool to sum-<--size=8.9-->
marize all the applications on their customerâ€™s data. We update<--size=9.1-->
the feature model (ğ¹ğ‘€ğ‘‡0 â†’ğ¹ğ‘€ğ‘‡1) by including company-specific<--size=9.0-->
solution components, new initial dataset properties relevant to an-<--size=8.9-->
alyzing customer datasets, and new features necessary to describe<--size=8.9-->
the customer business requirements. Then we collect applications<--size=9.0-->
conducted by the companyâ€™s data scientists, keeping only the solu-<--size=8.9-->
tions from deployed workflows and solving customersâ€™ anomaly<--size=9.1-->
detection problems. We have thus selected six workflows whose<--size=9.1-->
resulting product models are in production. The production of these<--size=8.9-->
workflows can take several months for the data scientists. We have<--size=8.9-->
generated partial configurations containing information about the<--size=8.9-->
dataset and solution based on automatic solution extraction and<--size=9.1-->
data analysis tools. We used these partial configurations to initialize<--size=8.9-->
the configurator. We then completed the source data and business<--size=9.0-->
requirements parts via a discussion with the application authors.<--size=9.0-->

Consolidation by extraction of OpenML workflows. At ğ‘‡2, we ex-<--size=9.0-->
tract some practices from the OpenML platform. OpenML is an<--size=9.1-->
automated machine learning environment [53], from which ML<--size=9.1-->
practices can be downloaded and uploaded i.e., solutions (runs and<--size=8.9-->
flows in OpenML) to a given problem (task and dataset in OpenML).<--size=8.9-->
The interest in exploiting OpenMLâ€™s practices is to analyze the<--size=9.1-->
impact of upgrading the SPL with external sources. In OpenML, we<--size=8.9-->
selected time-series datasets and associated tasks of type Supervised<--size=8.9-->
Learning and Unsupervised learning since anomaly detection is su-<--size=9.0-->
pervised or unsupervised learning with unbalanced classes. We only<--size=8.9-->
had four datasets that matched these criteria. We kept 4 tasks of Su-<--size=8.9-->
pervised learning that had runs associated with them. Among these<--size=8.9-->
runs, we selected only the best runs on F1-score evaluation criterion<--size=8.9-->
as evaluations on other measures such as user CPU-time were not<--size=8.9-->
available for these runs. We preferred the runs using the scikitlearn<--size=8.9-->
library when we had the choice. We then extracted the associated<--size=9.0-->
flows and generated the associated partial configurations for each<--size=9.0-->
run. We had already studied in ğ‘‡0the meta-features proposed by<--size=9.0-->
OpenML to characterize datasets, so we only updated the feature<--size=9.0-->
model(ğ¹ğ‘€ğ‘‡1 â†’ğ¹ğ‘€ğ‘‡2) by adding new solution components.<--size=9.0-->

4.2<--size=10.9-->
Knowledge extraction driven by SPL<--size=10.9-->
assessment<--size=10.9-->

We explain in the following subsections how we exploit pattern<--size=9.1-->
and metric analysis in our use case.<--size=9.0-->

Two different solutions for the same problem: algorithms<--size=9.1-->
side effects. At ğ‘‡0, we encountered the following scenario. For<--size=9.1-->
two equivalent problems, the solutions used two different scaling<--size=9.0-->
techniques in each workflow, min-max scaler and robust scaler [40].<--size=8.9-->
This equivalence of problems and not solutions raised a warning.<--size=9.0-->
We analyzed workflows for both experiments and observed that for<--size=8.9-->
the second workflow, the robust scaler results were equivalent to the<--size=8.9-->
min-max scaler results due to the data properties. In this scenario,<--size=9.0-->
we were able to confirm that the main particularity of the robust<--size=9.0-->
scaler was not required8. Therefore only the first application with<--size=8.9-->
min-max scaler was kept. We added a constraint to the selection of<--size=8.9-->
this algorithm to prevent the error from being repeated. i.e., data<--size=9.1-->
without outliers will not anymore be scaled with robust scaler.<--size=9.0-->

Two different solutions for the same problem: Data Scien-<--size=9.1-->
tist preferences impact. The data scientistâ€™s preferences bias her<--size=9.0-->
choice of the solution components. Atğ‘‡1, we identified two applica-<--size=8.9-->
tions that presented different solutions to equivalent problems. The<--size=8.9-->
two authors could not justify the difference in the choice of Solution<--size=8.9-->
components other than by their expertise in selected algorithms.<--size=9.1-->
Therefore, we have kept these two applications distinguishable by<--size=8.9-->
their author, with a warning for possible future treatment.<--size=9.0-->

Two problems same solution: factorizing unnecessary varia-<--size=8.9-->
bility. At ğ‘‡0, two problems differ only in acquisition sampling;<--size=9.1-->
data acquisition sampling is in seconds for one and in microseconds<--size=8.9-->
for the second. Otherwise, the data are similar, and the anomaly<--size=9.1-->
detection requirements are equivalent. After the detection of this<--size=9.0-->
pattern, we checked the impact of acquisition sampling on the<--size=9.1-->
algorithms and factorized all four regular sampling features into<--size=9.1-->
regularSampling for the SPL at ğ‘‡1.<--size=9.0-->

4.3<--size=10.9-->
Knowledge extraction driven by SPL<--size=10.9-->
evolution assessment<--size=10.9-->

We also exploited the analysis of the evolution of patterns and<--size=9.1-->
metrics as another source of information. We use Pbğ›¼and Pbğ›½to<--size=9.0-->
refer to the problem part of the configurations (i.e., the features of<--size=8.9-->
the InitialData and BusinessRequirements branches) and Sğ›¼<--size=9.1-->
and Sğ›½to refer to the solution part.<--size=9.0-->

4.3.1<--size=9.1-->
Pattern evolution and knowledge consolidation. At T0, Pbğ›¼<--size=9.1-->
and Pbğ›½are equivalent, but solved by two different clustering mod-<--size=8.9-->
els9, kmeans [26] on the one hand and Dbscan [46] on the other. At<--size=8.9-->
T0, we did not know which to delete; we kept both applications. At<--size=8.9-->
T1, we reconfigured the configurations to align with the new feature<--size=8.9-->
model, which now incorporates features detailing business expert<--size=9.0-->
insights into possible outliers in the data10. The feature model also<--size=8.9-->
includes associated constraints expressing compatibility between<--size=9.0-->

8Usage of the robust scaler is interesting only if outliers are within the values of the<--size=5.5-->
time series<--size=7.0-->
9Solution workflows vary according to machine learning algorithms<--size=5.5-->
10The data scientists can decide whether outliers are anomalies or not in the context<--size=5.5-->
of the experiment<--size=7.0-->

<--page_end:8-->

<--page_start:9-->
Evolvable SPL management with partial knowledge: an application to anomaly detection in time series<--size=7.0-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->

Table 4: Metrics Evolution in times and spaces<--size=9.0-->

ğ‘ğ‘œğ¹<--size=8.0-->
ğ‘ğ‘™ğ‘’ğ‘ğ‘“<--size=8.0-->
ğ¶ğ‘œğ‘£<--size=8.0-->
ğ¶ğ‘œğ‘š<--size=8.0-->
ğ‘ğ‘œğ¸ğ¶<--size=8.0-->
ğ‘ğ‘œğ´<--size=8.0-->
ğ‘ğ‘œğ¶<--size=8.0-->
ğ¶ğ‘‡ğ¶ğ‘…<--size=8.0-->

ğ‘‡0<--size=8.0-->

InitialData<--size=8.0-->
23<--size=8.0-->
16<--size=8.0-->
37%<--size=8.0-->
19%<--size=8.0-->
5<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
BusinessRequirements<--size=8.0-->
33<--size=8.0-->
24<--size=8.0-->
41%<--size=8.0-->
21,25 %<--size=8.0-->
7<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
Solution<--size=8.0-->
51<--size=8.0-->
25<--size=8.0-->
52%<--size=8.0-->
16.8 %<--size=8.0-->
7<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
Globalâˆ—<--size=8.0-->
156<--size=8.0-->
96<--size=8.0-->
35,04 %<--size=8.0-->
14,68 %<--size=8.0-->
10<--size=8.0-->
10<--size=8.0-->
25<--size=8.0-->
21,19%<--size=8.0-->

ğ‘‡1<--size=8.0-->

InitialData<--size=8.0-->
28<--size=8.0-->
19<--size=8.0-->
42,10 %<--size=8.0-->
18,94 %<--size=8.0-->
9<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
BusinessRequirements<--size=8.0-->
43<--size=8.0-->
33<--size=8.0-->
54,55 %<--size=8.0-->
17 %<--size=8.0-->
13<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
Solution<--size=8.0-->
67<--size=8.0-->
37<--size=8.0-->
48,64 %<--size=8.0-->
8,64 %<--size=8.0-->
11<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
Globalâˆ—<--size=8.0-->
194<--size=8.0-->
124<--size=8.0-->
40,32 %<--size=8.0-->
10,86 %<--size=8.0-->
14<--size=8.0-->
15<--size=8.0-->
31<--size=8.0-->
21,76%<--size=8.0-->

ğ‘‡2<--size=8.0-->

InitialData<--size=8.0-->
28<--size=8.0-->
19<--size=8.0-->
57,9 %<--size=8.0-->
18,00 %<--size=8.0-->
14<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
BusinessRequirements<--size=8.0-->
43<--size=8.0-->
33<--size=8.0-->
57,6 %<--size=8.0-->
17,24 %<--size=8.0-->
17<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
Solution<--size=8.0-->
74<--size=8.0-->
42<--size=8.0-->
57,14 %<--size=8.0-->
7,89 %<--size=8.0-->
15<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
-<--size=8.0-->
Globalâˆ—<--size=8.0-->
203<--size=8.0-->
131<--size=8.0-->
47,32 %<--size=8.0-->
10,41 %<--size=8.0-->
18<--size=8.0-->
19<--size=8.0-->
32<--size=8.0-->
21,78%<--size=8.0-->
âˆ—The difference between the global figures and the figures of the 3 spaces corresponds to the branches Sources and states.<--size=8.0-->
The feature model hierarchy is six levels deep for the Solution branch, and four for the InitialData and BusinessRequirements branches.<--size=8.0-->

solution components and these new features. The reconfiguration<--size=9.0-->
made it possible to distinguish the two problems and the adequacy<--size=8.9-->
of the two different solutions.<--size=9.0-->

4.3.2<--size=8.9-->
Pattern evolution and knowledge extraction. Pbğ›¼and Pbğ›½are<--size=8.9-->
equivalent in T0, Sğ›¼includes a dimension reduction process through<--size=8.9-->
PCA [1] while Sğ›½skips this step. Like in the previous example,<--size=9.1-->
we kept both applications. At T1, we extended the InitialData<--size=9.1-->
space with features to explicit time series dimensionalities and<--size=9.1-->
automated their evaluation by dataset analysis. The reconfiguration<--size=8.9-->
step indicated that in Pbğ›¼, the time series were multivariate. In<--size=9.1-->
contrast, Pbğ›½â€™s time series were uni-variate [2]. This unique change<--size=8.9-->
in configuration highlighted the link between PCA and time series<--size=9.0-->
dimensionalities.<--size=9.0-->

4.4<--size=10.9-->
Exploiting the metrics<--size=10.9-->

In sections 4.2 and 4.3, we established that the analysis of equiva-<--size=9.0-->
lence classes on both the problems and the solutions helps to trace<--size=8.9-->
the applications and their common points. We will now describe<--size=9.1-->
how the metrics defined in section 3.4 help us assess the evolution<--size=8.9-->
of the practices in each space.<--size=9.0-->

InitialData. The coverage rate (ğ¶ğ‘œğ‘£) increased from ğ‘‡0to ğ‘‡1,<--size=8.9-->
while the number of features (ğ‘ğ‘œğ¹) also increased. This increase in-<--size=8.9-->
dicates that the industrial applications cover different data set prop-<--size=8.9-->
erties from the first applications on benchmark datasets. Between<--size=9.0-->
ğ‘‡1and ğ‘‡2the coverage increased while the number of features<--size=9.0-->
did not change. New applications did involve new features of the<--size=9.1-->
InitialData. We rely on commonality analysis to better understand<--size=8.9-->
the variations between industrial and benchmark datasets. It shows<--size=8.9-->
that at ğ‘‡0all the features related to Missingvalues were unused<--size=9.1-->
features11which means that the datasets did not have missing val-<--size=8.9-->
ues of any type. Atğ‘‡1MCARMV12, and StructuralMV13features had<--size=8.9-->
a ğ‘ğ‘œğ‘š(ğ‘“) > 1, which means that the new datasets were exhibit-<--size=9.0-->
ing these two types of missing values. Similarly, we identify the<--size=9.1-->
emergence of irregular sampling time series at ğ‘‡1.<--size=9.0-->

11always deselected<--size=5.5-->
12Missing value completely at random<--size=5.5-->
13Missing values of structural nature<--size=5.5-->

BusinessRequirements. Within this feature space, we sought<--size=9.0-->
to identify the questions that experts answered the least. These<--size=9.1-->
questions may need rephrasing. The principle is then to identify<--size=9.1-->
the most undefined features of the penultimate level. We did not<--size=9.1-->
meet such a case yet, which was confirmed by the data scientists.<--size=9.0-->
The coverage and commonality analysis highlight the require-<--size=9.0-->
ments of industrial applications for memory, CPU, or energy con-<--size=9.0-->
sumption optimization. The features representing these hardware<--size=9.0-->
constraints are either undefined or deselected at ğ‘‡0and ğ‘‡2. They<--size=9.0-->
are selected at ğ‘‡1only.<--size=9.0-->

Solution. . The coverage rate decreases at stepğ‘‡1and increases<--size=8.9-->
atğ‘‡2, while the number of features increases strictly. The evolution<--size=8.9-->
of these two metrics indicates: (i) on the one hand, that industrial<--size=9.0-->
applications use new solution components; (ii) and on the other<--size=9.1-->
hand, that the applications we integrated at ğ‘‡2consolidate our SPL<--size=8.9-->
by reusing existing solution components. The commonality rate de-<--size=8.9-->
creases to reach 7.89%. However, a detailed analysis of the number<--size=8.9-->
of selections by feature indicates that some algorithms are used in<--size=8.9-->
several solutions, while others are never used. For instance, we ob-<--size=8.9-->
serve that each of LSTMAE (LSTM Auto-encoder) and MAE (mean<--size=8.9-->
absolute error) have been used 5 times out of 19, while padding,<--size=9.0-->
FrontFill and others have not been used. Therefore, correlated<--size=9.0-->
with broader coverage of problem space, this metric should help<--size=9.1-->
identify some of the preferences of data scientists and maybe some<--size=8.9-->
bias. Indeed, it is natural to think that data scientists generally rely<--size=8.9-->
on the algorithms they are comfortable with, sometimes maybe at<--size=8.9-->
the expense of the solution.<--size=9.0-->

5<--size=10.9-->
DISCUSSION<--size=10.9-->

In this section, we relate our findings to existing work, and discuss<--size=8.9-->
potential threats to validity and current limitations.<--size=9.0-->
Usability. . While we are confident that our SPL approach helps<--size=8.9-->
narrowing the problem, reducing the solution space, and identify-<--size=9.0-->
ing similar applications, these points remain to be proven through<--size=9.0-->
controlled experiments. To facilitate the use of the configurator,<--size=9.1-->
we rely on visualization techniques [41] since recommendation<--size=9.1-->
systems are not yet applicable [42, 51]. Yet, due to the increasing<--size=9.1-->
size and complexity of the feature model, one threat is that the<--size=9.1-->
configurator might become cumbersome to use because it exposed<--size=8.9-->

<--page_end:9-->

<--page_start:10-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
El Amraoui et al.<--size=7.0-->

too many questions and too many possible solutions. Controlling<--size=9.0-->
the evolution of the feature model is therefore essential to avoid<--size=9.1-->
irrelevant questions and poorly fitting solution components. Met-<--size=9.0-->
rics and patterns are part of the proposed solution to reduce this<--size=9.1-->
risk. Nevertheless, detecting patterns, especially those related to<--size=9.1-->
unification, can pose scalability issues on which we are currently<--size=9.0-->
working. Another threat to usability is related to the actual main-<--size=9.0-->
tenance of the SPL in response to metrics and patterns analysis.<--size=9.1-->
These tasks were performed by the SPL modelers, interacting with<--size=8.9-->
the data scientists. This point does not challenge the relevance of<--size=9.0-->
the approach, but we still need to demonstrate that the tools allows<--size=8.9-->
autonomous maintenance by data scientists and collaborative FM<--size=9.0-->
updating [31].<--size=9.0-->

Practice-driven feature modeling. To address the different percep-<--size=8.9-->
tions of domain concepts, we not only unified domain terminology<--size=8.9-->
with descriptive feature names, but also provided descriptions and<--size=8.9-->
sources that are accessible directly from the configurator. How-<--size=9.1-->
ever, we specified requirements only qualitatively (with proposi-<--size=9.1-->
tional FM) using an ordinal scale when necessary, instead of their<--size=9.0-->
scalar values (e.g., available memory greater than/lower than 1<--size=9.1-->
GB) [7]. To automate and ensure reproducibility of reasoning be-<--size=9.1-->
tween stakeholders, we scripted a mapping between time series<--size=9.1-->
metadata values and features. So far, these approximations have<--size=9.1-->
not hampered knowledge acquisition. Therefore, we did not need<--size=9.0-->
attributed feature models for which pattern detection has yet to be<--size=8.9-->
designed.<--size=9.0-->

Practice-driven evolution. . Our work follows a reactive SPL adop-<--size=8.9-->
tion process [20, 28], using different techniques to locate features [16].<--size=8.9-->
However, identifying the variations between workflows does not<--size=9.0-->
always enable us to understand the variations of the problem. The<--size=8.9-->
feature model then plays a crucial role in revealing undefined ele-<--size=9.0-->
ments of the problem from the known constraints on the solutions.<--size=8.9-->
It is therefore essential that the FM be rich enough. We have demon-<--size=8.9-->
strated through our case study that we can enrich it with pattern<--size=9.0-->
detection. Yet, other complementary avenues still need to be ex-<--size=9.1-->
plored to identify the relationships between solution components<--size=9.0-->
and source datasets. We are currently working on extracting the<--size=9.1-->
preconditions and effects of the algorithms by analyzing different<--size=9.0-->
techniques and ML environments [6, 35, 38, 52].<--size=9.0-->

Quality assurance. . When the feature model is modified, we<--size=9.1-->
check, through automatic reconfigurations [51], that the previous<--size=9.0-->
configurations are preserved or even enhanced. These systematic<--size=9.0-->
checks have already allowed us to identify errors in the defini-<--size=9.1-->
tion of new constraints. They participate in non-regression testing.<--size=8.9-->
However, SPL testing [15] and ML testing [55] are inherently dif-<--size=9.0-->
ficult activities that we do not yet address; Many algorithms built<--size=9.0-->
into SPL are too resource-intensive (CPU, memory, and time) to<--size=9.1-->
consider sampling techniques [23]. Nonetheless, we believe that<--size=9.1-->
some work on SPL configurations opens up new opportunities to<--size=9.0-->
help build portfolios for automatic algorithm selection [29]. For<--size=9.1-->
example, configuration similarity analysis should help analyze the<--size=8.9-->
coverage of the problem space [3, 15, 27], while modeled features<--size=9.0-->
provide additional information to the metadata usually considered<--size=8.9-->
in meta-learning [32].<--size=9.0-->

Generalizability. . External validity concerns the ability to gen-<--size=9.0-->
eralize the results to other environments [54]. Our study has been<--size=8.9-->

developed in the context of one company, taking into account in-<--size=9.0-->
dustrial applications. However, we have collected applications from<--size=8.9-->
three different sources, which mitigates the risk of dependency on<--size=8.9-->
the companyâ€™s applications. Pattern detection relies on our ability<--size=9.0-->
to distinguish between the problem space and the solution space,<--size=9.0-->
the essence of any SPL. However, we decided to showcase our<--size=9.1-->
work on the particular context of this SPL (i.e., focused on specific<--size=8.9-->
types of ML applications, with scientific knowledge yet to be dis-<--size=9.0-->
covered and with a small set of configurations) because it can be<--size=9.1-->
exploited industrially as is. We could generalize this approach to<--size=9.1-->
other systems as one of our most prized contributions is to build<--size=9.1-->
and evaluate an incremental SPL. However, t the particular context<--size=8.9-->
of this SPL (i.e., focused on specific types of ML applications, with<--size=8.9-->
scientific knowledge yet to be discovered and with a small set of<--size=9.1-->
configurations) does not allow us to state that our contribution<--size=9.1-->
is generalizable. Nevertheless, several subdomains of ML at least<--size=9.1-->
present the same characteristics.<--size=9.0-->

6<--size=10.9-->
CONCLUSION<--size=10.9-->

Recent technological advances have made possible to collect a large<--size=8.9-->
amount of data over time. The purpose of time series data mining<--size=9.0-->
is to enable classification, clustering, or outlier detection [9]. Our<--size=9.0-->
study focuses on this last task. In this paper, we have proposed<--size=9.1-->
a practice-driven approach to build an SPL as a first step toward<--size=9.1-->
allowing the design of generic solutions to detect anomalies in<--size=9.1-->
time series, while capturing new knowledge and capitalizing on<--size=9.1-->
the existing one.<--size=9.0-->
The incrementality in the acquisition of knowledge and the in-<--size=9.0-->
stability of the domain [44] are supported by the SPL through its<--size=9.0-->
structuring and the exploitation of partial configurations associ-<--size=9.1-->
ated with past applications. As far as we know, this is the first case<--size=8.9-->
of application of the SPL paradigm in such a context, and with a<--size=9.1-->
knowledge acquisition objective. We argue that using this para-<--size=9.1-->
digm to record and analyze practices will enable advances in the<--size=9.1-->
selection of ML workflows that are much less energy-intensive than<--size=8.9-->
meta-learning techniques, while assisting scientific knowledge pro-<--size=8.9-->
duction. By capturing practices in partial configurations, we obtain<--size=8.9-->
the abstractions to reason about datasets, solutions, and business<--size=9.0-->
requirements. The SPL is then used both to produce new solutions<--size=8.9-->
and compare them to past solutions, as well as to identify knowl-<--size=9.0-->
edge that was not explicit. The growing abstraction supported by<--size=9.0-->
the SPL also brings other benefits. In mentoring junior data sci-<--size=9.1-->
entists, we have observed a shift in the approach to creating ML<--size=9.1-->
workflows, focusing on analyzing problems before looking for sim-<--size=8.9-->
ilar applications, especially in choosing evaluation metrics. It is<--size=9.1-->
rather difficult for data scientists to explain the precise reasons for<--size=8.9-->
their choice. We observed that focusing only on particular cases<--size=9.1-->
identified as patterns makes the relevant criteria explicit.<--size=9.0-->
This preliminary work paves the way for new software engi-<--size=9.1-->
neering contributions to ML. Our SPL is now evolving through<--size=9.1-->
the various works of data scientists to enrich the knowledge of<--size=9.1-->
anomaly detection in time series. We are working on visualization<--size=8.9-->
tools to facilitate the exploitation of practices, and thus the SPL<--size=9.1-->
maintenance. Distinguishing the users of the SPL from those who<--size=9.0-->
maintain it is also part of our future plan in order to obtain an<--size=9.1-->
empirical validation.<--size=9.0-->

<--page_end:10-->

<--page_start:11-->
Evolvable SPL management with partial knowledge: an application to anomaly detection in time series<--size=7.0-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->

REFERENCES<--size=10.9-->

[1] HervÃ© Abdi and Lynne J Williams. 2010. Principal component analysis. Wiley<--size=7.0-->
interdisciplinary reviews: computational statistics 2, 4 (2010), 433â€“459.<--size=7.0-->
[2] Patrick Aboagye-Sarfo, Qun Mai, Frank M Sanfilippo, David B Preen, Louise M<--size=7.0-->
Stewart, and Daniel M Fatovich. 2015. A comparison of multivariate and univari-<--size=6.9-->
ate time series approaches to modelling and forecasting emergency department<--size=7.0-->
demand in Western Australia. Journal of biomedical informatics 57 (2015), 62â€“73.<--size=6.9-->
[3] M Al-Hajjaji, T ThÃ¼m, J Meinicke, M Lochau ... Software Product Line ..., and<--size=7.0-->
undefined 2014. 2014. Similarity-based prioritization in software product-line<--size=7.0-->
testing. dl.acm.org 1 (sep 2014), 197â€“206. https://doi.org/10.1145/2648511.2648532<--size=6.9-->
[4] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall,<--size=7.0-->
Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann.<--size=7.0-->
2019. Software Engineering for Machine Learning: A Case Study. In Proceedings<--size=6.9-->
- 2019 IEEE/ACM 41st International Conference on Software Engineering: Software<--size=7.0-->
Engineering in Practice, ICSE-SEIP 2019. IEEE, Montreal Quebec Canada, 291â€“300.<--size=6.9-->
https://doi.org/10.1109/ICSE-SEIP.2019.00042<--size=7.0-->
[5] S Apel, D Batory, C KÃ¤stner, and G Saake. 2016. Feature-oriented software product<--size=6.9-->
lines. Springer. https://link.springer.com/content/pdf/10.1007/978-3-642-37521-<--size=6.9-->
7.pdf<--size=7.0-->
[6] Benjamin Benni, Mireille Blay Fornarino, Sebastien Mosser, Frederic Precisio,<--size=7.0-->
and Gunther Jungbluth. 2019. When DevOps meets meta-learning: A portfolio<--size=7.0-->
to rule them all. In Proceedings - 2019 ACM/IEEE 22nd International Conference<--size=7.0-->
on Model Driven Engineering Languages and Systems Companion, MODELS-C<--size=7.0-->
2019. Institute of Electrical and Electronics Engineers Inc., 605â€“612.<--size=7.0-->
https:<--size=7.0-->
//doi.org/10.1109/MODELS-C.2019.00092<--size=7.0-->
[7] Thorsten Berger, Steven She, Rafael Lotufo, Andrzej Wasowski, and Krzysztof<--size=7.0-->
Czarnecki. 2013. A Study of Variability Models and Languages in the Systems<--size=7.0-->
Software Domain. IEEE Transactions on Software Engineering 39, 12 (2013), 1611â€“<--size=6.9-->
1640. https://doi.org/10.1109/TSE.2013.34<--size=7.0-->
[8] Besim Bilalli, Alberto AbellÃ³, and TomÃ s Aluja-Banet. 2017. On the predictive<--size=7.0-->
power of meta-features in OpenML. International Journal of Applied Mathematics<--size=6.9-->
and Computer Science 27, 4 (2017), 697â€“712.<--size=7.0-->
[9] Ane BlÃ¡zquez-GarcÃ­a, Angel Conde, Usue Mori, and Jose A. Lozano. 2021. A<--size=7.0-->
Review on Outlier/Anomaly Detection in Time Series Data. ACM Computing<--size=7.0-->
Surveys (CSUR) 54, 3 (feb 2021), 33. https://doi.org/10.1145/3444690<--size=7.0-->
[10] SÃ©rgio Branco, AndrÃ© G Ferreira, and Jorge Cabral. 2019. Machine learning in<--size=7.0-->
resource-scarce embedded systems, FPGAs, and end-devices: A survey. Electronics<--size=6.9-->
8, 11 (2019), 1289.<--size=7.0-->
[11] Mikel Canizo, Isaac Triguero, Angel Conde, and Enrique Onieva. 2019. Multi-head<--size=7.0-->
CNNâ€“RNN for multi-time series anomaly detection: An industrial case study.<--size=7.0-->
Neurocomputing 363 (2019), 246â€“260.<--size=7.0-->
[12] Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection:<--size=7.0-->
A survey. ACM computing surveys (CSUR) 41, 3 (2009), 1â€“58.<--size=7.0-->
[13] Hoang Anh Dau, Anthony J Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh,<--size=7.0-->
Yan Zhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn J<--size=6.9-->
Keogh. 2018. The UCR Time Series Archive. CoRR abs/1810.07758 (2018), 1â€“12.<--size=7.0-->
http://arxiv.org/abs/1810.07758<--size=7.0-->
[14] Hans Degroote, Bernd Bischl, Lars Kotthoff, and Patrick De Causmaecker. 2016.<--size=7.0-->
Reinforcement Learning for Automatic Online Algorithm Selection - an Empirical<--size=6.9-->
Study. In ITAT 2016 Proceedings, CEUR Workshop Proceedings Vol. 1649 (CEUR<--size=7.0-->
Workshop Proceedings, Vol. 1649), Brona BrejovÃ¡ (Ed.). CEUR-WS.org, 93â€“101.<--size=7.0-->
http://ceur-ws.org/Vol-1649/93.pdf<--size=7.0-->
[15] Xavier Devroey, Gilles Perrouin, Axel Legay, Pierre Yves Schobbens, and Patrick<--size=7.0-->
Heymans. 2015. Covering SPL behaviour with sampled configurations: An initial<--size=6.9-->
assessment. In Proceedings of the Ninth International Workshop on Variability<--size=7.0-->
Modelling of Software-Intensive Systems. ACM Press, Hildesheim, Germany, 59â€“<--size=7.0-->
66. https://doi.org/10.1145/2701319.2701325<--size=7.0-->
[16] Bogdan Dit, Meghan Revelle, Malcom Gethers, and Denys Poshyvanyk. 2013.<--size=7.0-->
Feature location in source code: A taxonomy and survey. Journal of software:<--size=7.0-->
Evolution and Process 25, 1 (jan 2013), 53â€“95. https://doi.org/10.1002/SMR.567<--size=7.0-->
[17] Chris Drummond. 2006. Machine learning as an experimental science (revisited).<--size=7.0-->
In AAAI workshop on evaluation methods for machine learning. AAAI Press,<--size=7.0-->
Phoenix, Arizona USA, 1â€“5.<--size=7.0-->
http://www.aaai.org/Library/Workshops/ws06-<--size=7.0-->
06.php<--size=7.0-->
[18] Sascha El-Sharkawy, Nozomi Yamagishi-Eichler, and Klaus Schmid. 2019. Metrics<--size=7.0-->
for analyzing variability and its implementation in software product lines: A<--size=7.0-->
systematic literature review. Information and Software Technology 106 (feb 2019),<--size=6.9-->
1â€“30. https://doi.org/10.1016/j.infsof.2018.08.015<--size=7.0-->
[19] Dennis ElbrÃ¤chter, Dmytro Perekrestenko, Philipp Grohs, and Helmut BÃ¶lcskei.<--size=7.0-->
2019. Deep neural network approximation theory. CoRR abs/1901.02220 (2019),<--size=7.0-->
1â€“43. http://arxiv.org/abs/1901.02220<--size=7.0-->
[20] Stefan Fischer, Lukas Linsbauer, Roberto E. Lopez-Herrejon, and Alexander Egyed.<--size=7.0-->
2015. The ECCO Tool: Extraction and Composition for Clone-and-Own. In<--size=7.0-->
Proceedings - International Conference on Software Engineering. IEEE, Florence,<--size=7.0-->
Italy, 665â€“668. https://doi.org/10.1109/ICSE.2015.218<--size=7.0-->
[21] Xin He, Kaiyong Zhao, and Xiaowen Chu. 2021. AutoML: A survey of the state-<--size=7.0-->
of-the-art. Knowledge-Based Systems 212 (2021), 106622.<--size=7.0-->

[22] Christopher Henard, Mike Papadakis, Gilles Perrouin, Jacques Klein, Patrick<--size=7.0-->
Heymans, and Yves Le Traon. 2014. Bypassing the combinatorial explosion:<--size=7.0-->
Using similarity to generate and prioritize t-wise test configurations for software<--size=6.9-->
product lines. IEEE Transactions on Software Engineering 40, 7 (2014), 650â€”-670.<--size=7.0-->
https://ieeexplore.ieee.org/abstract/document/6823132/<--size=7.0-->
[23] Ruben Heradio, David Fernandez-Amoros, JosÃ© A. Galindo, David Benavides,<--size=7.0-->
and Don Batory. 2022. Uniform and scalable sampling of highly configurable<--size=7.0-->
systems. Empirical Software Engineering 27, 2 (mar 2022), 44. https://doi.org/10.<--size=6.9-->
1007/s10664-021-10102-5<--size=7.0-->
[24] Mohammad Hossin and Md Nasir Sulaiman. 2015. A review on evaluation<--size=7.0-->
metrics for data classification evaluations. International journal of data mining &<--size=6.9-->
knowledge management process 5, 2 (2015), 1.<--size=7.0-->
[25] Jianglin Huang, Yan-Fu Li, and Min Xie. 2015. An empirical analysis of data<--size=7.0-->
preprocessing for machine learning-based software cost estimation. Information<--size=6.9-->
and software Technology 67 (2015), 108â€“127.<--size=7.0-->
[26] Xiaohui Huang, Yunming Ye, Liyan Xiong, Raymond YK Lau, Nan Jiang, and<--size=7.0-->
Shaokai Wang. 2016. Time series k-means: A new k-means type smooth subspace<--size=6.9-->
clustering for time series data. Information Sciences 367 (2016), 1â€“13.<--size=7.0-->
[27] Christian Kaltenecker, Alexander Grebhahn, Norbert Siegmund, Jianmei Guo,<--size=7.0-->
Sven Apel, and Michael Felderer. 2019. Distance-based sampling of software<--size=7.0-->
configuration spaces. In 2019 IEEE/ACM 41st International Conference on Software<--size=6.9-->
Engineering (ICSE). IEEE Press, 1084â€“1094. https://doi.org/10.1109/ICSE.2019.<--size=7.0-->
00112<--size=7.0-->
[28] T Kehrer, T ThÃ¼m, A Schultheis ... Conference on Software ..., and Undefined<--size=7.0-->
2021. 2021. Bridging the gap between clone-and-own and software product lines.<--size=6.9-->
In 2021 IEEE/ACM 43rd International Conference on Software Engineering: New<--size=7.0-->
Ideas and Emerging Results (ICSE-NIER). IEEE, 21â€“25. https://ieeexplore.ieee.org/<--size=6.9-->
abstract/document/9402254/<--size=7.0-->
[29] Pascal Kerschke, Holger H. Hoos, Frank Neumann, and Heike Trautmann. 2018.<--size=7.0-->
Automated algorithm selection: Survey and perspectives. Evolutionary Computa-<--size=6.9-->
tion 27, 1 (2018), 3â€“45. https://doi.org/10.1162/evco_a_00242 arXiv:1811.11597<--size=7.0-->
[30] Jacob KrÃ¼ger, Wardah Mahmood, and Thorsten Berger. 2020. Promote-pl: a<--size=7.0-->
round-trip engineering process model for adopting and evolving product lines.<--size=7.0-->
In Proceedings of the 24th ACM Conference on Systems and Software Product Line:<--size=6.9-->
Volume A-Volume A, Vol. Part F1642. Association for Computing Machinery,<--size=7.0-->
263â€“273. https://doi.org/10.1145/3382025.3414970<--size=7.0-->
[31] Elias Kuiter, Sebastian Krieter, Jacob KrÃ¼ger, Gunter Saake, and Thomas Leich.<--size=7.0-->
2021. variED: an editor for collaborative, real-time feature modeling. Empirical<--size=7.0-->
Software Engineering 26, 2 (mar 2021), 1â€“47. https://doi.org/10.1007/S10664-020-<--size=6.9-->
09892-X<--size=7.0-->
[32] Luc Lesoil, Hugo Martin, Mathieu Acher, Arnaud Blouin, and Jean-Marc JÃ©zÃ©quel.<--size=7.0-->
2022. Transferring Performance between Distinct Configurable Systems : A Case<--size=6.9-->
Study. Proceedings of the 16th International Working Conference on Variability<--size=7.0-->
Modelling of Software-Intensive Systems 6 (feb 2022), 1â€“6. https://doi.org/10.1145/<--size=6.9-->
3510466.3510486<--size=7.0-->
[33] MaÃ­ra Marques, Jocelyn Simmonds, Pedro O. Rossel, and MarÃ­a Cecilia Bastarrica.<--size=7.0-->
2019. Software product line evolution: A systematic literature review. , 190â€“<--size=7.0-->
208 pages. https://doi.org/10.1016/j.infsof.2018.08.014<--size=7.0-->
[34] Silverio MartÃ­nez-FernÃ¡ndez, Justus Bogner, Xavier Franch, Marc Oriol, Julien<--size=7.0-->
Siebert, Adam Trendowicz, Anna Maria Vollmer, and Stefan Wagner. 2021. Soft-<--size=6.9-->
ware Engineering for AI-Based Systems: A Survey. CoRR abs/2105.0 (may 2021),<--size=6.9-->
54. arXiv:2105.01984 https://arxiv.org/abs/2105.01984v1http://arxiv.org/abs/2105.<--size=6.9-->
01984<--size=7.0-->
[35] Hoan Anh Nguyen, Robert Dyer, Tien N. Nguyen, and Hridesh Rajan. 2014.<--size=7.0-->
Mining preconditions of APIs in large-scale code corpus. In Proceedings of the<--size=7.0-->
ACM SIGSOFT Symposium on the Foundations of Software Engineering (FSE). ACM,<--size=6.9-->
Hong Kong, China, 166â€“177. https://doi.org/10.1145/2635868.2635924<--size=7.0-->
[36] Michael Nieke, Gabriela Sampaio, Thomas ThÃ¼m, Christoph Seidl, Leopoldo<--size=7.0-->
Teixeira, and Ina Schaefer. 2022. Guiding the evolution of product-line con-<--size=7.0-->
figurations.<--size=7.0-->
Software and Systems Modeling 21 (jul 2022), 225â€“247.<--size=7.0-->
https:<--size=7.0-->
//doi.org/10.1007/S10270-021-00906-W/TABLES/5<--size=7.0-->
[37] Lina Ochoa, Juliana Alves Pereira, Oscar GonzÃ¡lez-Rojas, Harold Castro, and<--size=7.0-->
Gunter Saake. 2017. A survey on scalability and performance concerns in ex-<--size=7.0-->
tended product lines configuration. In Proceedings of the Eleventh International<--size=7.0-->
Workshop on Variability Modelling of Software-intensive Systems. Association for<--size=6.9-->
Computing Machinery, 5â€“12. https://doi.org/10.1145/3023956.3023959<--size=7.0-->
[38] Pascal Olz, Conny and Biundo, Susanne and Bercher. 2021. Revealing Hidden<--size=7.0-->
Preconditions and Effects of Compound HTN Planning Tasksâ€“A Complexity<--size=7.0-->
Analysis. In 35th AAAI Conference on Artificial Intelligence (AAAI). AAAI Press.<--size=7.0-->
AAAI Press, Virtual Event, 1903â€“11912. https://www.aaai.org/AAAI21Papers/<--size=7.0-->
AAAI-655.OlzC.pdf<--size=7.0-->
[39] OMG. 2006.<--size=7.0-->
Business Process Modeling Notation (BPMN) Specification, Final<--size=7.0-->
Adopted Specification. Technical Report. Object Management Group (OMG).<--size=7.0-->
[40] S. Gopal Krishna Patro and Kishore Kumar Sahu. 2015. Normalization: A Prepro-<--size=7.0-->
cessing Stage. CoRR abs/1503.06462 (2015), 1â€“3. http://arxiv.org/abs/1503.06462<--size=6.9-->
[41] Juliana Alves Pereira, Sebastian Krieter, Jens Meinicke, Reimar SchrÃ¶ter, Gunter<--size=7.0-->
Saake, and Thomas Leich. 2016. FeatureIDE: Scalable product configuration of<--size=7.0-->
variable systems. In International Conference on Software Reuse, Lecture Notes in<--size=7.0-->

<--page_end:11-->

<--page_start:12-->
SPLC â€™22, September 12â€“16, 2022, Graz, Austria<--size=7.0-->
El Amraoui et al.<--size=7.0-->

Computer Science, Vol. 9679. Springer Verlag, 397â€“401. https://doi.org/10.1007/<--size=7.0-->
978-3-319-35122-3_27<--size=7.0-->
[42] Juliana Alves Pereira, Pawel Matuszyk, Sebastian Krieter, Myra Spiliopoulou,<--size=7.0-->
and Gunter Saake. 2018. Personalized recommender systems for product-line<--size=7.0-->
configuration processes. Computer Languages, Systems and Structures 54 (2018),<--size=7.0-->
451â€“471. https://doi.org/10.1016/j.cl.2018.01.003<--size=7.0-->
[43] Nelishia Pillay, Rong Qu, Dipti Srinivasan, Barbara Hammer, and Kenneth<--size=7.0-->
Sorensen. 2018. Automated design of machine learning and search algorithms<--size=7.0-->
[guest editorial]. IEEE Computational intelligence magazine 13, 2 (2018), 16â€“17.<--size=7.0-->
[44] Klaus Pohl, GÃ¼nter BÃ¶ckle, and Frank J van der Linden. 2005. Software Product<--size=7.0-->
Line Engineering: Foundations, Principles and Techniques. Springer-Verlag.<--size=7.0-->
[45] BelÃ©n Ramos-GutiÃ©rrez, Ãngel JesÃºs Varela-Vaca, JosÃ© A. Galindo, MarÃ­a Teresa<--size=7.0-->
GÃ³mez-LÃ³pez, and David Benavides. 2021. Discovering configuration workflows<--size=6.9-->
from existing logs using process mining. Empir. Softw. Eng. 26, 1 (jan 2021), 11.<--size=7.0-->
https://doi.org/10.1007/s10664-020-09911-x<--size=7.0-->
[46] Erich Schubert, JÃ¶rg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei Xu.<--size=7.0-->
2017. DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.<--size=6.9-->
ACM Transactions on Database Systems (TODS) 42, 3 (2017), 1â€“21.<--size=7.0-->
[47] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar<--size=7.0-->
Ebner, Vinay Chaudhary, and Michael Young. 2015. Machine Learning: The High<--size=6.9-->
Interest Credit Card of Technical Debt. In Proceedings of the 28th International<--size=7.0-->
Conference on Neural Information Processing Systems - Volume 2, Ghahramani<--size=7.0-->
Zoubin, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger<--size=6.9-->
(Eds.). MIT Press, Montreal, Canada, 2503â€“2511. https://ai.google/research/pubs/<--size=6.9-->
pub43146<--size=7.0-->
[48] Marina Sokolova, Nathalie Japkowicz, and Stan Szpakowicz. 2006. Beyond ac-<--size=7.0-->
curacy, F-score and ROC: a family of discriminant measures for performance<--size=7.0-->
evaluation. In Australasian joint conference on artificial intelligence. Springer,<--size=7.0-->

Springer, Berlin, Heidelberg, 1015â€“1021.<--size=7.0-->
[49] Leopoldo Teixeira, Rohit Gheyi, and Paulo Borba. 2020. Safe Evolution of Product<--size=7.0-->
Lines Using Configuration Knowledge Laws. In Brazilian Symposium on Formal<--size=7.0-->
Methods, Lecture Notes in Computer Science, Vol. 12475 LNCS. Springer, Cham,<--size=7.0-->
210â€“227. https://doi.org/10.1007/978-3-030-63882-5_13<--size=7.0-->
[50] A Tornhill. 2015. Your Code as a Crime Scene. Pragmatic Bookshelf.<--size=7.0-->
https:<--size=7.0-->
//books.google.fr/books?id=l7dDnQAACAAJ<--size=7.0-->
[51] Mathias Uta, Alexander Felfernig, Viet Man Le, Andrei Popescu, Thi Ngoc Trang<--size=7.0-->
Tran, and Denis Helic. 2021. Evaluating recommender systems in feature model<--size=6.9-->
configuration. In Proceedings of the 25th ACM International Systems and Software<--size=6.9-->
Product Line Conference, Vol. Part F1716. ACM, New York, NY, USA, 58â€“63. https:<--size=6.9-->
//doi.org/10.1145/3461001.3471144<--size=7.0-->
[52] Jan N. Van Rijn and Joaquin Vanschoren. 2015. Sharing RapidMiner workflows<--size=7.0-->
and experiments with OpenML. In CEUR Workshop Proceedings, Vol. 1455. CEUR-<--size=6.9-->
WS, 93â€“103.<--size=7.0-->
[53] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. 2013. OpenML:<--size=7.0-->
Networked Science in Machine Learning. SIGKDD Explorations 15, 2 (2013), 49â€“60.<--size=6.9-->
https://doi.org/10.1145/2641190.2641198 arXiv:1407.7722<--size=7.0-->
[54] C Wohlin, P Runeson, M HÃ¶st, MC Ohlsson, and B Regnell. 2012.<--size=7.0-->
Experimentation<--size=7.0-->
in<--size=7.0-->
software<--size=7.0-->
engineering.<--size=7.0-->
Springer.<--size=7.0-->
1â€“236<--size=7.0-->
pages.<--size=7.0-->
https://books.google.com/books?hl=fr&lr=&id=QPVsM1_U8nkC&oi=fnd&pg=<--size=7.0-->
PR5&dq=Experimentation+in+Software+Engineering.&ots=GPx7rciRCu&sig=<--size=7.0-->
KyBLRUIbGY48ZlXMyE9nRVCbP_o<--size=7.0-->
[55] Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2022.<--size=7.0-->
Machine Learn-<--size=7.0-->
ing Testing: Survey, Landscapes and Horizons. IEEE Transactions on Software<--size=7.0-->
Engineering 48, 01 (jan 2022), 1â€“36. https://doi.org/10.1109/TSE.2019.2962027<--size=7.0-->
arXiv:1906.10742<--size=7.0-->

<--page_end:12-->

