<--size:10.909000396728516-->1 INTRODUCTION<--size:10.909000396728516-->
<--size:9.055215835571289-->Configurable software systems can be customized to meet differ-<--size:9.055215835571289-->
<--size:8.92556095123291-->ent use cases by setting binary and numeric configuration options.<--size:8.92556095123291-->
<--size:9.055215835571289-->Usually, a variability model (e.g., a feature model [ 1 ]) describes<--size:9.055215835571289-->
<--size:9.01071834564209-->all configuration options and constraints among them. A configu-<--size:9.01071834564209-->
<--size:9.055215835571289-->ration specifies a system variant’s functional and non-functional<--size:9.055215835571289-->
<--size:8.903016090393066-->properties. This way, users can tailor the software system not only<--size:8.903016090393066-->
<--size:9.055215835571289-->according to their functional requirements, but also optimize it<--size:9.055215835571289-->
<--size:9.055215835571289-->with respect to non-functional properties, such as response time,<--size:9.055215835571289-->
<--size:8.965999603271484-->memory size, and energy consumption.<--size:8.965999603271484-->
<--size:9.041889190673828-->Optimizing non-functional properties is a non-trivial task and<--size:9.041889190673828-->
<--size:9.055215835571289-->has been subject to considerable research [ 7 , 8 , 15 , 16 , 19 , 24 , 26 ,<--size:9.055215835571289-->
<--size:8.965999603271484-->27 ]. Most approaches rely on a surrogate model in the form of an<--size:8.965999603271484-->
<--size:9.055215835571289-->attributed variability model (AVM) , which specifies not only the<--size:9.055215835571289-->
<--size:9.055215835571289-->variability of the configurable system, but also assigns attributes<--size:9.055215835571289-->
<--size:9.055215835571289-->to individual options and interactions among options. Figure 1<--size:9.055215835571289-->
<--size:8.979438781738281-->depicts an exemplary AVM with energy consumption as attribute.<--size:8.979438781738281-->
<--size:8.965999603271484-->Using all individual options’ attribute values, we can compute the<--size:8.965999603271484-->
<--size:8.880414009094238-->resulting configuration’s value by summing up the attribute values<--size:8.880414009094238-->
<--size:9.055215835571289-->of selected options. Equation 1 formalizes this computation as a<--size:9.055215835571289-->
<--size:8.875886917114258-->polynomial of options o 1 , . . . , o n ∈{ 0 , 1 } that form a configuration<--size:8.875886917114258-->
<--size:8.965999603271484-->C i from the set of all valid configurations C . The coefficient α i<--size:8.965999603271484-->
<--size:8.898500442504883-->represent the influence of o i on the attribute and α 0 represents the<--size:8.898500442504883-->
<--size:9.055215835571289-->base values of the software system. Function π : C → R maps<--size:9.055215835571289-->
<--size:9.055215835571289-->a given configuration to a real value quantifying the property in<--size:9.055215835571289-->
<--size:8.965999603271484-->question:<--size:8.965999603271484-->
<--size:8.965999603271484-->π ( C i ) = α 0 + α 1 · o 1 + α 2 · o 2 + · · · + α n · o n<--size:8.965999603271484-->
<--size:8.965999603271484-->(1)<--size:8.965999603271484-->
<--size:8.875886917114258-->Note that additional terms for interactions among options might be<--size:8.875886917114258-->
<--size:8.921056747436523-->added as software systems usually have non-linear non-functional<--size:8.921056747436523-->
<--size:8.965999603271484-->behavior.<--size:8.965999603271484-->
<--size:5.071676254272461-->Energy<--size:5.071676254272461-->
<--size:5.071676254272461-->Consumption<--size:5.071676254272461-->
<--size:3.381117343902588-->+10 Wh<--size:3.381117343902588-->
<--size:3.381117343902588-->+2 Wh<--size:3.381117343902588-->
<--size:3.381117343902588-->+1 Wh<--size:3.381117343902588-->
<--size:3.381117343902588-->+3 Wh<--size:3.381117343902588-->
<--size:3.381117343902588-->+5 Wh<--size:3.381117343902588-->
<--size:8.965999603271484-->Figure 1: Simplified attributed variability model of the data-<--size:8.965999603271484-->
<--size:8.965999603271484-->base system SQLite. Each option shows the estimated energy<--size:8.965999603271484-->
<--size:8.965999603271484-->consumption in terms of its attribute value.<--size:8.965999603271484-->
<--size:8.965999603271484-->VaMoS ’20, February 5–7, 2020, Magdeburg, Germany<--size:8.965999603271484-->
<--size:8.965999603271484-->Johannes Dorn, Sven Apel, and Norbert Siegmund<--size:8.965999603271484-->
<--size:8.94355583190918-->AVMs support optimization as they evaluate attribute values of<--size:8.94355583190918-->
<--size:9.028543472290039-->individual configurations without expensive measurement of the<--size:9.028543472290039-->
<--size:9.055215835571289-->corresponding variant. Beside optimization, developers may use<--size:9.055215835571289-->
<--size:8.875886917114258-->AVMs to analyze which options slow down their system and which<--size:8.875886917114258-->
<--size:8.965999603271484-->variant fits on memory-restricted devices.<--size:8.965999603271484-->
<--size:9.055215835571289-->Typically, an AVM represents attribute values of a single envi-<--size:9.055215835571289-->
<--size:9.055215835571289-->ronment [ 6 , 10 – 12 ]. That is, the values have been determined by<--size:9.055215835571289-->
<--size:8.875886917114258-->measuring the software system in a specific version, with a specific<--size:8.875886917114258-->
<--size:9.037443161010742-->workload, and on specific hardware. As soon as the environment<--size:9.037443161010742-->
<--size:8.875886917114258-->changes, one cannot guarantee the soundness of the AVM anymore<--size:8.875886917114258-->
<--size:9.028543472290039-->since the attribute values may not reflect the specifics of the new<--size:9.028543472290039-->
<--size:9.055215835571289-->environment. Unfortunately, software is in constant change, and<--size:9.055215835571289-->
<--size:8.875886917114258-->different customers have different workloads and hardware settings,<--size:8.875886917114258-->
<--size:8.965999603271484-->rendering a static AVM insufficient.<--size:8.965999603271484-->
<--size:8.875886917114258-->Current practice is to obtain an entirely new AVM for the changed<--size:8.875886917114258-->
<--size:9.055215835571289-->environment by measuring non-functional properties again for<--size:9.055215835571289-->
<--size:9.015177726745605-->the same variants in the new environment and afterwards deduc-<--size:9.015177726745605-->
<--size:9.046334266662598-->ing the changed attribute values, for instance, via machine learn-<--size:9.046334266662598-->
<--size:8.875886917114258-->ing [ 5 , 17 , 20 ]. Clearly, this does not scale for frequent environmen-<--size:8.875886917114258-->
<--size:9.055215835571289-->tal changes and is also not efficient as always some information<--size:9.055215835571289-->
<--size:8.884939193725586-->remains relevant in the new environment. For example, Valov et al .<--size:8.884939193725586-->
<--size:9.055215835571289-->have shown that a change in hardware often only shifts the op-<--size:9.055215835571289-->
<--size:8.875886917114258-->tions’ influences by a linear amount [ 25 ]. Their relative importance<--size:8.875886917114258-->
<--size:9.055215835571289-->and ranking stay the same. Jamshidi et al . observed in an empir-<--size:9.055215835571289-->
<--size:9.055215835571289-->ical study that performance distributions, relative importance of<--size:9.055215835571289-->
<--size:9.055215835571289-->options and interactions, and the ranking of configurations with<--size:9.055215835571289-->
<--size:8.939061164855957-->respect to performance maintains some characteristics for various<--size:8.939061164855957-->
<--size:8.965999603271484-->environmental changes of configurable software systems [10].<--size:8.965999603271484-->
<--size:8.875886917114258-->Transfer learning is a technique for transferring knowledge from<--size:8.875886917114258-->
<--size:9.055215835571289-->a source model to a target model such that only few additional<--size:9.055215835571289-->
<--size:9.006256103515625-->measurements need to be conducted in the changed (target) envi-<--size:9.006256103515625-->
<--size:8.875886917114258-->ronment. The soundness of the new model depends on the quality of<--size:8.875886917114258-->
<--size:8.934563636779785-->the transfer-learning technique and its applicability to the specific<--size:8.934563636779785-->
<--size:8.997325897216797-->case. Hence, transfer-learning techniques must be evaluated care-<--size:8.997325897216797-->
<--size:9.028543472290039-->fully to find the best target model. Evaluating a transfer-learning<--size:9.028543472290039-->
<--size:8.875886917114258-->technique can be done by assessing the accuracy of the target model<--size:8.875886917114258-->
<--size:8.875886917114258-->for a representative set of environment changes. But here comes the<--size:8.875886917114258-->
<--size:8.875886917114258-->caveat: To extensively evaluate the AVM’s accuracy, we would need<--size:8.875886917114258-->
<--size:8.875886917114258-->to measure all variants across all environments. Clearly, this is infea-<--size:8.875886917114258-->
<--size:8.997325897216797-->sible even for medium-sized configurable software systems. Here,<--size:8.997325897216797-->
<--size:9.055215835571289-->we struggle not only with the complexity of a software system’s<--size:9.055215835571289-->
<--size:8.970481872558594-->configuration space, but also with an exponential number of envi-<--size:8.970481872558594-->
<--size:9.037443161010742-->ronments affecting the system’s non-functional properties. Thus,<--size:9.037443161010742-->
<--size:8.893982887268066-->the experimental effort for evaluating transfer-learning techniques<--size:8.893982887268066-->
<--size:8.875886917114258-->in the field of configurable software systems is even more challeng-<--size:8.875886917114258-->
<--size:9.055215835571289-->ing than for optimization techniques for single AVMs. Moreover,<--size:9.055215835571289-->
<--size:9.055215835571289-->for the development of new techniques in a single environment,<--size:9.055215835571289-->
<--size:9.041889190673828-->researchers often rely on synthetic, non-realistic attribute values<--size:9.041889190673828-->
<--size:9.055215835571289-->due to the huge measurement effort [ 21 ]. This can lead to biased<--size:9.055215835571289-->
<--size:8.903016090393066-->and non-optimal techniques when applied to realistic settings [ 21 ].<--size:8.903016090393066-->
<--size:9.055215835571289-->We expect that this problem even grows for the development of<--size:9.055215835571289-->
<--size:8.965999603271484-->transfer-learning techniques.<--size:8.965999603271484-->
<--size:9.019635200500488-->To address these problems and ease the development and eval-<--size:9.019635200500488-->
<--size:9.028543472290039-->uation of new practical transfer-learning techniques, we propose<--size:9.028543472290039-->
<--size:9.019635200500488-->Loki, a tool that synthesizes a realistic target AVM that is related<--size:9.019635200500488-->
<--size:8.952540397644043-->to, but different from a given source AVM. Loki supports the eval-<--size:8.952540397644043-->
<--size:8.988387107849121-->uation of transfer-learning techniques on several synthetic AVMs<--size:8.988387107849121-->
<--size:9.055215835571289-->and requires only a single source AVM, which can be based on<--size:9.055215835571289-->
<--size:8.875886917114258-->measurements. Using user specifications regarding the target AVM,<--size:8.875886917114258-->
<--size:9.055215835571289-->we build a set of candidate AVMs by modifying the source AVM<--size:9.055215835571289-->
<--size:9.041889190673828-->with operators that exist for certain user specifications. Next, we<--size:9.041889190673828-->
<--size:8.948049545288086-->use a genetic algorithm to find candidate AVMs with attribute val-<--size:8.948049545288086-->
<--size:8.875886917114258-->ues that best satisfy all user specifications, including those without<--size:8.875886917114258-->
<--size:8.965999603271484-->applicable operators.<--size:8.965999603271484-->
<--size:9.055215835571289-->Suppose the AVM of the database system in Figure 1 has been<--size:9.055215835571289-->
<--size:9.055215835571289-->built based on measurements on cheap cluster machines, but a<--size:9.055215835571289-->
<--size:8.875886917114258-->company intents to deploy it on high-performance server machines.<--size:8.875886917114258-->
<--size:8.875886917114258-->The company could generate a range of AVMs with different degrees<--size:8.875886917114258-->
<--size:8.875886917114258-->of relatedness to the source AVM in Figure 1 and test their transfer-<--size:8.875886917114258-->
<--size:8.939061164855957-->learning technique against the resulting pairs of source and target<--size:8.939061164855957-->
<--size:9.055215835571289-->models. They can also apply optimization on the target AVMs to<--size:9.055215835571289-->
<--size:9.024090766906738-->infer which configurations are suitable in range of environments.<--size:9.024090766906738-->
<--size:9.055215835571289-->This way, a subsequent optimization of the database system in<--size:9.055215835571289-->
<--size:8.965999603271484-->different environments can be simulated upfront.<--size:8.965999603271484-->
<--size:9.055215835571289-->Technically, we re-implemented the tool Thor 1 [ 21 ] as an op-<--size:9.055215835571289-->
<--size:8.992857933044434-->tional pre-step to start with a realistic AVM in cases where actual<--size:8.992857933044434-->
<--size:9.055215835571289-->measurements are not possible to conduct at all. To summarize,<--size:9.055215835571289-->
<--size:8.965999603271484-->our contributions are as follows:<--size:8.965999603271484-->
<--size:8.965999603271484-->• an approach to synthesize a realistic target AVM that is<--size:8.965999603271484-->
<--size:9.01071834564209-->related to a given source AVM to support the development<--size:9.01071834564209-->
<--size:8.965999603271484-->and evaluation of transfer-learning techniques;<--size:8.965999603271484-->
<--size:8.965999603271484-->• Loki, an open-source 2 tool that implements our approach;<--size:8.965999603271484-->
<--size:8.965999603271484-->• a replication of the existing transfer-learning approach Learn<--size:8.965999603271484-->
<--size:8.965999603271484-->to Sample [11] using Loki to demonstrate its applicability.<--size:8.965999603271484-->
<--size:10.909000396728516-->2 PRELIMINARIES AND RELATED WORK<--size:10.909000396728516-->
<--size:10.909000396728516-->2.1<--size:10.909000396728516-->
<--size:10.909000396728516-->Transfer Learning<--size:10.909000396728516-->
<--size:8.921056747436523-->Attribute values in an AVM are either assigned manually based on<--size:8.921056747436523-->
<--size:9.046334266662598-->user experience or can be determined via machine learning from<--size:9.046334266662598-->
<--size:9.032994270324707-->a set of measurements, referred to as a training set . Typically, the<--size:9.032994270324707-->
<--size:8.875886917114258-->training set is acquired in a single fixed environment; consequently,<--size:8.875886917114258-->
<--size:9.055215835571289-->the inferred AVM is expected to be valid for the respective envi-<--size:9.055215835571289-->
<--size:8.893982887268066-->ronment only, and likely to perform worse for other environments.<--size:8.893982887268066-->
<--size:8.875886917114258-->Among several components, an environment is composed of (1) the<--size:8.875886917114258-->
<--size:9.055215835571289-->hardware setup that runs the software system, (2) the workload<--size:9.055215835571289-->
<--size:8.965999603271484-->that is executed, and (3) the version of the software system.<--size:8.965999603271484-->
<--size:9.032994270324707-->In the worst case, for each environment, a new AVM has to be<--size:9.032994270324707-->
<--size:8.875886917114258-->obtained from scratch with entirely new measurements. By contrast,<--size:8.875886917114258-->
<--size:8.94355583190918-->transfer-learning techniques leverage knowledge from an existing<--size:8.94355583190918-->
<--size:8.965999603271484-->model to infer a new model more data-efficiently.<--size:8.965999603271484-->
<--size:9.055215835571289-->Different approaches to transfer learning have been proposed<--size:9.055215835571289-->
<--size:9.006256103515625-->in the past. Jamshidi et al . developed Learning to Sample (L2S) , an<--size:9.006256103515625-->
<--size:8.875886917114258-->approach that identifies influential options for an existing AVM and<--size:8.875886917114258-->
<--size:8.875886917114258-->guides measurements in the new environment to reuse information<--size:8.875886917114258-->
<--size:8.916550636291504-->on these options [ 11 ]. Valov et al . use a small training set from the<--size:8.916550636291504-->
<--size:8.875886917114258-->new environment to infer a linear transfer function for the existing<--size:8.875886917114258-->
<--size:8.997325897216797-->AVM, resulting in an AVM for the new environment [ 25 ]. Among<--size:8.997325897216797-->
<--size:9.028543472290039-->a guided-measurement, linear-transfer and tree-based non-linear<--size:9.028543472290039-->
<--size:8.884939193725586-->transfer approach, Iqbal et al . found the guided-sampling approach<--size:8.884939193725586-->
<--size:5.480000019073486-->1 Thor allows users to generate an AVM based on a given variability model and a<--size:5.480000019073486-->
<--size:6.903907775878906-->specification of attribute values and interactions. The generated values mimic the ones<--size:6.903907775878906-->
<--size:6.973999977111816-->found in real-world software systems.<--size:6.973999977111816-->
<--size:5.480000019073486-->2 Download and contribute to Loki at https://github.com/digital-bauhaus/Loki<--size:5.480000019073486-->
<--size:8.965999603271484-->Generating Attributed Variability Models for Transfer Learning<--size:8.965999603271484-->
<--size:8.965999603271484-->VaMoS ’20, February 5–7, 2020, Magdeburg, Germany<--size:8.965999603271484-->
<--size:8.875886917114258-->to be superior for transfer learning hyper parameters of deep neural<--size:8.875886917114258-->
<--size:8.965999603271484-->networks across different machines [9].<--size:8.965999603271484-->
<--size:8.93006420135498-->Reasons for the effectiveness of transfer learning were found in<--size:8.93006420135498-->
<--size:8.952540397644043-->an exploratory study [ 10 ] and confirmed by a causal analysis [ 12 ]:<--size:8.952540397644043-->
<--size:8.965999603271484-->• For non-severe hardware environment changes, a strong<--size:8.965999603271484-->
<--size:8.875886917114258-->linear correlation has been found for option attribute values.<--size:8.875886917114258-->
<--size:8.965999603271484-->• Across all studied environment changes, the distribution of<--size:8.965999603271484-->
<--size:9.015177726745605-->variant attribute values stays similar as measured with the<--size:9.015177726745605-->
<--size:8.965999603271484-->Kullback-Leibler-Divergence [2].<--size:8.965999603271484-->
<--size:8.965999603271484-->• Rank correlation of the variant attribute values proved to be<--size:8.965999603271484-->
<--size:8.965999603271484-->even more robust than Kullback-Leibler-Divergence.<--size:8.965999603271484-->
<--size:8.965999603271484-->• Half of all options and 6 − 28% of all possible interactions<--size:8.965999603271484-->
<--size:8.880414009094238-->were classified as influential and remained influential across<--size:8.880414009094238-->
<--size:8.965999603271484-->environments.<--size:8.965999603271484-->
<--size:8.965999603271484-->• The attribute values for the investigated interactions de-<--size:8.965999603271484-->
<--size:8.965999603271484-->picted a high linear correlation.<--size:8.965999603271484-->
<--size:9.01071834564209-->To the best of our knowledge, there exists no test-bed creation<--size:9.01071834564209-->
<--size:8.948049545288086-->tool for transfer learning that fits into a researcher’s experimental<--size:8.948049545288086-->
<--size:8.875886917114258-->pipeline, despite active research in the field of configurable software<--size:8.875886917114258-->
<--size:9.055215835571289-->systems. Such a tool should provide means to synthesize AVMs<--size:9.055215835571289-->
<--size:8.965999603271484-->based on a given AVM and reflect the aspects listed above.<--size:8.965999603271484-->
<--size:10.909000396728516-->2.2<--size:10.909000396728516-->
<--size:10.909000396728516-->State-of-the-Art AVM Synthesis<--size:10.909000396728516-->
<--size:9.024090766906738-->Loki is neither the first tool to synthesize AVMs nor is it the first<--size:9.024090766906738-->
<--size:9.050776481628418-->to target transfer learning. In the following, we describe existing<--size:9.050776481628418-->
<--size:8.880414009094238-->and missing capabilities of publicly available tools for synthesizing<--size:8.880414009094238-->
<--size:8.965999603271484-->AVMs.<--size:8.965999603271484-->
<--size:9.01071834564209-->Thor. Thor [ 21 ] is designed to support the evaluation of algo-<--size:9.01071834564209-->
<--size:9.055215835571289-->rithms and tools operating on a single AVM. It enables users to<--size:9.055215835571289-->
<--size:9.055215835571289-->synthesize attribute values for arbitrarily large and constrained<--size:9.055215835571289-->
<--size:9.055215835571289-->target variability models while behaving like a supplied source<--size:9.055215835571289-->
<--size:9.055215835571289-->AVM. Thor uses the genetic algorithm NSGA-II [ 4 ] to optimize<--size:9.055215835571289-->
<--size:9.001791954040527-->distribution similarities of option and interaction attribute values<--size:9.001791954040527-->
<--size:9.055215835571289-->as well as variant attribute values to synthesize a realistic AVM.<--size:9.055215835571289-->
<--size:9.006256103515625-->Varying the target model’s number of options and interactions, it<--size:9.006256103515625-->
<--size:8.8894624710083-->is possible to test an algorithm’s fitness on problems with different<--size:8.8894624710083-->
<--size:8.965999603271484-->complexities.<--size:8.965999603271484-->
<--size:8.916550636291504-->Loki covers the functionality of Thor while adding support for<--size:8.916550636291504-->
<--size:8.965999603271484-->transfer learning, which we describe in Section 3.<--size:8.965999603271484-->
<--size:8.934563636779785-->GenPerf. Jamshidi et al . use the tool GenPerf to evaluate their<--size:8.934563636779785-->
<--size:9.055215835571289-->transfer-learning approach [ 11 ]. Given a source AVM, GenPerf<--size:9.055215835571289-->
<--size:8.961515426635742-->synthesizes a target AVM that covers a subset of aspects observed<--size:8.961515426635742-->
<--size:9.055215835571289-->with real pairs of source and target AVMs. GenPerf is able to<--size:9.055215835571289-->
<--size:9.055215835571289-->synthesize target AVMs whose option attribute values correlate<--size:9.055215835571289-->
<--size:9.055215835571289-->to a (user-specified degree) with the given source AVM and, at<--size:9.055215835571289-->
<--size:9.055215835571289-->the same time, keeps the variant attribute distributions of both<--size:9.055215835571289-->
<--size:8.875886917114258-->AVMs similar. In this process, GenPerf changes attribute values for<--size:8.875886917114258-->
<--size:8.880414009094238-->options and interactions, while setting the attribute value for some<--size:8.880414009094238-->
<--size:8.875886917114258-->attributes and interactions to zero and increasing the value of others<--size:8.875886917114258-->
<--size:8.875886917114258-->significantly in a stochastic manner; thus, not all influential options<--size:8.875886917114258-->
<--size:8.875886917114258-->stay influential across environments and previously non-influential<--size:8.875886917114258-->
<--size:8.961515426635742-->options may become influential [ 11 ]. However, GenPerf does not<--size:8.961515426635742-->
<--size:8.965999603271484-->offer the selective change of only the most influential options and<--size:8.965999603271484-->
<--size:9.055215835571289-->interaction attribute values nor a way to ensure rank correlation<--size:9.055215835571289-->
<--size:8.884939193725586-->between source and target variant attribute values. In addition, the<--size:8.884939193725586-->
<--size:8.952540397644043-->user needs to define the number of desired influential options and<--size:8.952540397644043-->
<--size:8.934563636779785-->interactions as program variables, which requires the user to have<--size:8.934563636779785-->
<--size:9.055215835571289-->detailed knowledge of the source AVM and GenPerf’s internals.<--size:9.055215835571289-->
<--size:8.965999603271484-->This is, however, an unrealistic requirement.<--size:8.965999603271484-->
<--size:8.875886917114258-->BeTTy. While Thor generates attributions for variability models<--size:8.875886917114258-->
<--size:9.055215835571289-->based on existing AVMs, BeTTy [ 18 ] lets the user specify most<--size:9.055215835571289-->
<--size:9.055215835571289-->parameters for AVM generation. Attribute distributions may be<--size:9.055215835571289-->
<--size:9.055215835571289-->specified for options and interactions as well as parameters for<--size:9.055215835571289-->
<--size:9.032994270324707-->constraints and size of the target model. BeTTy does not provide<--size:9.032994270324707-->
<--size:9.001791954040527-->a mechanism to ensure a realistic distribution of variant attribute<--size:9.001791954040527-->
<--size:8.875886917114258-->values and does not support the generation of changed AVMs from<--size:8.875886917114258-->
<--size:8.965999603271484-->original AVMs.<--size:8.965999603271484-->
<--size:10.909000396728516-->3 SYNTHESIZING TARGET AVMS WITH LOKI<--size:10.909000396728516-->
<--size:10.909000396728516-->3.1<--size:10.909000396728516-->
<--size:10.909000396728516-->Overview<--size:10.909000396728516-->
<--size:8.992857933044434-->With Loki, we aim at applying realistic changes to a source AVM,<--size:8.992857933044434-->
<--size:9.055215835571289-->yielding a target model that may be used to evaluate transfer-<--size:9.055215835571289-->
<--size:9.055215835571289-->learning approaches. To be realistic, we pursue the synthesis of<--size:9.055215835571289-->
<--size:9.055215835571289-->all common aspects among environment changes, as laid out in<--size:9.055215835571289-->
<--size:8.939061164855957-->Section 2.1. With Loki, users are able to create a highly controlled<--size:8.939061164855957-->
<--size:8.875886917114258-->test bed in which they can control the degrees and types of changes<--size:8.875886917114258-->
<--size:8.965999603271484-->they want to see reflected in the target model.<--size:8.965999603271484-->
<--size:8.875886917114258-->Figure 2 visualizes the steps of our approach. Before synthesizing<--size:8.875886917114258-->
<--size:9.001791954040527-->a target model, the user may use the included Thor functionality<--size:9.001791954040527-->
<--size:9.055215835571289-->to derive a large source AVM from a given smaller AVM or com-<--size:9.055215835571289-->
<--size:9.041889190673828-->mon variability model (step 0). Using the synthesized AVM or an<--size:9.041889190673828-->
<--size:9.055215835571289-->AVM based on real measurements, Loki first forms an initial set<--size:9.055215835571289-->
<--size:9.01071834564209-->of candidate AVMs following the process described in Section 3.2<--size:9.01071834564209-->
<--size:9.055215835571289-->(step 1). This set AVMs forms the initial population for the opti-<--size:9.055215835571289-->
<--size:9.055215835571289-->mization step (step 2) using a genetic algorithm (GA), which we<--size:9.055215835571289-->
<--size:9.01071834564209-->explain in Section 3.3. Steps 1 and 2 are responsible for satisfying<--size:9.01071834564209-->
<--size:8.974961280822754-->user requirements regarding the conformity with realistic aspects<--size:8.974961280822754-->
<--size:8.875886917114258-->of environment changes as well as user-defined challenges to evalu-<--size:8.875886917114258-->
<--size:8.92556095123291-->ate transfer-learning techniques. Finally, Loki exports all AVMs of<--size:8.92556095123291-->
<--size:8.875886917114258-->the final population and provides analytic plots as well as a python<--size:8.875886917114258-->
<--size:9.055215835571289-->module that provides a sampling API for the new AVMs (step 3).<--size:9.055215835571289-->
<--size:9.055215835571289-->Alternatively, the user can constrain the export to a single AVM<--size:9.055215835571289-->
<--size:9.055215835571289-->that has the highest score according to a weighted average of all<--size:9.055215835571289-->
<--size:8.965999603271484-->fitness functions.<--size:8.965999603271484-->
<--size:9.046334266662598-->User specifications and optional expert settings are stated in a<--size:9.046334266662598-->
<--size:9.055215835571289-->single YAML configuration file, whose format is documented at<--size:9.055215835571289-->
<--size:8.94355583190918-->Loki’s online repository. In the following, we describe steps 1 and<--size:8.94355583190918-->
<--size:9.001791954040527-->2 of our approach in more detail. We refer to Siegmund et al . [ 21 ]<--size:9.001791954040527-->
<--size:8.965999603271484-->for details on the Thor functionality in step 0.<--size:8.965999603271484-->
<--size:10.909000396728516-->3.2<--size:10.909000396728516-->
<--size:10.909000396728516-->Generating Candidate AVMs<--size:10.909000396728516-->
<--size:8.875886917114258-->In step 1, Loki obtains a set of initial candidate AVMs, which forms<--size:8.875886917114258-->
<--size:8.898500442504883-->the starting point for the GA in step 2. The process starts by evalu-<--size:8.898500442504883-->
<--size:8.912040710449219-->ating the user specifications for generating the AVM. The user can<--size:8.912040710449219-->
<--size:8.965999603271484-->specify the following:<--size:8.965999603271484-->
<--size:8.965999603271484-->• Aspects for the target AVM (see below);<--size:8.965999603271484-->
<--size:8.965999603271484-->• Custom selection of options and interactions to change;<--size:8.965999603271484-->
<--size:8.965999603271484-->• Internal parameters for tailoring the optimization process.<--size:8.965999603271484-->
<--size:8.934563636779785-->Aspects describe relations between source and target AVM, in par-<--size:8.934563636779785-->
<--size:8.965999603271484-->ticular:<--size:8.965999603271484-->
<--size:8.965999603271484-->VaMoS ’20, February 5–7, 2020, Magdeburg, Germany<--size:8.965999603271484-->
<--size:8.965999603271484-->Johannes Dorn, Sven Apel, and Norbert Siegmund<--size:8.965999603271484-->
<--size:7.33477258682251-->Loki<--size:7.33477258682251-->
<--size:7.33477258682251-->Thor<--size:7.33477258682251-->
<--size:6.11230993270874-->Initial Modiﬁed AVMs<--size:6.11230993270874-->
<--size:6.11230993270874-->Initial Modiﬁed AVMs<--size:6.11230993270874-->
<--size:6.11230993270874-->Initial Modiﬁed AVMs<--size:6.11230993270874-->
<--size:6.11230993270874-->Initial Modiﬁed AVMs<--size:6.11230993270874-->
<--size:6.11230993270874-->Initial Modiﬁed AVMs<--size:6.11230993270874-->
<--size:6.11230993270874-->Candidate AVMs for GA<--size:6.11230993270874-->
<--bold--><--size:7.33477258682251-->1 2<--size:7.33477258682251--><--bold-->
<--bold--><--size:7.33477258682251-->3 0<--size:7.33477258682251--><--bold-->
<--size:6.11230993270874-->Target AVM<--size:6.11230993270874-->
<--size:7.33477258682251-->Python API<--size:7.33477258682251-->
<--size:7.33477258682251-->Analyses<--size:7.33477258682251-->
<--size:6.11230993270874-->Source AVM<--size:6.11230993270874-->
<--size:6.11230993270874-->Larger Source AVM<--size:6.11230993270874-->
<--size:7.33477258682251-->User-deﬁned<--size:7.33477258682251-->
<--size:7.33477258682251-->Aspects<--size:7.33477258682251-->
<--size:8.965999603271484-->Figure 2: Workflow of Loki: 0 optional AVM extension with Loki’s implementation of Thor, 1 directly apply aspects to<--size:8.965999603271484-->
<--size:8.965999603271484-->build initial population for the genetic algorithm, 2 optimize all aspects with the genetic algorithm, 3 chose final AVM(s).<--size:8.965999603271484-->
<--size:8.965999603271484-->• The level of Gaussian noise to be applied;<--size:8.965999603271484-->
<--size:8.965999603271484-->• The linear transformation or<--size:8.965999603271484-->
<--size:8.965999603271484-->• Exponential transformation of attribute values to be applied;<--size:8.965999603271484-->
<--size:8.965999603271484-->• The similarity of option, interaction, and variant attribute<--size:8.965999603271484-->
<--size:8.94355583190918-->values for source and target AVM given a desired similarity<--size:8.94355583190918-->
<--size:8.965999603271484-->metric;<--size:8.965999603271484-->
<--size:8.965999603271484-->• The target degree of similarity ranging 0 − 1.<--size:8.965999603271484-->
<--size:8.965999603271484-->The user can chose from the following similarity metrics:<--size:8.965999603271484-->
<--size:8.965999603271484-->• Pearson’s r [23]<--size:8.965999603271484-->
<--size:8.965999603271484-->• Spearman rank-order correlation [22]<--size:8.965999603271484-->
<--size:8.965999603271484-->• Euclidean Distance<--size:8.965999603271484-->
<--size:8.965999603271484-->• Kullback-Leibler-Divergence [14]<--size:8.965999603271484-->
<--size:8.965999603271484-->• Kolmogorov–Smirnov test statistic [3]<--size:8.965999603271484-->
<--size:9.055215835571289-->NSGA-II — the GA that we use — additionally has configurable<--size:9.055215835571289-->
<--size:8.979438781738281-->parameters. Loki exposes these parameters for expert users; how-<--size:8.979438781738281-->
<--size:8.94355583190918-->ever, these parameters can stay default. The configurable NSGA-II<--size:8.94355583190918-->
<--size:8.965999603271484-->parameters are the following:<--size:8.965999603271484-->
<--size:8.965999603271484-->• Population size<--size:8.965999603271484-->
<--size:8.965999603271484-->• Budget<--size:8.965999603271484-->
<--size:8.965999603271484-->• Cross-over strategy<--size:8.965999603271484-->
<--size:8.965999603271484-->• Number of reference variants used to assess fitness functions<--size:8.965999603271484-->
<--size:8.965999603271484-->• Sampling strategy for reference variants<--size:8.965999603271484-->
<--size:8.875886917114258-->We implemented genetic operators to change the source AVM in<--size:8.875886917114258-->
<--size:8.934563636779785-->way that we meet all user specifications. Also, some specifications<--size:8.934563636779785-->
<--size:8.880414009094238-->represent configuration options for the optimization process. Since<--size:8.880414009094238-->
<--size:9.055215835571289-->some specifications might provide a tradeoff or have conflicting<--size:9.055215835571289-->
<--size:8.875886917114258-->effects on the attribute values, we need to optimize for all objectives<--size:8.875886917114258-->
<--size:9.055215835571289-->(i.e., specifications) using a weighted fitness function. By default,<--size:9.055215835571289-->
<--size:8.875886917114258-->the GA applies the operators in the order they are listed. If another<--size:8.875886917114258-->
<--size:8.875886917114258-->order is desired, Loki needs to be executed once per individual user<--size:8.875886917114258-->
<--size:8.957029342651367-->specification. By repeatedly applying the genetic operators on the<--size:8.957029342651367-->
<--size:8.884939193725586-->source AVM, we obtain a set of candidate AVMs. Next, we describe<--size:8.884939193725586-->
<--size:8.965999603271484-->the genetic operators.<--size:8.965999603271484-->
<--size:9.055215835571289-->Formalization. We represent an AVM as a set of options and<--size:9.055215835571289-->
<--size:9.055215835571289-->interactions: O where option o ∈O . Without loss of generality,<--size:9.055215835571289-->
<--size:9.055215835571289-->we model interactions similar to options. The only difference is<--size:9.055215835571289-->
<--size:9.019635200500488-->that an interaction cannot be set by a user or sampling approach;<--size:9.019635200500488-->
<--size:8.875886917114258-->instead, it is selected iff all options that the interaction corresponds<--size:8.875886917114258-->
<--size:9.055215835571289-->to are selected. Furthermore, function α : O → R maps for a<--size:9.055215835571289-->
<--size:9.055215835571289-->given configuration option or interaction to the corresponding<--size:9.055215835571289-->
<--size:9.055215835571289-->attribute value represented as a real number. In other words, α<--size:9.055215835571289-->
<--size:8.992857933044434-->returns for option o the coefficient α o . We further define function<--size:8.992857933044434-->
<--size:8.965999603271484-->Π : P (C) → R |C| which enumerates the set of all configurations<--size:8.965999603271484-->
<--size:9.055215835571289-->and produces a real-valued set containing the attribute values of<--size:9.055215835571289-->
<--size:8.965999603271484-->all variants. Internally, we compute it as follows:<--size:8.965999603271484-->
<--size:8.965999603271484-->Π (C) = { π ( c ) | c ∈C}<--size:8.965999603271484-->
<--size:8.965999603271484-->(2)<--size:8.965999603271484-->
<--size:9.055215835571289-->In general, each operator has a user-specified per-option and<--size:9.055215835571289-->
<--size:9.055215835571289-->per-interaction probability p to be applied. This way, we realize<--size:9.055215835571289-->
<--size:9.050776481628418-->a stochastic and iterative process to change an AVM, making the<--size:9.050776481628418-->
<--size:8.92556095123291-->final model less deterministic and posing a challenge for the learn-<--size:8.92556095123291-->
<--size:9.050776481628418-->ing technique. Moreover, we can combine this probability with a<--size:9.050776481628418-->
<--size:8.965999603271484-->filter such that the following specification is possible: Apply large<--size:8.965999603271484-->
<--size:9.055215835571289-->noise (50%) randomly to 20% of the 50% most-influential options<--size:9.055215835571289-->
<--size:9.055215835571289-->and interactions. Options and interactions with higher absolute<--size:9.055215835571289-->
<--size:9.015177726745605-->coefficient value α o are considered to be more influential with an<--size:9.015177726745605-->
<--size:8.965999603271484-->AVM according to Equation 1.<--size:8.965999603271484-->
<--size:8.875886917114258-->Transformation Operator. Users may specify a linear or exponen-<--size:8.875886917114258-->
<--size:8.93006420135498-->tial transformation of attribute values from the source AVM to the<--size:8.93006420135498-->
<--size:8.907529830932617-->target AVM. That is, we change the attribute values of options and<--size:8.907529830932617-->
<--size:9.055215835571289-->interactions to meet the specified transformation. The factor λ is<--size:9.055215835571289-->
<--size:8.965999603271484-->multiplied with each attribute value α ( o ) s to yield α ( o ) t :<--size:8.965999603271484-->
<--size:8.965999603271484-->∀ o ∈O ′ : α ( o ) t = λ · α ( o ) s ,<--size:8.965999603271484-->
<--size:8.965999603271484-->(3)<--size:8.965999603271484-->
<--size:8.965999603271484-->Generating Attributed Variability Models for Transfer Learning<--size:8.965999603271484-->
<--size:8.965999603271484-->VaMoS ’20, February 5–7, 2020, Magdeburg, Germany<--size:8.965999603271484-->
<--size:9.055215835571289-->where O ′ ⊆O . The user specifies whether all options’ values<--size:9.055215835571289-->
<--size:8.916550636291504-->should be transformed O ′ = O or only the top p % most-influential<--size:8.916550636291504-->
<--size:8.965999603271484-->options and interactions O ′ ⊂O .<--size:8.965999603271484-->
<--size:9.028543472290039-->Recall that linear correlation is one of the aspects of a realistic<--size:9.028543472290039-->
<--size:9.050776481628418-->environment change [ 10 ]. Since linear transformation causes lin-<--size:9.050776481628418-->
<--size:9.055215835571289-->ear correlation, this specification is useful to evaluate whether a<--size:9.055215835571289-->
<--size:9.001791954040527-->transfer-learning approach harnesses this aspect. Conversely, the<--size:9.001791954040527-->
<--size:8.952540397644043-->user can specify an exponential mapping. In this case, λ is used as<--size:8.952540397644043-->
<--size:8.965999603271484-->exponent for α ( o ) s to compute a new α ( o ) t :<--size:8.965999603271484-->
<--size:8.965999603271484-->∀ o ∈O : α ( o ) t = α ( o ) λ<--size:8.965999603271484-->
s (4)
<--size:9.055215835571289-->Since an exponential relationship has not been observed to be<--size:9.055215835571289-->
<--size:8.98391342163086-->an aspect of realistic environment changes, this transformation is<--size:8.98391342163086-->
<--size:8.965999603271484-->useful to challenge a transfer-learning technique.<--size:8.965999603271484-->
<--size:8.875886917114258-->Gaussian-Noise Operator. Jamshidi et al . observed different types<--size:8.875886917114258-->
<--size:8.875886917114258-->of attribute-value correlations for source and target AVMs, although<--size:8.875886917114258-->
<--size:9.055215835571289-->none were perfect correlations [ 10 ]. To mimic imperfect correla-<--size:9.055215835571289-->
<--size:8.992857933044434-->tions, Loki’s supports the application of noise on attribute values<--size:8.992857933044434-->
<--size:8.939061164855957-->of the source model. The operator for Gaussian noise relies on the<--size:8.939061164855957-->
<--size:9.015177726745605-->bell-shaped Gaussian distribution N( µ , σ 2 ) . It is centered around<--size:9.015177726745605-->
<--size:9.055215835571289-->its mean µ , while its shape is defined by its standard deviation σ .<--size:9.055215835571289-->
<--size:8.939061164855957-->Loki’s Gaussian-noise operator is configured with a relative noise<--size:8.939061164855957-->
<--size:8.965999603271484-->σ rel and draws a new value for each attribute value according to<--size:8.965999603271484-->
<--size:8.884939193725586-->Equation 5. Thus, larger values are likely to be changed by a larger<--size:8.884939193725586-->
<--size:8.965999603271484-->margin.<--size:8.965999603271484-->
<--size:8.965999603271484-->α ( o ) t = N( α ( o ) s , ( σ rel · α ( o ) s ) 2 )<--size:8.965999603271484-->
<--size:8.965999603271484-->(5)<--size:8.965999603271484-->
<--size:10.909000396728516-->3.3<--size:10.909000396728516-->
<--size:10.909000396728516-->Optimizing Aspects<--size:10.909000396728516-->
<--size:9.024090766906738-->As described before, the user can specify different aspects for the<--size:9.024090766906738-->
<--size:9.055215835571289-->new AVM to be met, such as transformations, similarities of dis-<--size:9.055215835571289-->
<--size:8.974961280822754-->tributions, noise, and so on. As these aspects might be conflicting,<--size:8.974961280822754-->
<--size:9.055215835571289-->there is no single AVM that meets all specifications, but a range<--size:9.055215835571289-->
<--size:9.055215835571289-->of AVMs that favor some aspects over others. Hence, we define<--size:9.055215835571289-->
<--size:8.880414009094238-->the search for a target AVM as a multi-objective optimization prob-<--size:8.880414009094238-->
<--size:9.055215835571289-->lem containing one objective per specification. In the following,<--size:9.055215835571289-->
<--size:9.055215835571289-->we abbreviate the source and target AVM with AVM s and AVM t ,<--size:9.055215835571289-->
<--size:9.055215835571289-->respectively and formalize the optimization using the following<--size:9.055215835571289-->
<--size:8.965999603271484-->fitness function:<--size:8.965999603271484-->
<--size:8.965999603271484-->max f ( AVM s , AVM t ) = β 0 · f 1 ( AVM s , AVM t )<--size:8.965999603271484-->
<--size:8.965999603271484-->+ β 1 · f 2 ( AVM s , AVM t )<--size:8.965999603271484-->
<--size:8.965999603271484-->+ . . .<--size:8.965999603271484-->
<--size:8.965999603271484-->+ β n · f n ( AVM s , AVM t ) ,<--size:8.965999603271484-->
<--size:8.965999603271484-->(6)<--size:8.965999603271484-->
<--size:8.875886917114258-->where f 1 . . . f n represent individual fitness functions that assess<--size:8.875886917114258-->
<--size:8.875886917114258-->the fitness of an individual aspect and β represents the user-defined<--size:8.875886917114258-->
<--size:8.965999603271484-->weighting of the individual aspects.<--size:8.965999603271484-->
<--size:8.875886917114258-->We use the genetic algorithm NSGA-II [ 4 ] to solve this optimiza-<--size:8.875886917114258-->
<--size:9.055215835571289-->tion problem. It starts with the initial population of step 1 and<--size:9.055215835571289-->
<--size:9.032994270324707-->iteratively applies genetic operators to combine candidate AVMs,<--size:9.032994270324707-->
<--size:8.875886917114258-->assesses the combined AVMs by means of our fitness functions, and<--size:8.875886917114258-->
<--size:9.055215835571289-->finally selects the best candidates for the next iteration. We stop<--size:9.055215835571289-->
<--size:9.055215835571289-->the GA after a given budget of iterations or after three iterations<--size:9.055215835571289-->
<--size:8.875886917114258-->without improvement. To exemplify how we compute an individual<--size:8.875886917114258-->
<--size:9.024090766906738-->fitness score, we explain the computations of the fitness function<--size:9.024090766906738-->
<--size:8.965999603271484-->for Gaussian noise and the distribution similarity.<--size:8.965999603271484-->
<--size:8.934563636779785-->Gaussian-Noise Fitness. We assess how well the Gaussian-Noise<--size:8.934563636779785-->
<--size:9.055215835571289-->specification is fulfilled by comparing the relative option and in-<--size:9.055215835571289-->
<--size:8.965999603271484-->teraction attribute value differences between the source AVM and<--size:8.965999603271484-->
<--size:9.055215835571289-->the respective candidate AVM against the Gaussian distribution<--size:9.055215835571289-->
<--size:8.965999603271484-->specified by the user with Pearson’s r [23]:<--size:8.965999603271484-->
<--size:8.965999603271484-->∆ α = {( α target<--size:8.965999603271484-->
i − α source
i )/ α source
i | α i ∈ AVM s }
<--size:8.965999603271484-->(7)<--size:8.965999603271484-->
<--size:8.965999603271484-->f noise = ρ ( ∆ α , N)<--size:8.965999603271484-->
<--size:8.965999603271484-->(8)<--size:8.965999603271484-->
<--size:8.974961280822754-->Distribution-Similarity Fitness. Loki is the first tool in this field<--size:8.974961280822754-->
<--size:9.032994270324707-->that allows for the specification and evaluation of multiple distri-<--size:9.032994270324707-->
<--size:9.055215835571289-->bution distance metrics, such as (1) Pearson’s correlation coeffi-<--size:9.055215835571289-->
<--size:8.898500442504883-->cient to ensure linear correlation [ 23 ], (2) Spearman’s rank correla-<--size:8.898500442504883-->
<--size:9.055215835571289-->tion coefficient for high rank correlation, (3) Euclidean Distance,<--size:9.055215835571289-->
<--size:8.875886917114258-->(4) Kolmogorov–Smirnov test statistic, and (5) the Kullback-Leibler-<--size:8.875886917114258-->
<--size:8.884939193725586-->Divergence to keep option interaction, and variant attribute values<--size:8.884939193725586-->
<--size:9.055215835571289-->of the target model similar to the source model [ 11 ]. If the user<--size:9.055215835571289-->
<--size:9.01071834564209-->chooses multiple distance metrics, their average is used as the fit-<--size:9.01071834564209-->
<--size:8.965999603271484-->ness value.<--size:8.965999603271484-->
<--size:9.037443161010742-->We use as a fitness function f similarity the user-defined similar-<--size:9.037443161010742-->
<--size:9.055215835571289-->ity metric. For instance, when using Pearson’s correlation coeffi-<--size:9.055215835571289-->
<--size:8.965999603271484-->cient [23], the fitness function is as follows:<--size:8.965999603271484-->
<--size:8.965999603271484-->f similarity =<--size:8.965999603271484-->
<--size:8.965999603271484-->cov ( Π ( C AVM s ) , Π ( C AVM t ))<--size:8.965999603271484-->
<--size:8.965999603271484-->σ ( Π ( C AVM s )) · σ ( Π ( C AVM t )) ,<--size:8.965999603271484-->
<--size:8.965999603271484-->(9)<--size:8.965999603271484-->
<--size:9.055215835571289-->where cov is the covariance between the variant attribute values<--size:9.055215835571289-->
<--size:8.965999603271484-->Π (C) of the source and the target AVM, and σ the standard deviation<--size:8.965999603271484-->
<--size:8.965999603271484-->of the attribute values of all variants of the respective AVM.<--size:8.965999603271484-->
<--size:10.909000396728516-->4 EVALUATION: APPLICABILITY<--size:10.909000396728516-->
<--size:8.875886917114258-->The applicability of Loki depends on whether we can mimic actual<--size:8.875886917114258-->
<--size:9.055215835571289-->changes in environments. That is, we need to evaluate whether<--size:9.055215835571289-->
<--size:9.055215835571289-->a generated target AVM assembles changes to be useful in the<--size:9.055215835571289-->
<--size:9.046334266662598-->evaluation of a transfer-learning technique. To this end, we state<--size:9.046334266662598-->
<--size:8.965999603271484-->the following question:<--size:8.965999603271484-->
<--size:8.965999603271484-->RQ: With increasing user-defined degree of change, does Loki<--size:8.965999603271484-->
<--size:8.875886917114258-->synthesize target AVMs that are harder to infer with transfer<--size:8.875886917114258-->
<--size:8.965999603271484-->learning?<--size:8.965999603271484-->
<--size:9.006256103515625-->The rationale for this research question is that small changes and<--size:9.006256103515625-->
<--size:8.875886917114258-->closely related target AVMs should be easy to learn with a transfer-<--size:8.875886917114258-->
<--size:8.961515426635742-->learning approach than larger changes. If we observe better learn-<--size:8.961515426635742-->
<--size:8.98391342163086-->ability of transfer learning for AVMs that are supposed to be very<--size:8.98391342163086-->
<--size:9.041889190673828-->similar than those that are supposed to be different, we conclude<--size:9.041889190673828-->
<--size:8.912040710449219-->that Loki is able to generate models according to the given specifi-<--size:8.912040710449219-->
<--size:8.965999603271484-->cation.<--size:8.965999603271484-->
<--size:10.909000396728516-->4.1<--size:10.909000396728516-->
<--size:10.909000396728516-->Setup<--size:10.909000396728516-->
<--size:8.875886917114258-->To answer our research question, we synthesize several target AVMs<--size:8.875886917114258-->
<--size:8.94355583190918-->with varying degree of change, based on source AVMs of different<--size:8.94355583190918-->
<--size:9.055215835571289-->real-world software systems. With the resulting pairs of source<--size:9.055215835571289-->
<--size:8.92556095123291-->and target AVMs, we can then run the transfer-learning technique<--size:8.92556095123291-->
<--size:8.934563636779785-->L2S [ 11 ] to obtain an estimate for the target AVMs. Then, we com-<--size:8.934563636779785-->
<--size:9.055215835571289-->pare the attribute values of options and interactions of the true<--size:9.055215835571289-->
<--size:8.875886917114258-->and estimated target AVM as described below. Overall, we examine<--size:8.875886917114258-->
<--size:8.965999603271484-->VaMoS ’20, February 5–7, 2020, Magdeburg, Germany<--size:8.965999603271484-->
<--size:8.965999603271484-->Johannes Dorn, Sven Apel, and Norbert Siegmund<--size:8.965999603271484-->
5% 20%
<--size:6.100709915161133-->50%<--size:6.100709915161133-->
<--size:6.100709915161133-->100%<--size:6.100709915161133-->
<--size:6.655320167541504-->Relative noise<--size:6.655320167541504-->
<--size:6.100709915161133-->0 5<--size:6.100709915161133-->
<--size:6.100709915161133-->10 15<--size:6.100709915161133-->
<--size:6.100709915161133-->20 25<--size:6.100709915161133-->
<--size:6.655320167541504-->MAPE<--size:6.655320167541504-->
<--size:6.655320167541504-->x264<--size:6.655320167541504-->
5% 20%
<--size:6.100709915161133-->50%<--size:6.100709915161133-->
<--size:6.100709915161133-->100%<--size:6.100709915161133-->
<--size:6.655320167541504-->Relative noise<--size:6.655320167541504-->
<--size:6.100709915161133-->0 100<--size:6.100709915161133-->
<--size:6.100709915161133-->200<--size:6.100709915161133-->
<--size:6.100709915161133-->300<--size:6.100709915161133-->
<--size:6.100709915161133-->400<--size:6.100709915161133-->
<--size:6.655320167541504-->Berkeley DB (C)<--size:6.655320167541504-->
5% 20%
<--size:6.100709915161133-->50%<--size:6.100709915161133-->
<--size:6.100709915161133-->100%<--size:6.100709915161133-->
<--size:6.655320167541504-->Relative noise<--size:6.655320167541504-->
<--size:6.100709915161133-->0 2<--size:6.100709915161133-->
<--size:6.100709915161133-->4 6<--size:6.100709915161133-->
<--size:6.655320167541504-->8 LLVM<--size:6.655320167541504-->
<--size:8.965999603271484-->Figure 3: Mean absolute percentage error (MAPE) of learning-to-sample AVMs built on target AVMs with different noise levels.<--size:8.965999603271484-->
<--size:9.055215835571289-->whether the estimated AVMs are more accurate for target AVMs<--size:9.055215835571289-->
<--size:9.055215835571289-->that were specified to be similar to the source AVMs than AVMs<--size:9.055215835571289-->
<--size:8.979438781738281-->with a higher degree of change. In the following, we will describe<--size:8.979438781738281-->
<--size:8.965999603271484-->our experiment in more detail.<--size:8.965999603271484-->
<--size:9.006256103515625-->There are several ways to specify degrees of change with Loki,<--size:9.006256103515625-->
<--size:9.055215835571289-->as described in Section 3. Here, we opted for specifying different<--size:9.055215835571289-->
<--size:9.055215835571289-->degrees of noise to specify the degree of change and keep other<--size:9.055215835571289-->
<--size:9.055215835571289-->aspects constant. The rationale is that the noise can be highly<--size:9.055215835571289-->
<--size:9.055215835571289-->controlled in our experiment and is simultaneously a stochastic<--size:9.055215835571289-->
<--size:9.055215835571289-->operator. Moreover, noise can mimic also linear transfers (as we<--size:9.055215835571289-->
<--size:9.055215835571289-->also observed during our experiments). The parameters were as<--size:9.055215835571289-->
<--size:8.965999603271484-->follows:<--size:8.965999603271484-->
<--size:8.965999603271484-->• Loki changes attribute values of the top 50% influential op-<--size:8.965999603271484-->
<--size:8.965999603271484-->tions and interactions and<--size:8.965999603271484-->
<--size:8.965999603271484-->• maximizes Person’s correlation coefficient for variant, op-<--size:8.965999603271484-->
<--size:8.875886917114258-->tion, and interaction attribute values of the source and target<--size:8.875886917114258-->
<--size:8.965999603271484-->AVM.<--size:8.965999603271484-->
<--size:9.055215835571289-->We choose 5%, 20%, 50%, and 100% as relative noise levels for<--size:9.055215835571289-->
<--size:8.965999603271484-->σ rel in the noise specification for each target attribute value, which<--size:8.965999603271484-->
<--size:9.046334266662598-->is computed according to Equation 5. Note that this equality will<--size:9.046334266662598-->
<--size:8.875886917114258-->not strictly hold for every option and interaction in the target AVM,<--size:8.875886917114258-->
<--size:8.965999603271484-->because we also account for the constant aspects.<--size:8.965999603271484-->
<--size:9.055215835571289-->As source AVMs, we rely on attribute measurements for each<--size:9.055215835571289-->
<--size:8.965999603271484-->possible variant of three real-world software systems:<--size:8.965999603271484-->
<--size:8.965999603271484-->Berkeley DB (C) is an embedded database engine. The response<--size:8.965999603271484-->
<--size:8.98391342163086-->time as attribute in the source AVM has been measured for<--size:8.98391342163086-->
<--size:8.965999603271484-->different read and write queries.<--size:8.965999603271484-->
<--size:8.965999603271484-->X 264 is a video encoder for the H.264 compression format. From<--size:8.965999603271484-->
<--size:8.92556095123291-->different metrics that were recorded during the encoding of<--size:8.92556095123291-->
<--size:8.965999603271484-->a video clip, we use energy consumption as the attribute.<--size:8.965999603271484-->
<--size:8.965999603271484-->LLVM is a compiler infrastructure. With a compiling benchmark<--size:8.965999603271484-->
<--size:9.055215835571289-->as workload, we use energy consumption as the attribute<--size:9.055215835571289-->
<--size:8.965999603271484-->from several recorded metrics.<--size:8.965999603271484-->
<--size:9.055215835571289-->We refer to Kaltenecker et al . for details on the actual measure-<--size:9.055215835571289-->
<--size:8.903016090393066-->ments [ 13 ]. For each subject system, we obtain a source AVM with<--size:8.903016090393066-->
<--size:9.055215835571289-->Lasso regression using all available measurements. We use Loki<--size:9.055215835571289-->
<--size:9.055215835571289-->to synthesize target AVMs with each noise level, for each source<--size:9.055215835571289-->
<--size:9.055215835571289-->AVM. Finally, we run L2S on each pair of source and target AVM<--size:9.055215835571289-->
<--size:8.965999603271484-->Table 1: Mean absolute percentage error (MAPE) of AVMs<--size:8.965999603271484-->
<--size:8.965999603271484-->built with L2S on different system’s target AVMs with differ-<--size:8.965999603271484-->
<--size:8.965999603271484-->ent noise levels.<--size:8.965999603271484-->
<--size:8.965999603271484-->Noise<--size:8.965999603271484-->
5% 20%
<--size:8.965999603271484-->50%<--size:8.965999603271484-->
<--size:8.965999603271484-->100%<--size:8.965999603271484-->
<--size:8.965999603271484-->Berkeley DB (C)<--size:8.965999603271484-->
<--size:8.965999603271484-->421.3<--size:8.965999603271484-->
<--size:8.965999603271484-->100.3<--size:8.965999603271484-->
<--size:8.965999603271484-->126.0<--size:8.965999603271484-->
<--size:8.965999603271484-->115.7<--size:8.965999603271484-->
<--size:8.965999603271484-->LLVM<--size:8.965999603271484-->
<--size:8.965999603271484-->6.8<--size:8.965999603271484-->
<--size:8.965999603271484-->6.9<--size:8.965999603271484-->
<--size:8.965999603271484-->8.2<--size:8.965999603271484-->
<--size:8.965999603271484-->5.1<--size:8.965999603271484-->
<--size:8.965999603271484-->x264<--size:8.965999603271484-->
<--size:8.965999603271484-->5.9<--size:8.965999603271484-->
<--size:8.965999603271484-->6.9<--size:8.965999603271484-->
<--size:8.965999603271484-->14.2<--size:8.965999603271484-->
<--size:8.965999603271484-->19.8<--size:8.965999603271484-->
<--size:9.055215835571289-->and compute the accuracy for the resulting L2S AVMs [ 11 ]. The<--size:9.055215835571289-->
<--size:9.055215835571289-->accuracy computation is provided by L2S; it computes the mean<--size:9.055215835571289-->
<--size:8.875886917114258-->absolute percentage error (MAPE) for its evaluation set of n variants<--size:8.875886917114258-->
<--size:8.965999603271484-->with Equation 10:<--size:8.965999603271484-->
<--size:8.965999603271484-->MAPE = 100%<--size:8.965999603271484-->
n n
Õ i = 1
<--size:8.965999603271484-->Π ( C i<--size:8.965999603271484-->
<--size:7.2729997634887695-->AVM t ) − Π ( C i<--size:7.2729997634887695-->
<--size:7.2729997634887695-->L2S )<--size:7.2729997634887695-->
<--size:8.965999603271484-->Π ( C i<--size:8.965999603271484-->
<--size:7.2729997634887695-->AVM t )<--size:7.2729997634887695-->
<--size:8.965999603271484-->(10)<--size:8.965999603271484-->
<--size:8.965999603271484-->Π ( C i<--size:8.965999603271484-->
<--size:7.2729997634887695-->AVM t ) is the measured variant attribute value of the target AVM,<--size:7.2729997634887695-->
<--size:9.055215835571289-->whereas the prediction of the L2S AVM is given as Π ( C i<--size:9.055215835571289-->
<--size:7.2729997634887695-->L2S ) . We<--size:7.2729997634887695-->
<--size:9.019635200500488-->repeated each run three times and report the result MAPE as box<--size:9.019635200500488-->
<--size:8.965999603271484-->plots.<--size:8.965999603271484-->
<--size:10.909000396728516-->4.2<--size:10.909000396728516-->
<--size:10.909000396728516-->Results<--size:10.909000396728516-->
<--size:9.055215835571289-->We report the median MAPE values for the three repetitions in<--size:9.055215835571289-->
<--size:8.875886917114258-->Table 1 and visualize all MAPE values in Figure 3. Note that we use<--size:8.875886917114258-->
<--size:9.041889190673828-->a training set size of only 5 configurations with which L2S infers<--size:9.041889190673828-->
<--size:9.055215835571289-->a new AVM from a given model. This small number of training<--size:9.055215835571289-->
<--size:8.875886917114258-->configurations led to acceptable results in prior tests and resembles<--size:8.875886917114258-->
<--size:8.875886917114258-->a case of scarce data in the source environment. In what follows, we<--size:8.875886917114258-->
<--size:8.875886917114258-->describe the results per subject system. For x264, the error increases<--size:8.875886917114258-->
<--size:8.884939193725586-->with higher relative noise. A relative noise of 5% results in an easy-<--size:8.884939193725586-->
<--size:8.884939193725586-->to-learn model. As expected, increasing noise (again, stochastically<--size:8.884939193725586-->
<--size:8.875886917114258-->distributed among options and interactions) increases the difficulty<--size:8.875886917114258-->
<--size:9.032994270324707-->to infer new attribute values that correspond to the ground truth<--size:9.032994270324707-->
<--size:8.965999603271484-->(an increasing error of up to 20%).<--size:8.965999603271484-->
<--size:8.965999603271484-->Generating Attributed Variability Models for Transfer Learning<--size:8.965999603271484-->
<--size:8.965999603271484-->VaMoS ’20, February 5–7, 2020, Magdeburg, Germany<--size:8.965999603271484-->
<--size:7.320808410644531-->1500<--size:7.320808410644531-->
<--size:7.320808410644531-->2000<--size:7.320808410644531-->
<--size:7.320808410644531-->2500<--size:7.320808410644531-->
<--size:7.320808410644531-->3000<--size:7.320808410644531-->
<--size:7.320808410644531-->0.0000<--size:7.320808410644531-->
<--size:7.320808410644531-->0.0005<--size:7.320808410644531-->
<--size:7.320808410644531-->0.0010<--size:7.320808410644531-->
<--size:7.320808410644531-->0.0015<--size:7.320808410644531-->
<--size:7.9863362312316895-->Frequency<--size:7.9863362312316895-->
<--size:7.320808410644531-->Source AVM<--size:7.320808410644531-->
<--size:7.320808410644531-->Target AVM<--size:7.320808410644531-->
<--size:7.320808410644531-->1500<--size:7.320808410644531-->
<--size:7.320808410644531-->2000<--size:7.320808410644531-->
<--size:7.320808410644531-->2500<--size:7.320808410644531-->
<--size:7.320808410644531-->1500<--size:7.320808410644531-->
<--size:7.320808410644531-->2000<--size:7.320808410644531-->
<--size:7.320808410644531-->2500<--size:7.320808410644531-->
<--size:7.320808410644531-->3000<--size:7.320808410644531-->
<--size:8.965999603271484-->Figure 4: Variant attribute distributions of a source and a target AVM of LLVM with 20% relative noise.<--size:8.965999603271484-->
<--size:9.019635200500488-->The picture is different for Berkeley DB (C): for 5% noise, L2S<--size:9.019635200500488-->
<--size:9.055215835571289-->results in an error rate of up to 400%. For the other noise levels,<--size:9.055215835571289-->
<--size:9.055215835571289-->the error is still high. Although these results seem odd at first,<--size:9.055215835571289-->
<--size:9.055215835571289-->they make actually sense when considering the low training set<--size:9.055215835571289-->
<--size:8.92556095123291-->size of 5 configurations and the usage of three repetitions. We will<--size:8.92556095123291-->
<--size:9.055215835571289-->discuss this aspect in more detail in Section 4.3. For LLVM, L2S<--size:9.055215835571289-->
<--size:8.893982887268066-->achieves similar error rates for all noise levels; it has no difficulties<--size:8.893982887268066-->
<--size:8.965999603271484-->in learning new attribute values.<--size:8.965999603271484-->
<--size:10.909000396728516-->4.3<--size:10.909000396728516-->
<--size:10.909000396728516-->Discussion<--size:10.909000396728516-->
<--size:9.055215835571289-->Berkeley DB (C). We obtained the highest MAPE for Berke-<--size:9.055215835571289-->
<--size:9.055215835571289-->ley DB (C) and error rates of up to 400% even when adding only<--size:9.055215835571289-->
<--size:9.055215835571289-->5% noise. We hypothesize that the low number of configurations<--size:9.055215835571289-->
<--size:8.997325897216797-->as input for L2S might be a reason for the problems of inferring a<--size:8.997325897216797-->
<--size:8.903016090393066-->correct model. Also, the variance in the selection of configurations<--size:8.903016090393066-->
<--size:9.006256103515625-->for the training set seems to be more influential for the error rate<--size:9.006256103515625-->
<--size:9.055215835571289-->than the noise level. To verify our assumptions, we conducted a<--size:9.055215835571289-->
<--size:8.992857933044434-->second experiment, in which we increased the training set size to<--size:8.992857933044434-->
<--size:8.875886917114258-->ten, which still are less than 0 . 4% of all valid configurations. Figure 5<--size:8.875886917114258-->
<--size:8.903016090393066-->shows the results. The error substantially decreased to a minimum<--size:8.903016090393066-->
<--size:9.032994270324707-->of 5% for the 5% noise case. Also, the MAPE shows now a similar<--size:9.032994270324707-->
<--size:8.965999603271484-->behavior as for x264.<--size:8.965999603271484-->
<--size:8.93006420135498-->x264. The results for x264 clearly meet our expectations in that<--size:8.93006420135498-->
<--size:9.032994270324707-->a larger change to the source AVM results in more difficulties for<--size:9.032994270324707-->
<--size:8.903016090393066-->learning a target AVM. The rationale is that L2S would need to see<--size:8.903016090393066-->
<--size:8.875886917114258-->more configurations to account for the severe changes and that less<--size:8.875886917114258-->
<--size:8.875886917114258-->information could be transferred from the source to the target. Here<--size:8.875886917114258-->
<--size:9.055215835571289-->lies the power of Loki: We can perform fine-grained analysis to<--size:9.055215835571289-->
<--size:8.957029342651367-->which degree information translation is possible for AVMs, which<--size:8.957029342651367-->
<--size:8.92556095123291-->aspects are easier to transfer than others, and what configurations<--size:8.92556095123291-->
<--size:8.875886917114258-->should be selected first by the transfer-learning approach to transfer<--size:8.875886917114258-->
<--size:9.055215835571289-->information (i.e., attribute values, distributions, etc.) in the most<--size:9.055215835571289-->
<--size:8.965999603271484-->efficient way.<--size:8.965999603271484-->
<--size:9.055215835571289-->LLVM. Since LLVM’s average MAPE values of all noise levels<--size:9.055215835571289-->
<--size:8.875886917114258-->are similar, we reason that source and target AVMs must be equally<--size:8.875886917114258-->
<--size:9.055215835571289-->closely related for all noise levels. Indeed, reviewing Loki’s out-<--size:9.055215835571289-->
<--size:9.055215835571289-->put plots, we find that the target AVM’s distribution of variant<--size:9.055215835571289-->
<--size:8.875886917114258-->attribute values is usually only a shifted version of the source AVM’<--size:8.875886917114258-->
<--size:9.055215835571289-->distribution. Figure 4 shows source and target variant attribute<--size:9.055215835571289-->
<--size:8.979438781738281-->distributions for one replication for the 20% noise specification. A<--size:8.979438781738281-->
<--size:9.055215835571289-->shift of the distribution of variant attribute values may happen<--size:9.055215835571289-->
<--size:8.934563636779785-->when the option with the highest attribute value overshadows the<--size:8.934563636779785-->
<--size:9.055215835571289-->influence of all other options on the attribute, thus shifting the<--size:9.055215835571289-->
<--size:9.055215835571289-->whole distribution by the extent this option changes. A shift in<--size:9.055215835571289-->
<--size:8.880414009094238-->the distribution of attribute values of variants resembles a scenario<--size:8.880414009094238-->
<--size:9.050776481628418-->that has been found to occur for real-world systems as described<--size:9.050776481628418-->
<--size:9.055215835571289-->in Section 2.1— an indication for Loki’s applicability. Since L2S<--size:9.055215835571289-->
<--size:9.055215835571289-->has been developed to utilize such environment changes, it is no<--size:9.055215835571289-->
<--size:9.055215835571289-->surprise but to be expected that it achieves a high accuracy. This<--size:9.055215835571289-->
<--size:8.965999603271484-->further backs our approach.<--size:8.965999603271484-->
<--size:6.9738898277282715-->5 10<--size:6.9738898277282715-->
<--size:7.607879638671875-->Training set size<--size:7.607879638671875-->
<--size:6.9738898277282715-->0 100<--size:6.9738898277282715-->
<--size:6.9738898277282715-->200<--size:6.9738898277282715-->
<--size:6.9738898277282715-->300<--size:6.9738898277282715-->
<--size:6.9738898277282715-->400<--size:6.9738898277282715-->
<--size:7.607879638671875-->MAPE<--size:7.607879638671875-->
<--size:7.607879638671875-->Berkeley DB (C)<--size:7.607879638671875-->
<--size:6.466697692871094-->Relative noise<--size:6.466697692871094-->
5% 20%
<--size:6.9738898277282715-->50%<--size:6.9738898277282715-->
<--size:6.9738898277282715-->100%<--size:6.9738898277282715-->
<--size:8.965999603271484-->Figure 5: MAPE of L2S AVMs built on Berkeley DB (C) tar-<--size:8.965999603271484-->
<--size:8.965999603271484-->get AVMs with varying noise levels and training set sizes.<--size:8.965999603271484-->
<--size:9.015177726745605-->Our results show that Loki is indeed able to provide a test bed<--size:9.015177726745605-->
<--size:8.898500442504883-->for transfer-learning techniques. We could replicate the error rates<--size:8.898500442504883-->
<--size:8.970481872558594-->for L2S as expected: A higher noise level leads to more difficulties<--size:8.970481872558594-->
<--size:9.055215835571289-->to learn a model. More importantly, we could even determine an<--size:9.055215835571289-->
<--size:9.055215835571289-->optimal training set size for Berkeley DB (C) and demonstrate<--size:9.055215835571289-->
<--size:9.019635200500488-->that the nature of the variability model together with the ratio of<--size:9.019635200500488-->
<--size:9.055215835571289-->attribute values (very small to large values for Berkeley DB (C))<--size:9.055215835571289-->
<--size:8.875886917114258-->can influence the accuracy of transfer-learning techniques. Already<--size:8.875886917114258-->
<--size:8.965999603271484-->VaMoS ’20, February 5–7, 2020, Magdeburg, Germany<--size:8.965999603271484-->
<--size:8.965999603271484-->Johannes Dorn, Sven Apel, and Norbert Siegmund<--size:8.965999603271484-->
<--size:8.965999603271484-->this insight can stimulate new developments in this area. It seems<--size:8.965999603271484-->
<--size:9.055215835571289-->that the training set size should be a function of the distribution<--size:9.055215835571289-->
<--size:9.006256103515625-->and range of attribute values, and thus be determined per subject<--size:9.006256103515625-->
<--size:8.8894624710083-->system. This agrees with the findings of LLVM, where even Gauss-<--size:8.8894624710083-->
<--size:9.055215835571289-->ian changes result in large shifts of the whole value distribution.<--size:9.055215835571289-->
<--size:8.965999603271484-->Hence, we answer our research question positively.<--size:8.965999603271484-->
<--size:10.909000396728516-->5 SUMMARY<--size:10.909000396728516-->
<--size:8.98391342163086-->Transfer learning is a growing research area in the field of config-<--size:8.98391342163086-->
<--size:8.916550636291504-->urable software systems, yet there exists no user-friendly tool that<--size:8.916550636291504-->
<--size:9.055215835571289-->can synthesize target attributed variability models (AVMs) for a<--size:9.055215835571289-->
<--size:9.055215835571289-->given source AVM such that both mimic a realistic environment<--size:9.055215835571289-->
<--size:9.01071834564209-->change. For that reason, we propose Loki, a tool and approach to<--size:9.01071834564209-->
<--size:9.055215835571289-->generate a target AVM such that it allows for simulating several<--size:9.055215835571289-->
<--size:8.965999603271484-->aspects of environment changes at once.<--size:8.965999603271484-->
<--size:9.041889190673828-->Loki provides a user-friendly implementation leveraging a ge-<--size:9.041889190673828-->
<--size:9.055215835571289-->netic algorithm to generate a target AVM according to multiple<--size:9.055215835571289-->
<--size:9.006256103515625-->user-specified criteria. We evaluated Loki by generating multiple<--size:9.006256103515625-->
<--size:8.884939193725586-->new AVMs from AVMs that have been determined from real-world<--size:8.884939193725586-->
<--size:8.875886917114258-->subject systems. We assessed whether the existing transfer-learning<--size:8.875886917114258-->
<--size:8.98391342163086-->approach Learn to Sample is able to infer the new attribute values<--size:8.98391342163086-->
<--size:8.94355583190918-->depending on the degree of change we have performed. We found<--size:8.94355583190918-->
<--size:8.916550636291504-->that the generated models indeed increase complexity for learning<--size:8.916550636291504-->
<--size:9.055215835571289-->the changed attribute values depending on the degree of change.<--size:9.055215835571289-->
<--size:9.055215835571289-->We also found that Loki can be used to find optimal training set<--size:9.055215835571289-->
<--size:8.965999603271484-->sizes for different systems.<--size:8.965999603271484-->
<--size:9.032994270324707-->By making the tool and implementation publicly available, we<--size:9.032994270324707-->
<--size:8.875886917114258-->aim at stimulating new research in this area and reducing the burden<--size:8.875886917114258-->
<--size:8.8894624710083-->of conducting a full-fledged evaluation or parameter adjustment of<--size:8.8894624710083-->
<--size:8.965999603271484-->a learning technique.<--size:8.965999603271484-->
<--size:10.909000396728516-->ACKNOWLEDGMENT<--size:10.909000396728516-->
<--size:9.032994270324707-->Siegmund’s and Apel’s work has been funded by the German Re-<--size:9.032994270324707-->
<--size:8.965999603271484-->search Foundation (SI 2171/3-1, SI 2171/2, and AP 206/11-1).<--size:8.965999603271484-->
<--size:10.909000396728516-->REFERENCES<--size:10.909000396728516-->
<--size:6.973999977111816-->[1] Don Batory. 2005. Feature Models, Grammars, and Propositional Formulas. In<--size:6.973999977111816-->
<--size:7.043394565582275-->Software Product Lines , Henk Obbink and Klaus Pohl (Eds.). Springer Berlin<--size:7.043394565582275-->
<--size:6.973999977111816-->Heidelberg, Berlin, Heidelberg, 7–20. https://doi.org/10.1007/11554844_3<--size:6.973999977111816-->
<--size:6.973999977111816-->[2] Thomas M. Cover and Joy A. Thomas. 2012. Elements of Information Theory . John<--size:6.973999977111816-->
<--size:6.973999977111816-->Wiley & Sons.<--size:6.973999977111816-->
<--size:6.973999977111816-->[3] W.W. Daniel. 1990. Applied Nonparametric Statistics . PWS-Kent Publ.<--size:6.973999977111816-->
<--size:6.973999977111816-->[4] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan. 2002. A Fast and Elitist Multiob-<--size:6.973999977111816-->
<--size:6.903907775878906-->jective Genetic Algorithm: NSGA-II. Trans. Evol. Comp 6, 2 (April 2002), 182–197.<--size:6.903907775878906-->
<--size:6.973999977111816-->https://doi.org/10.1109/4235.996017<--size:6.973999977111816-->
<--size:6.973999977111816-->[5] Jianmei Guo, Krzysztof Czarnecki, Sven Apel, Norbert Siegmund, and Andrzej<--size:6.973999977111816-->
<--size:6.928520679473877-->Wasowski. [n. d.]. Variability-Aware Performance Modeling: A Statistical Learn-<--size:6.928520679473877-->
<--size:6.973999977111816-->ing Approach. 12.<--size:6.973999977111816-->
<--size:6.973999977111816-->[6] Huong Ha and Hongyu Zhang. 2019. DeepPerf: Performance Prediction for<--size:6.973999977111816-->
<--size:7.043394565582275-->Configurable Software with Deep Sparse Neural Network. Proceedings - In-<--size:7.043394565582275-->
<--size:7.043394565582275-->ternational Conference on Software Engineering (May 2019), 1095–1106. https:<--size:7.043394565582275-->
<--size:6.973999977111816-->//doi.org/10.1109/ICSE.2019.00113<--size:6.973999977111816-->
<--size:6.973999977111816-->[7] Christopher Henard, Mike Papadakis, Mark Harman, and Yves Le Traon. 2015.<--size:6.973999977111816-->
<--size:6.903907775878906-->Combining Multi-Objective Search and Constraint Solving for Configuring Large<--size:6.903907775878906-->
<--size:6.9320292472839355-->Software Product Lines. In 2015 IEEE/ACM 37th IEEE International Conference on<--size:6.9320292472839355-->
<--size:6.973999977111816-->Software Engineering . 517–528. https://doi.org/10.1109/ICSE.2015.69<--size:6.973999977111816-->
<--size:6.973999977111816-->[8] Robert M. Hierons, Miqing Li, Xiaohui Liu, Sergio Segura, and Wei Zheng. 2016.<--size:6.973999977111816-->
<--size:7.043394565582275-->SIP: Optimal Product Selection from Feature Models Using Many-Objective<--size:7.043394565582275-->
<--size:7.043394565582275-->Evolutionary Optimization. ACM Transactions on Software Engineering and<--size:7.043394565582275-->
<--size:6.973999977111816-->Methodology 25, 2 (April 2016), 1–39. https://doi.org/10.1145/2897760<--size:6.973999977111816-->
<--size:6.973999977111816-->[9] Md Shahriar Iqbal, Lars Kotthoff, and Pooyan Jamshidi. 2019. Transfer Learning<--size:6.973999977111816-->
<--size:7.015718936920166-->for Performance Modeling of Deep Neural Network Systems. arXiv:1904.02838<--size:7.015718936920166-->
<--size:6.973999977111816-->[cs] (April 2019). arXiv:cs/1904.02838<--size:6.973999977111816-->
<--size:6.973999977111816-->[10] Pooyan Jamshidi, Norbert Siegmund, Miguel Velez, Christian Kästner, Akshay<--size:6.973999977111816-->
<--size:6.903907775878906-->Patel, and Yuvraj Agarwal. 2017. Transfer Learning for Performance Modeling of<--size:6.903907775878906-->
<--size:6.914466857910156-->Configurable Systems: An Exploratory Analysis. arXiv:1709.02280 [cs, stat] (Sept.<--size:6.914466857910156-->
<--size:6.973999977111816-->2017). arXiv:cs, stat/1709.02280<--size:6.973999977111816-->
<--size:6.973999977111816-->[11] Pooyan Jamshidi, Miguel Velez, Christian Kästner, and Norbert Siegmund. 2018.<--size:6.973999977111816-->
<--size:7.005312442779541-->Learning to Sample: Exploiting Similarities Across Environments to Learn Per-<--size:7.005312442779541-->
<--size:6.973999977111816-->formance Models for Configurable Systems. (2018), 12.<--size:6.973999977111816-->
<--size:6.973999977111816-->[12] Mohammad Ali Javidian, Pooyan Jamshidi, and Marco Valtorta. 2019. Transfer<--size:6.973999977111816-->
<--size:6.903907775878906-->Learning for Performance Modeling of Configurable Systems: A Causal Analysis.<--size:6.903907775878906-->
<--size:6.973999977111816-->arXiv:1902.10119 [cs] (Feb. 2019). arXiv:cs/1902.10119<--size:6.973999977111816-->
<--size:6.973999977111816-->[13] Christian Kaltenecker, Alexander Grebhahn, Norbert Siegmund, Jianmei Guo,<--size:6.973999977111816-->
<--size:6.903907775878906-->and Sven Apel. 2019. Distance-Based Sampling of Software Configuration Spaces.<--size:6.903907775878906-->
<--size:6.9530463218688965-->In Proceedings of the 41st International Conference on Software Engineering (ICSE<--size:6.9530463218688965-->
<--size:6.973999977111816-->’19) . 1084–1094. https://doi.org/10.1109/ICSE.2019.00112<--size:6.973999977111816-->
<--size:6.973999977111816-->[14] S. Kullback and R. A. Leibler. 1951. On Information and Sufficiency. Ann. Math.<--size:6.973999977111816-->
<--size:6.973999977111816-->Statist. 22, 1 (March 1951), 79–86. https://doi.org/10.1214/aoms/1177729694<--size:6.973999977111816-->
<--size:6.973999977111816-->[15] Jeho Oh, Don Batory, Margaret Myers, and Norbert Siegmund. 2017. Finding Near-<--size:6.973999977111816-->
<--size:6.907429218292236-->Optimal Configurations in Product Lines by Random Sampling. In Proceedings of<--size:6.907429218292236-->
<--size:7.043394565582275-->the 2017 11th Joint Meeting on Foundations of Software Engineering - ESEC/FSE<--size:7.043394565582275-->
<--size:6.973999977111816-->2017 . 61–71. https://doi.org/10.1145/3106237.3106273<--size:6.973999977111816-->
<--size:6.973999977111816-->[16] Rafael Olaechea, Derek Rayside, Jianmei Guo, and Krzysztof Czarnecki. 2014.<--size:6.973999977111816-->
<--size:7.0364861488342285-->Comparison of Exact and Approximate Multi-Objective Optimization for Soft-<--size:7.0364861488342285-->
<--size:7.043394565582275-->ware Product Lines. In Proceedings of the 18th International Software Product<--size:7.043394565582275-->
<--size:6.96702241897583-->Line Conference - Volume 1 (SPLC ’14) . 92–101. https://doi.org/10.1145/2648511.<--size:6.96702241897583-->
<--size:6.973999977111816-->2648521<--size:6.973999977111816-->
<--size:6.973999977111816-->[17] Juliana Alves Pereira, Hugo Martin, Mathieu Acher, Jean-Marc Jézéquel, Goetz<--size:6.973999977111816-->
<--size:7.043394565582275-->Botterweck, and Anthony Ventresque. 2019.<--size:7.043394565582275-->
<--size:7.043394565582275-->Learning Software Configura-<--size:7.043394565582275-->
<--size:7.043394565582275-->tion Spaces: A Systematic Literature Review.<--size:7.043394565582275-->
<--size:7.043394565582275-->ArXiv abs/1906.03018 (2019).<--size:7.043394565582275-->
<--size:6.973999977111816-->arXiv:1906.03018<--size:6.973999977111816-->
<--size:6.973999977111816-->[18] Sergio Segura, José A. Galindo, David Benavides, José A. Parejo, and Antonio<--size:6.973999977111816-->
<--size:6.903907775878906-->Ruiz-Cortés. 2012. BeTTy: Benchmarking and Testing on the Automated Analysis<--size:6.903907775878906-->
<--size:6.903907775878906-->of Feature Models. In Proceedings of the Sixth International Workshop on Variability<--size:6.903907775878906-->
<--size:7.043394565582275-->Modeling of Software-Intensive Systems - VaMoS ’12 . 63–71. https://doi.org/10.<--size:7.043394565582275-->
<--size:6.973999977111816-->1145/2110147.2110155<--size:6.973999977111816-->
<--size:6.973999977111816-->[19] Kai Shi. 2017. Combining Evolutionary Algorithms with Constraint Solving for<--size:6.973999977111816-->
<--size:7.029570579528809-->Configuration Optimization. In 2017 IEEE International Conference on Software<--size:7.029570579528809-->
<--size:7.043394565582275-->Maintenance and Evolution (ICSME) . IEEE, 665–669.<--size:7.043394565582275-->
<--size:7.043394565582275-->https://doi.org/10.1109/<--size:7.043394565582275-->
<--size:6.973999977111816-->ICSME.2017.32<--size:6.973999977111816-->
<--size:6.973999977111816-->[20] Norbert Siegmund, Alexander Grebhahn, Sven Apel, and Christian Kästner. 2015.<--size:6.973999977111816-->
<--size:6.960038185119629-->Performance-Influence Models for Highly Configurable Systems. In Proceedings<--size:6.960038185119629-->
<--size:6.935536861419678-->of the 2015 10th Joint Meeting on Foundations of Software Engineering - ESEC/FSE<--size:6.935536861419678-->
<--size:6.973999977111816-->2015 . ACM, 284–294. https://doi.org/10.1145/2786805.2786845<--size:6.973999977111816-->
<--size:6.973999977111816-->[21] Norbert Siegmund, Stefan Sobernig, and Sven Apel. 2017. Attributed Variability<--size:6.973999977111816-->
<--size:6.9425458908081055-->Models: Outside the Comfort Zone. In Proceedings of the 2017 11th Joint Meeting<--size:6.9425458908081055-->
<--size:7.043394565582275-->on Foundations of Software Engineering (ESEC/FSE 2017) . ACM, New York, NY,<--size:7.043394565582275-->
<--size:6.973999977111816-->USA, 268–278. https://doi.org/10.1145/3106237.3106251<--size:6.973999977111816-->
<--size:6.973999977111816-->[22] C. Spearman. 1904. The Proof and Measurement of Association between Two<--size:6.973999977111816-->
<--size:6.973999977111816-->Things. The American Journal of Psychology 15, 1 (1904), 72–101.<--size:6.973999977111816-->
<--size:6.973999977111816-->[23] Student. 1908. Probable Error of Correlation Coefficient. Biometrika 6, 2-3 (Sept.<--size:6.973999977111816-->
<--size:6.973999977111816-->1908), 302–310. https://doi.org/10.1093/biomet/6.2-3.302<--size:6.973999977111816-->
<--size:6.973999977111816-->[24] Tian Huat Tan, Yinxing Xue, Manman Chen, Jun Sun, Yang Liu, and Jin Song<--size:6.973999977111816-->
<--size:6.9320292472839355-->Dong. 2015. Optimizing Selection of Competing Features via Feedback-Directed<--size:6.9320292472839355-->
<--size:6.9530463218688965-->Evolutionary Algorithms. In Proceedings of the 2015 International Symposium on<--size:6.9530463218688965-->
<--size:7.039941310882568-->Software Testing and Analysis (ISSTA 2015) . ACM, 246–256. https://doi.org/10.<--size:7.039941310882568-->
<--size:6.973999977111816-->1145/2771783.2771808<--size:6.973999977111816-->
<--size:6.973999977111816-->[25] Pavel Valov, Jean-Christophe Petkovich, Jianmei Guo, Sebastian Fischmeister,<--size:6.973999977111816-->
<--size:7.043394565582275-->and Krzysztof Czarnecki. 2017. Transferring Performance Prediction Models<--size:7.043394565582275-->
<--size:7.043394565582275-->Across Different Hardware Platforms. In Proceedings of the 8th ACM/SPEC on<--size:7.043394565582275-->
<--size:6.903907775878906-->International Conference on Performance Engineering - ICPE ’17 . ACM Press, 39–50.<--size:6.903907775878906-->
<--size:6.973999977111816-->https://doi.org/10.1145/3030207.3030216<--size:6.973999977111816-->
<--size:6.973999977111816-->[26] Fan Wu, Westley Weimer, Mark Harman, Yue Jia, and Jens Krinke. 2015. Deep<--size:6.973999977111816-->
<--size:6.921497344970703-->Parameter Optimisation. In Proceedings of the 2015 Annual Conference on Genetic<--size:6.921497344970703-->
<--size:6.98793363571167-->and Evolutionary Computation (GECCO ’15) . ACM, 1375–1382. https://doi.org/<--size:6.98793363571167-->
<--size:6.973999977111816-->10.1145/2739480.2754648<--size:6.973999977111816-->
<--size:6.973999977111816-->[27] Yi Xiang, Yuren Zhou, Zibin Zheng, and Miqing Li. 2018. Configuring Software<--size:6.973999977111816-->
<--size:7.043394565582275-->Product Lines by Combining Many-Objective Optimization and SAT Solvers.<--size:7.043394565582275-->
<--size:7.043394565582275-->ACM Transactions on Software Engineering and Methodology 26, 4 (Feb. 2018),<--size:7.043394565582275-->
<--size:6.973999977111816-->1–46. https://doi.org/10.1145/3176644<--size:6.973999977111816-->
